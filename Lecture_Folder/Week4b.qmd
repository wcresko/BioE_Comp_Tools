---
title: "Week 4b - Statistics for Bioengineering"
author: "Bill Cresko"
format: 
  revealjs:
    transition: fade
    transition-speed: slow
editor: visual
---

```{r}
library(tidyverse)
library(gt)
library(readxl)
library(nycflights23)
theme_set(theme_minimal())
```

# Sampling, parameter estimation, error and power

## R INTERLUDE - Simulate a population and sample it! {.smaller}

## Sampling Exercise

- Step 1 - take 50 samples of 
    - size 10
    - size 100
    - size 1000
- Step 2 - Calculate the range from each sample 
- Step 3 - Calculate key statistics using `summary`
- Step 4 - Plot the distribution of sample mean estimates
- Step 5 - Calculate the 2.5% and 97.5% quantiles

## Different sample sizes

### Sample size of 10

```{r, echo = TRUE, out.width="50%"}
set.seed(32)
true_pop <- rpois(n=10000, lambda = 3)
samps_var_10 <- replicate(50, sample(true_pop, 10))
samps_var_means_10 <- apply(samps_var_10, 2, mean)
```

### Sample size of 100

```{r, echo = TRUE, out.width="50%"}
set.seed(32)
true_pop <- rpois(n=10000, lambda = 3)
samps_var_1000 <- replicate(50, sample(true_pop, 100))
samps_var_means_100 <- apply(samps_var_1000, 2, mean)
```

### Sample size of 1000

```{r, echo = TRUE, out.width="50%"}
set.seed(32)
true_pop <- rpois(n=10000, lambda = 3)
samps_var_1000 <- replicate(50, sample(true_pop, 1000))
samps_var_means_1000 <- apply(samps_var_1000, 2, mean)
```


## Statistics for original distribution

```{r, echo=TRUE, eval=TRUE}
range(true_pop)
summary(true_pop)
```

\

```{r}
hist(true_pop)
```

## {.smaller}

### Sample size of 10
```{r, echo=TRUE, eval=TRUE}
range(samps_var_means_10)
summary(samps_var_means_10)
```

### Sample size of 100
```{r, echo=TRUE, eval=TRUE}
range(samps_var_means_100)
summary(samps_var_means_100)
```

### Sample size of 1000
```{r, echo=TRUE, eval=TRUE}
range(samps_var_means_1000)
summary(samps_var_means_1000)
```


## Histograms

::::: columns
::: {.column width="50%"}
```{r}
hist(true_pop)
```
:::

::: {.column width="50%"}
```{r}
hist(samps_var_means_10, xlim =c(0,5))
```
:::

::: {.column width="50%"}
```{r}
hist(samps_var_means_100, xlim =c(0,5))
```
:::

::: {.column width="50%"}
```{r}
hist(samps_var_means_1000, xlim =c(0,5))
```
:::

:::::

## Percentiles and quantiles

- Percentile - an intuitive way of measuring a relative position in a data set
- `p-th` percentile is a value that is greater than `p%` of the data
- 50th percentile of a data set is the median
- 25th percentile is the 1st quartile
- 75th percentile is the 3rd quartile
- Quantiles divide a dataset into equal parts
- The number of groups can be any integer, but is often 10
- Also the basis of the p-value for test statistics

## Percentiles and quantiles in R

```{r, echo=TRUE, eval=TRUE}
data_RNAseq <- read.table("Stickle_RNAseq.tsv", header=TRUE, sep = "\t")
quantile(data_RNAseq$Gene20, probs = c(0.25, 0.5, 0.75))
```

\
```{r, echo=TRUE, eval=TRUE}
data_RNAseq <- read.table("Stickle_RNAseq.tsv", header=TRUE, sep = "\t")
summary(data_RNAseq$Gene20)
```

\

```{r, echo=TRUE, eval=TRUE}
data_RNAseq <- read.table("Stickle_RNAseq.tsv", header=TRUE, sep = "\t")
data_RNAseq$Gene20 |>
summary()
```

## CHALLENGE - Bootstrapping to produce a confidence interval {.smaller}

-   The 2.5th and 97.5th percentiles of the bootstrap sampling distribution are a passable 95% confidence interval
-   Note that no transformations or normality assumptions needed
-   You can use the `quantile()` function to calculate these
-   You can also use `qnorm`
-   Use R to figure out the bootstrap distribution for other parameters (such as variance).
-   Try to produce bootstrap distributions for the mean and variance of gene expresssion of one gene from the stickleback data set
-   Plot the resulting distributions
-   Determine the value of the 2.5th and 97.5th percentiles


## Finding the CI's of the empirical distribution {.smaller}

### Sample size of 10

```{r, echo=TRUE, eval=TRUE}
samps_var_means_10 |>
quantile(probs = c(0.025, 0.975))
```

### Sample size of 100

```{r, echo=TRUE, eval=TRUE}
samps_var_means_100 |>
quantile(probs = c(0.025, 0.975))
```
### Sample size of 1000

```{r, echo=TRUE, eval=TRUE}
samps_var_means_1000 |>
quantile(probs = c(0.025, 0.975))
```

------------------------------------------------------------------------

# Hypothesis testing, test statistics, p-values {.smaller}

## What is a hypothesis {.vcenter .flexbox}

-   A statement of belief about the world
-   Need a **critical** test to
    -   accept or reject the hypothesis
    -   compare the relative merits of different models
-   This is where **statistical sampling distributions** come into play
-   Statistical sampling distributions are built upon sampling distributions of random variables

## What is a hypothesis {.vcenter .flexbox}

::: {.incremental}

- $H_0$ : *Null hypothesis* : The amino acid substitution ***does not change*** the catalytic rate of an enzyme

\

- $H_A$ : *Alternative Hypothesis*: The amino acid substitution ***does change*** the catalytic rate of an enzyme

- $H_A$ : *Alternative Hypothesis*: The amino acid substitution ***increases*** the catalytic rate of an enzyme

- $H_A$ : *Alternative Hypothesis*: The amino acid substitution ***decreases*** the catalytic rate of an enzyme

:::

## Hypothesis tests {.vcenter .flexbox}

-   What is the probability that we would reject a **true null hypothesis**?

-   What is the probability that we would accept a **false null hypothesis**?

-   How do we **decide** when to reject a null hypothesis and support an alternative?

-   What can we **conclude** if we fail to reject a null hypothesis?

-   What **parameter estimates of distributions** are important to test hypotheses?

## Null and alternative hypotheses \| population distributions {.vcenter .flexbox}

```{r, echo=FALSE, out.width='90%', fig.align='center'}
knitr::include_graphics("images/week_3.001.jpeg")
```

## Null and alternative hypotheses - population distributions {.vcenter .flexbox}

```{r, echo=FALSE, out.width='60%', fig.align='center'}
knitr::include_graphics("images/week_3.002.jpeg")
```

## Type 1 and Type 2 errors {.smaller}

<br>

```{r, echo=FALSE, out.width='90%', fig.align='center'}
knitr::include_graphics("images/week_3.007.jpeg")
```

## Components of hypothesis testing {.smaller}

-   **p-value** = the long run probability of rejecting a true null hypothesis
-   $\alpha$ = critical value of p-value cutoff for experiments. The Type I error we are willing to tolerate.
-   $\beta$ = cutoff for probability of accepting a false null hypothesis
-   **Power** = the probability that a test will reject a false null hypothesis (1 - $\beta$). 
    - It depends on effect size, sample size, chosen alpha, and population standard deviation
    - Makes sense - **probability of rejecting a false null hypothesis**
-   **Multiple testing** = performing the same or similar tests multiple times - need to correct alpha value
-   Can correct multiple testing using a tax (e.g. **Bonferonni**) or directly estimating a **False Discovery Rate (FDR)**


## Why do we use $\alpha = 0.5$ as a cutoff? {.smaller}

```{r, echo=FALSE, out.width='90%', fig.align='center'}
knitr::include_graphics("images/week_3.008.jpeg")
```


## Statistical sampling distributions {.smaller .vcenter .flexbox}

-   Statistical tests provide a way to perform **critical tests** of hypotheses
-   Just like raw data, statistics are **random variables** and depend on sampling distributions of the underlying data
-   The particular **form of the statistical distribution** depends on the test statistic and parameters such as the degrees of freedom that are determined by sample size.
-   Essentially we get **a distribution for statistics values too**
-   How frequently do we see a statistics value as large/small as this one?

## Statistical tests - parametric  {.smaller .vcenter .flexbox}

-   In some cases we can perform **parameteric statistical tests** where we make assumptions about the distributions of the underlying variables
-   Mathematical proofs relate the observed test statistic to a theoretical probability distribution based upon the sample size
-   They are called `parametric` because we are estimating the parameter of an assumed distribution
-   The assumed distribution means that parametric tests often have assumptions

## Statistical tests - non-parametric  {.smaller .vcenter .flexbox}

- In many other cases we create an **empricial null statistical distribution** that models the distribution of a test statistic under the **null hypothesis**.
- We can use this for `non-parametric statistical tests`
-   Similar to point estimates, we calculate an **observed test statistic value** for our data.
-   Then see how probable it was by comparing against **the null distribution.**
-   The probability of seeing that value or greater is called the **p-value** of the statistic.
-   Parametric and non-parametric tests function the same way, they really only differ by whether the null distribution is assumed or created empirically through resampling

## Four common statistical distributions {.vcenter .flexbox}

```{r, echo=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics("images/week_3.003.jpeg")
```

## t sampling distribution {.smaller .vcenter .flexbox}

-  One of the oldest test statistic is the t-statistic.
-   Compares 
    - one group mean against a value (one sample t-test)
    - or two group means against one another (two sample t-test)
-   The t-statistic is a standardized difference between two sample means
    -   t = 0 indicates no difference between population means
    -   t-distribution is ~ Normal, with the center and peak at 0
-   We can evaluate the t-statistic for our sample data and see whether it falls far enough away from zero 
-   If our p-value falls below our critical value we reject the null hypothesis


## {.vcenter .flexbox}

$$\large t = \frac{(\bar{y}_1-\bar{y}_2)}{s_{\bar{y}_1-\bar{y}_2}} $$

where

```{r, echo=FALSE, out.width='60%', fig.align='center'}
knitr::include_graphics("images/week_3.016.jpeg")
```

which is the calculation for the standard error of the mean difference

## The t-test and t sampling distribution under different degrees of freedom {.smaller}

```{r, echo=FALSE, out.width='90%', fig.align='center'}
knitr::include_graphics("images/week_3.004.jpeg")
```


## The t-test and t sampling distribution \| one- and two-tailed test {.smaller}

```{r, echo=FALSE, out.width='60%', fig.align='center'}
knitr::include_graphics("images/week_3.006.jpeg")
```

## The t-test and t sampling distribution {.vcenter .flexbox}

$H_0$ : *Null hypothesis* : The microbiota treatment of stickleback fish ***does not change*** the level of expression of a particular gene

$$H_0 : \mu_{conv} = \mu_{gf}$$

$H_A$ : *Alternative hypothesis* : The microbiota treatment of stickleback fish ***does change*** the level of expression of a particular gene

$$H_A : \mu_{conv} \neq \mu_{gf}$$

## Assumptions of parameteric t-tests {.vcenter .flexbox}

<br>

-   The theoretical t-distributions for each degree of freedom were calculated for populations that are:
    -   normally distributed
    -   have equal variances (if comparing two means)
    -   observations are independent (randomly drawn)
-   What do you do if the there is non-normality?
    -   nonparametric tests such as Mann-Whitney-Wilcoxon
    -   randomization tests to create a null distribution

## A tangent on degrees of freedom

-   In statistics, degrees of freedom refers to the number of values in a calculation that are free to vary.
-   Example: you have a set of numbers, like 1, 2, 3, and 4. 
-   If you know the average (mean) of these numbers is 3, you can actually choose three of the numbers freely
-   But the fourth number will be determined by the other three. 
-   So in this case, you have three degrees of freedom.



## Let's try it out

-   Use the below code to set up two simulated populations:

```{r, echo=TRUE, eval=TRUE}
set.seed(518)
pop1 <- sample(rnorm(n=10000, mean=2, sd=0.5), size = 100)
pop2 <- sample(rnorm(n=10000, mean=5, sd=0.5), size = 100)

```

## Let's try it out

:::: columns

::: {.column width="50%"}
```{r, echo=TRUE, eval=TRUE}
hist(pop1, xlim = c(0, 10))
```
:::

::: {.column width="50%"}
```{r, echo=TRUE, eval=TRUE}
hist(pop2, xlim = c(0,10))
```
:::

::::


## Now for the t-test!

-   Use the function `t.test()` in R
-   How do we interpret this? How would you write this conclusion as a sentence?

```{r, echo=TRUE, eval=FALSE}
t.test(pop1, pop2)
t.test(data$continuous, data$categorical)
```


## Now for the t-test!

::::{.columns}

::: {.column width="50%"}
```{r, echo=TRUE, eval=TRUE}
t.test(pop1, pop2)
```
:::

::: {.column width="50%"}
```{r, echo=TRUE, eval=TRUE}
ttest_output <- t.test(pop1, pop2)
print(ttest_output)
```
:::

::::

## R-interlude

- try doing a two sample t-test on the stickleback microbiome RNA-seq data


# Nonparametric tests

## Nonparametric tests

-   What if your data does not meet the requirements of a parametric t-test?
-   What do you do if the there is non-normality?
    -   nonparametric tests such as Mann-Whitney-Wilcoxon
    -   randomization tests to create a null distribution

## Mann-Whitney-Wilcoxon Tests

-   The Mann-Whitney U (also called “Mann-Whitney-Wilcoxon) Test tests for distributional differences between the ranks of two samples.
-   In R the function `wilcox.test()` can be used to perform it, in much the same way the `t.test()` function is used.

## Let's try it out

```{r, out.width = "50%"}
set.seed(518)
pop1 <- sample(rnorm(n=10000, mean=2, sd=0.5), size = 100)
pop2 <- sample(rnorm(n=10000, mean=5, sd=1.5), size = 100)
```


## The Mann Whit U Test

-   Use the function `wilcox.test()` in R to compare pop1 and pop2
-   How do we interpret that result? 
-   How would you write it as a sentence for a paper?

## The Mann Whit U Test

```{r, echo=TRUE}
wilcox.test(pop1, pop2)
```

# Empirical null distributions

## Null distributions and p-values

-   We can also create a null statistical distribution that models the distribution of a test statistic under the null hypothesis
-   To create the null distribution we can use either randomization or resampling

## Creating a null distribution through randomization {.smaller}

1.  Combine values from both populations into a single vector
2.  Randomly shuffle the vector using the `sample()` function
3.  Calculate a t statistic based on the first n1 and n2 observations as our “pseudo samples” from “populations” 1 and 2, respectively, and save the value
4.  Repeat steps 2 and 3 many times (e.g. 1000)
5.  Calculate the proportion of pseudo replicates in which t is ≥ to our original, observed value of t. **This proportion is our estimated p-value for the test**.

## Let's do it - the starting populations

```{r, echo=TRUE }
set.seed(56)
pop_1 <- rnorm(n=50, mean=20.1, sd=2)#simulate population 1 for this example
pop_2 <- rnorm(n=50, mean=19.3, sd=2)#simulate population 2 for this example
```

```{r, echo=TRUE, out.width="40%"}
hist(pop_1)
hist(pop_2)
```

## Calculate the t-test

```{r, echo=TRUE}
# Store the t statistic calculated from our samples, using t.test()
t_obs <- t.test(x=pop_1, y=pop_2, alternative="greater")$statistic
```

## Combine the populations, sample from that, and calculate t-tests

```{r, echo=TRUE}
# Combine both population vectors into one
pops_comb <- c(pop_1, pop_2)

# Randomly shuffle and calculate t statistic 1000 times
t_rand <- replicate(1000, {
  pops_shuf <- sample(pops_comb)
  t.test(x=pops_shuf[1:50], y=pops_shuf[51:100], alternative="greater")$statistic
  })
```

## Plot the null distribution

```{r, echo=TRUE}

# Plot the "null distribution" from the randomization-based t-values
hist(t_rand)
```

## Null distributions and p-values {.flexbox .vcenter}

```{r, echo=FALSE, out.width='90%', fig.align='center'}
knitr::include_graphics("images/week_3.009.jpeg")
```

## What's our result?

-   How do we interpret this?

```{r, echo=TRUE}
# Calculate the p-value for the test as the number of randomization t-values greater
# than or equal to our actual t-value observed from the data
p <- sum(t_rand>=t_obs)/1000

p
```


_________________________

# Statistical Power

## Statistical power

- Helps us to interpret the failure to reject a null hypothesis
-   What factors can affect the power of our experiment - our ability to avoid a Type 2 error?
    -   Sample size
    -   Effect size (difference between the groups)
    -   Variance (range of values for this trait/measure)

## Statistical power

-   Type 1 error - $\alpha$ - incorrectly rejecting a true null hypothesis
    -   This is saying that there is an effect when there isn’t
-   Type 2 error - $\beta$ - incorrectly accepting a false null hypothesis
    -   This is saying that there isn’t an effect when there is
-   Power is the probability of rejecting a false null hypothesis
-   Mostly we shoot for a power of around 80%
-   Power can be calculated *post hoc* or *a priori*

## A priori Power analyses

-   Before we start an experiment, we are interested in what sample size we should collect
-   We can use simulations to test different sample sizes
-   Let's say we're studying college students again, and we're interested in seeing if there's a difference in study hours between freshman and seniors
-   How many students should we sample?
-   This will depend on our predictions about the effect size of this measurement

## Power \| the things one needs to know

$$ Power \propto \frac{(ES)(\alpha)(\sqrt n)}{\sigma}$$

-   Power is proportional to the combination of these parameters

    -   **ES** - effect size; how large is the change of interest?
    -   $\alpha$ - significance level (usually 0.05)
    -   **n** - sample size
    -   $\sigma$ - standard deviation among experimental units within the same group.

## Power \| what we usually want to know {.flexbox .vcenter}

<br>

```{r, echo=FALSE, out.width='90%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_6b.002.jpeg")
```

## Power \| rough calculation

```{r, echo=FALSE, out.width='90%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_6b.003.jpeg")
```

## Steps to power analysis

1.  Simulate the true distributions of our populations (decide on effect size, distribution type, and variance)
2.  Draw random samples of different sizes from those populations
3.  Perform our statistical test (t-test) on these samples
4.  Repeat 2 & 3 \~1000 times
5.  Plot our resulting p-values against sample size

## Step 1: simulating our true populations

-   What is the distribution type?
-   What is the effect size: difference in means between populations?
-   What is the variance?

## Step 1: simulating our true populations

```{r, echo=TRUE}
senior <- rpois(5000, lambda = 10)
fresh <- rpois(5000, lambda = 12)
```

## Step 1: simulating our true populations

```{r, echo=TRUE, out.width="40%"}
hist(senior)
hist(fresh)
```

## Step 2: drawing a sample

```{r, echo=TRUE}
sample_s <- sample(senior, size = 10, replace = FALSE)
sample_f <- sample(fresh, size = 10, replace = FALSE)
```

## Step 2: drawing a sample

```{r, echo=TRUE, out.width="40%"}
hist(sample_s)
hist(sample_f)
```

## Step 3: statistical test

```{r, echo=TRUE}
t.test(sample_f, sample_s)
```

## Step 4: setting up our replicates

-   Take a look at the "samps_var" vectors, how are they arranged? How would we begin conducting t-tests using each replicate from the two populations?

```{r, echo=TRUE}
## sample size of 10
samps_var_s <- replicate(n = 100, sample(senior, size = 10))
samps_var_f <- replicate(n = 100, sample(fresh, size = 10))

```

## Step 4: Testing our replicates

```{r, echo=TRUE}
# setting up a "test" dataframe
tests <- data.frame(1:100)
tests$SampleSize <- rep("10", 100)

for (i in 1:ncol(samps_var_f)){
  tests$result[i] <- t.test(samps_var_s[,i], samps_var_f[,i])$p.value
}
table(tests$result < 0.05)
```

## Step 4 contd: changing the sample size

## Step 4 contd: multiple sample sizes

-   Requires a more complex for loop

```{r, echo=TRUE, eval = FALSE}


results <- data.frame()


# using seq to set up the sample sizes


for (x in seq(10,100, by = 10)){
  samps_var_s <- replicate(n = 100, sample(senior, size = x))
  samps_var_f <- replicate(n = 100, sample(fresh, size = x))
  
  tests <- data.frame(1:100)
  tests$SampleSize <- rep(x, 100)
  for (i in 1:100){
    tests$p.value[i] <- t.test(samps_var_s[,i], samps_var_f[,i])$p.value
  }
  results <- rbind(results, tests)
}
```



