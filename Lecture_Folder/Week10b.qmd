---
title: "Week 10b - Statistics for Bioengineering"
author: "Bill Cresko"
format: 
  revealjs:
    transition: fade
    transition-speed: slow
editor: visual
---

```{r}
library(tidyverse)
library(gt)
library(readxl)
library(ggplot2)
library(tibble)
library(randomForest)
library(rpart)
library(rpart.plot)
library(caret)
library(pROC)
library(ROCR)
```

## Today's Topics

-   Bias-Variance tradeoff
-   Decision trees and random forests
-   Cross-validation strategies
-   ROC curves and AUC analysis
-   Bioengineering applications

## Bias-Variance Tradeoff {.smaller}

$$
\text{Expected Test Error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}
$$

-   **Bias**: Error from overly simple models (underfitting)
-   **Variance**: Error from overly complex models (overfitting)
-   **Goal**: Balance model complexity with generalization

![](images/bias-variance.jpeg)

## Strategies to Address the Tradeoff

-   **Regularization** (Lasso, Ridge) to constrain model complexity

-   **Cross-validation** to choose optimal model complexity

-   **Ensemble methods** (bagging, boosting) to reduce variance

    -   This often used with decision tree models

## Decision Trees {.smaller}

-   A **non-parametric** way to build models

-   **Key Characteristics:**

    -   Tree-like structure of decision rules
    -   Easy to interpret and visualize
    -   Handle both numerical and categorical data
    -   No distributional assumptions
    -   Capture non-linear relationships naturally

-   **Algorithm:**

    -   Recursively split data to maximize information gain until stopping criteria are met

## Decision tree example

![](images/decision_tree_illustration.png)

## Introduction to Impurity Measures

-   **Decision trees** need to decide which features to split on
-   **Impurity measures** help quantify how "mixed" or "pure" a dataset is
-   Two main measures: **Gini Index** and **Entropy**
-   Both measure the same concept but with different mathematical approaches

## What is "Impurity"? {.smaller}

-   **Pure Node:**
    -   All samples belong to the same class
    -   Easy to make predictions
    -   Low impurity
-   **Impure Node:**
    -   Samples belong to multiple classes
    -   Harder to make predictions
    -   High impurity
-   Example: If participants at a node are "injured" → **pure**
-   If 50% injured, 50% not → **impure**

## Gini Index: Definition {.smaller}

The **Gini Index** measures the probability of misclassifying a randomly chosen sample.

$$\text{Gini} = 1 - \sum_{i=1}^{c} p_i^2$$

-   Where:
    -   $c$ = number of classes
    -   $p_i$ = probability of class $i$
-   **Range:** 0 to 0.5 (for binary classification)
    -   **0** = perfectly pure
    -   **0.5** = maximum impurity (50-50 split)

## Gini Index: Examples {.smaller}

-   **Example 1: Pure Node**
    -   100 inuries, 0 not injuries
    -   $p_{injury} = 1.0, p_{not} = 0.0$
    -   $\text{Gini} = 1 - (1.0^2 + 0.0^2) = 0$
-   **Example 2: Mixed Node**\
    -   60 injuries, 40 not injuries
    -   $p_{injury} = 0.6, p_{not} = 0.4$
    -   $\text{Gini} = 1 - (0.6^2 + 0.4^2)$
    -   $\text{Gini} = 1 - (0.36 + 0.16) = 0.48$

## Entropy: Definition {.smaller}

-   **Entropy** measures the amount of information or surprise in the data.

$$\text{Entropy} = -\sum_{i=1}^{c} p_i \log_2(p_i)$$

-   Where:
    -   $c$ = number of classes\
    -   $p_i$ = probability of class $i$
-   **Range:** 0 to $\log_2(c)$
    -   **0** = perfectly pure
    -   **1** = maximum impurity (for binary classification)

## Entropy: Examples {.smaller}

-   **Example 1: Pure Node**
    -   100 injuries, 0 not injufies
    -   $p_{injury} = 1.0, p_{not} = 0.0$
    -   $\text{Entropy} = -1.0 \log_2(1.0) = 0$
-   **Example 2: Mixed Node**
    -   60 injuries, 40 not injuries
    -   $p_{injury} = 0.6, p_{not} = 0.4$
    -   $\text{Entropy} = -0.6\log_2(0.6) - 0.4\log_2(0.4)$
    -   $\text{Entropy} = 0.44 + 0.53 = 0.97$

## Gini vs Entropy {.smaller}

| Aspect             | Gini Index                | Entropy                   |
|--------------------|---------------------------|---------------------------|
| **Computation**    | Faster                    | Slower                    |
| **Range (binary)** | 0 to 0.5                  | 0 to 1.0                  |
| **Sensitivity**    | Less sensitive to changes | More sensitive to changes |

**In practice:** Both usually give similar results, Gini preferred for speed.

## Information Gain {.smaller}

Both measures are used to calculate **Information Gain**:

$$\text{Information Gain} = \text{Impurity}_{parent} - \sum \frac{n_{child}}{n_{parent}} \times \text{Impurity}_{child}$$

-   Decision trees choose splits that **maximize information gain**
-   Higher gain = better split = more homogeneous child nodes

## Practical Example {.smaller}

-   Dataset: 100 patients (60 injured, 40 not injured)

-   **Before split:**

    -   Gini = $1 - (0.6^2 + 0.4^2) = 0.48$
    -   Entropy = $-0.6\log_2(0.6) - 0.4\log_2(0.4) = 0.97$

-   **After split on "BMI":**

    -   Left: 50 patients (45 injured, 5 not injured)
    -   Right: 50 patients (15 injured, 35 not injured)

-   Calculate weighted average impurity to find information gain!

-   Do this again and again on different variables (features) to see which one provides the biggest gain

## Calculations After Split {.smaller}

-   Left Node (50 patients: 45 injured, 5 not injured)
    -   Proportions: $p_{injured} = 45/50 = 0.9$, $p_{not\_injured} = 5/50 = 0.1$
    -   **Gini Left:** $1 - (0.9^2 + 0.1^2) = 0.18$
    -   **Entropy Left:** $-0.9\log_2(0.9) - 0.1\log_2(0.1) = 0.469$
-   Right Node (50 patients: 15 injured, 35 not injured)
    -   Proportions: $p_{injured} = 15/50 = 0.3$, $p_{not\_injured} = 35/50 = 0.7$
    -   **Gini Right:** $1 - (0.3^2 + 0.7^2) = 0.42$
    -   **Entropy Right:** $-0.3\log_2(0.3) - 0.7\log_2(0.7) = 0.882$

## Calculations After Split {.smaller}

-   Weighted Average Impurity After Split
    -   **Weighted Gini:** $\frac{50}{100} \times 0.18 + \frac{50}{100} \times 0.42 = 0.09 + 0.21 = 0.30$
    -   **Weighted Entropy:** $\frac{50}{100} \times 0.469 + \frac{50}{100} \times 0.882 = 0.235 + 0.441 = 0.676$
-   Information Gain
    -   **Gini Information Gain:** $0.48 - 0.30 = 0.18$
    -   **Entropy Information Gain:** $0.97 - 0.676 = 0.294$

## Building Decision Trees in R

```{r}
#| echo: true
#| eval: false

library(rpart)
library(rpart.plot)

data(iris)
set.seed(123)
train_index <- createDataPartition(iris$Species, p = 0.7, list = FALSE)
train_data <- iris[train_index, ]
test_data <- iris[-train_index, ]

# Build and visualize tree
tree_model <- rpart(Species ~ ., data = train_data, method = "class")
rpart.plot(tree_model, type = 4, extra = 102, main = "Iris Classification Tree")

# Evaluate performance
predictions <- predict(tree_model, test_data, type = "class")
confusionMatrix(predictions, test_data$Species)
```

## Building Decision Trees in R

```{r}
#| echo: false
#| eval: true

# Classification example
library(rpart)
library(rpart.plot)

data(iris)
set.seed(123)
train_index <- createDataPartition(iris$Species, p = 0.7, list = FALSE)
train_data <- iris[train_index, ]
test_data <- iris[-train_index, ]

# Build and visualize tree
tree_model <- rpart(Species ~ ., data = train_data, method = "class")
rpart.plot(tree_model, type = 4, extra = 102, main = "Iris Classification Tree")

```

## Building Decision Trees in R

```{r}
#| echo: false
#| eval: true

# Classification example
library(rpart)
library(rpart.plot)

data(iris)
set.seed(123)
train_index <- createDataPartition(iris$Species, p = 0.7, list = FALSE)
train_data <- iris[train_index, ]
test_data <- iris[-train_index, ]

# Build and visualize tree
tree_model <- rpart(Species ~ ., data = train_data, method = "class")

# Evaluate performance
predictions <- predict(tree_model, test_data, type = "class")
confusionMatrix(predictions, test_data$Species)
```

## Decision Trees: Pros and Cons {.smaller}

-   **Advantages:**
    -   Highly interpretable
    -   No preprocessing required
    -   Automatic feature selection
    -   Handles interactions naturally
-   **Limitations:**
    -   Prone to overfitting
    -   High variance (unstable)
    -   Bias toward features with more levels
    -   Limited expressiveness for linear relationships
-   **Solution:** Use ensemble methods like Random Forests!

# Random Forests

## Random Forests - Embrace the Ensemble {.smaller}

**Key Innovation:** Combine multiple decision trees through:

1.  **Bootstrap sampling** (bagging) for training data
2.  **Random feature selection** at each split
3.  **Aggregate predictions** (voting/averaging)
4.  **Out-of-bag validation** for built-in cross-validation

**Result:** Reduced overfitting, improved accuracy, maintained interpretability through variable importance

## Random Forest Implementation {.smaller}

```{r}
#| echo: true
#| eval: false

# Classification
rf_model <- randomForest(
  Species ~ ., 
  data = train_data,
  ntree = 500,           # Number of trees
  mtry = 2,              # Features per split (√p for classification)
  importance = TRUE      # Calculate variable importance
)

print(rf_model)
plot(rf_model, main = "Random Forest Error Rates")

# Variable importance
varImpPlot(rf_model, main = "Variable Importance")
importance(rf_model)
```

## Random Forest Implementation {.smaller}

```{r}
#| echo: false
#| eval: true

# Classification
rf_model <- randomForest(
  Species ~ ., 
  data = train_data,
  ntree = 500,           # Number of trees
  mtry = 2,              # Features per split (√p for classification)
  importance = TRUE      # Calculate variable importance
)

print(rf_model)
plot(rf_model, main = "Random Forest Error Rates")

# Variable importance
varImpPlot(rf_model, main = "Variable Importance")
importance(rf_model)
```

## Hyperparameter Tuning with Cross-Validation {.smaller}

```{r}
#| echo: true
#| eval: false

# Define parameter grid
rf_grid <- expand.grid(mtry = c(1, 2, 3, 4))

# Cross-validation setup
ctrl <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 3,
  savePredictions = TRUE,
  classProbs = TRUE
)

# Train with tuning
rf_tuned <- train(
  Species ~ .,
  data = train_data,
  method = "rf",
  tuneGrid = rf_grid,
  trControl = ctrl,
  ntree = 300
)

plot(rf_tuned)
```

## Hyperparameter Tuning with Cross-Validation {.smaller}

```{r}
#| echo: false
#| eval: true

# Define parameter grid
rf_grid <- expand.grid(mtry = c(1, 2, 3, 4))

# Cross-validation setup
ctrl <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 3,
  savePredictions = TRUE,
  classProbs = TRUE
)

# Train with tuning
rf_tuned <- train(
  Species ~ .,
  data = train_data,
  method = "rf",
  tuneGrid = rf_grid,
  trControl = ctrl,
  ntree = 300
)

plot(rf_tuned)
```

## Cross-Validation: Beyond Built-in OOB

-   **Why Cross-Validation for Random Forests?**
    -   More reliable performance estimates
    -   Fair comparison with other algorithms
    -   Hyperparameter optimization
    -   Nested CV for unbiased evaluation
-   **Methods:**
    -   **K-fold CV**: Split into k folds, train on k-1, test on 1
    -   **Stratified CV**: Maintains class proportions
    -   **Repeated CV**: Multiple rounds for stability

## Biomedical Case Study: Knee Injury Prediction {.smaller}

**Context:** Early identification of injury risk factors

```{r}
#| echo: true
#| eval: true

# Load and prepare data
data <- read.csv("data/knee_injury.csv")
data$knee_injury <- as.factor(data$knee_injury)

# Train Random Forest with CV
set.seed(123)
train_control <- trainControl(
  method = "cv",
  number = 10,
  classProbs = FALSE,  # Set to FALSE
  summaryFunction = defaultSummary  # Use default instead of twoClassSummary
)

rf_knee <- train(
  knee_injury ~ .,
  data = data,
  method = "rf",
  trControl = train_control,
  metric = "Accuracy",  # Change metric since no ROC without probabilities
  importance = TRUE
)


```

## Clinical Interpretation: Variable Importance

```{r}
#| echo: true
#| eval: false

# Extract and visualize importance
importance_scores <- varImp(rf_knee)
plot(importance_scores, main = "Risk Factors for Knee Injury")

```

## Clinical Interpretation: Variable Importance

```{r}
#| echo: false
#| eval: true

# Extract and visualize importance
importance_scores <- varImp(rf_knee)
plot(importance_scores, main = "Risk Factors for Knee Injury")

```

## Model Evaluation: ROC Curves and AUC

-   **ROC Curve:** Plots True Positive Rate vs False Positive Rate across all classification thresholds

-   **Key Metrics:**

    -   **Sensitivity (TPR)**: $\frac{TP}{TP + FN}$ - How well we catch positive cases
    -   **Specificity (TNR)**: $\frac{TN}{TN + FP}$ - How well we avoid false alarms
    -   **AUC**: Area under ROC curve - Overall discriminative ability
    -   **AUC Range**: 0 to 1 (higher is better)

## Mathematical Foundation

For discrete points $(x_i, y_i)$ on the ROC curve:

$$AUC = \sum_{i=1}^{n-1} \frac{1}{2}(x_{i+1} - x_i)(y_i + y_{i+1})$$

-   Where:
    -   $x_i$ = FPR at threshold $i$
    -   $y_i$ = TPR at threshold $i$
    -   Points are sorted by increasing FPR

## AUC interpretation

-   AUC = 0.5: Random guessing
-   AUC = 0.7-0.8: Acceptable
-   AUC = 0.8-0.9: Excellent
-   AUC \> 0.9: Outstanding (check for overfitting)

## Calculation in R

```{r}
#| echo: true

# Load required libraries
library(ROCR)
library(pROC)
library(ggplot2)
library(dplyr)

# Set seed for reproducibility
set.seed(42)

# Generate sample data
n <- 1000
x1 <- rnorm(n)
x2 <- rnorm(n)
# Create binary outcome with some relationship to predictors
y <- rbinom(n, 1, plogis(0.5 * x1 + 0.3 * x2 + rnorm(n, 0, 0.5)))

# Split data
train_idx <- sample(1:n, 0.7 * n)
train_data <- data.frame(x1 = x1[train_idx], x2 = x2[train_idx], y = y[train_idx])
test_data <- data.frame(x1 = x1[-train_idx], x2 = x2[-train_idx], y = y[-train_idx])

# Fit logistic regression
model <- glm(y ~ x1 + x2, data = train_data, family = binomial)

# Get predicted probabilities
pred_probs <- predict(model, test_data, type = "response")
actual <- test_data$y
```

------------------------------------------------------------------------

## Calculate ROC Curve Points

```{r}
#| echo: true

# Using ROCR package
pred_obj <- prediction(pred_probs, actual)
perf_obj <- performance(pred_obj, "tpr", "fpr")

# Extract TPR and FPR
fpr <- perf_obj@x.values[[1]]
tpr <- perf_obj@y.values[[1]]
thresholds <- perf_obj@alpha.values[[1]]

# Calculate AUC using ROCR
auc_rocr <- performance(pred_obj, "auc")@y.values[[1]]

# Alternative using pROC package
roc_obj <- roc(actual, pred_probs, quiet = TRUE)
auc_proc <- auc(roc_obj)
```

## Visualization

```{r}
#| echo: true
#| output-location: slide
#| fig-width: 12
#| fig-height: 5

# Create comprehensive visualization
library(gridExtra)

# ROC Curve plot
roc_data <- data.frame(FPR = fpr, TPR = tpr)

p1 <- ggplot(roc_data, aes(x = FPR, y = TPR)) +
  geom_ribbon(aes(ymin = 0, ymax = TPR), alpha = 0.3, fill = "blue") +
  geom_line(color = "darkorange", size = 1.2) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray") +
  labs(title = paste("ROC Curve (AUC =", round(auc_rocr, 4), ")"),
       x = "False Positive Rate", y = "True Positive Rate") +
  theme_minimal() +
  coord_fixed()

# Distribution of predicted probabilities
prob_data <- data.frame(
  Probability = pred_probs,
  Actual = factor(actual, labels = c("Negative", "Positive"))
)

p2 <- ggplot(prob_data, aes(x = Probability, fill = Actual)) +
  geom_histogram(alpha = 0.7, bins = 30, position = "identity") +
  labs(title = "Distribution of Predicted Probabilities",
       x = "Predicted Probability", y = "Count") +
  theme_minimal() +
  scale_fill_manual(values = c("red", "blue"))

grid.arrange(p1, p2, ncol = 2)
```

## When to Use Each Method {.smaller}

| Method | Best For | Limitations |
|----|----|----|
| **Logistic Regression** | Linear relationships, inference needs, regulatory requirements | Assumes linearity, manual interaction terms |
| **Decision Trees** | High interpretability needs, small datasets, understanding interactions | Overfitting, instability |
| **Random Forest** | Prediction accuracy, complex patterns, robust performance | Less interpretable, computational cost |

## 

![](images/week_01.002.jpeg)
