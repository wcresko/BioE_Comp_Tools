---
title: "Week 8b - Statistics for Bioengineering"
author: "Bill Cresko"
format: 
  revealjs:
    transition: fade
    transition-speed: slow
editor: visual
---

```{r}
library(tidyverse)
library(gt)
library(readxl)
library(ggplot2)
library(tibble)
```

## This week {.smaller}

-   Key principles of experimental design

-   Linear Mixed Models (GLMMs)

-   Generalized Linear Models - Logistic and Poisson Regression

-   Chi-square tests of association and goodness of fit

# Variance components with 2 random factors using LME4 {.smaller}

```{r, echo=TRUE, eval=TRUE}
rnadata <- read.table('Stickle_RNAseq.tsv', header=T, sep='')

gene <- rnadata$Gene80[6:75] 
microbiota <- rnadata$Microbiota[6:75]
genotype <- rnadata$Genotype[6:75]

```

Estimate the variance components using Restricted Maximum Likelihood (REML)

```{r, echo=TRUE, eval=TRUE}
library(lme4)
lmer(gene ~ 1 + (1 | microbiota) + (1 | genotype) + (1 | microbiota:genotype))
```

------------------------------------------------------------------------

# Generalized Linear Models

## Using `glm` for more complicated mixed models

-   For a single categorical predictor, we can include effects of each factor level:

$$ y_i = \beta_0 + \beta_1(x~level1) + \beta_2(x~level2) + \beta_3x_3~... ~+ e_1 $$

-   Each factor level becomes it's own "effector" that will change the value of y
-   Instead of x being a continuous numerical value, for categorical data it will be either a 0 or 1
-   $x_3$ is also a continuous variable so this is a mixed model

## A Generalized Linear Model

-   Check out the help page in RStudio for `glm()`

```{r, echo=TRUE, eval=FALSE}
glm(formula = y ~ x1 + x2 + x3, family = gaussian(link = "identity"))
```

## `glm` to test Wolbachia infection {.smaller}

-   Set up a `glm()` to test for the effect of Wolbachia infection on Recombinant Fraction

```{r, echo=TRUE, eval=TRUE}
fly <- read.table("Mostoufi2022_Recombination_Edit.csv", header=T, sep=',')
model <- glm(fly$RecombinantFraction ~ fly$Wolbachia, family = gaussian(link="identity"))
summary(model)
```

## Adding more variables {.smaller}

```{r, echo=TRUE, eval=TRUE}
model2 <- glm(fly$RecombinantFraction ~ fly$Wolbachia + fly$Food, family = gaussian(link = "identity"))
summary(model2)
```

## Adding interaction effects {.smaller}

```{r, echo=TRUE, eval=TRUE}
model3 <- glm(fly$RecombinantFraction ~ fly$Wolbachia + fly$Food + fly$Wolbachia*fly$Food, family = gaussian(link = "identity"))
summary(model3)
```

## How do we know which model to use?

-   Want to be careful not to *overfit* your linear model (give too many variables)
-   We can use Deviance
-   The Akaike information criterion (AIC) is a metric for comparing different models to find the best fit
    -   Notice the AIC score in your output?
    -   This works the same way that it did in multiple linear regression

## What is Deviance?

-   Deviance measures model fit in **generalized linear models (GLMs)**
-   Similar to residual sum of squares in linear regression
-   Compares fitted model to a **saturated model**
-   Saturated model: fits data **perfectly** (one parameter per observation)

## Mathematical Definition

$$
\text{Deviance} = 2 \times (\log L_{\text{saturated}} - \log L_{\text{fitted}})
$$

-   Lower deviance = better fit
-   Used to compare models

## Types of Deviance in `glm()`

-   **Null Deviance**:
    -   From model with **only intercept**
    -   Measures fit using just the **mean response**
-   **Residual Deviance**:
    -   From the **fitted model** (with predictors)
    -   Measures how well your model fits relative to saturated model

## Interpreting Deviance

-   Large drop from null deviance to residual deviance → better model fit
-   Test improvement with chi-square test:

``` r
with(model, pchisq(null.deviance - deviance, 
                   df.null - df.residual, 
                   lower.tail = FALSE))
```

## Example Output

``` r
Null deviance: 120.45  on 99  degrees of freedom
Residual deviance:  98.23  on 97  degrees of freedom
```

-   The model explains variability beyond just the mean
-   Compare deviances to assess predictor usefulness

# Generalized Linear Models

## What is a generalized linear model?

-   The response variable of interest is binary - violates normality assumption because binomial distributed (**Logistic**)
-   The response variable is a count - violates the assumption of independent mean and variance because the data are multinomial or Poisson distributed (**Poisson or Log-linear**)
-   Provides a convenient way to connect predictor variables with categorical response variables
-   Basically an extension of general linear models that link the distribution of the response variable to the predictor variables

## Generalized linear models have three components

-   The **random component** that specifies the residual distribution of the response variable in terms of a function of the mean.
-   The **systematic component** represents the linear combination of the predictor variables, and is equivalent to what you’ve learned previously for general linear models
-   The **link function** connects the random and systematic components, and depends on the nature of the random response distribution (e.g. binomial or poisson)

## Assumptions

-   Independence of observations
-   No observations are overly influential
-   Linearity between the response and predictors is not assumed, but the relationship between each of the predictors and the link function is assumed to be linear
-   The dispersion is not extreme (over or under), and fits the assumptions for binomial or Poisson error distributions
-   What if the dispersion assumption is not met?
    -   Can fit ‘quasibinomial’ or ‘quasipoisson’ distributions where the dispersion parameter is derived from the data
    -   Use a Generalized Additive Model (GAM)

# Logistic Regression

## What is Logistic Regression?

-   A **classification** algorithm
-   Used to predict a **binary outcome** (e.g., success/failure)
-   Outputs a **probability** that is mapped to class labels

## Why Not Linear Regression?

-   Linear regression is unbounded
-   Logistic regression uses the **logit** function to map outputs between 0 and 1

$$P(y = 1 \mid x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x)}}$$

## The Logistic Function

```{r}
curve(1 / (1 + exp(-x)), from = -10, to = 10, col = "blue", ylab = "Probability", xlab = "Linear predictor")
```

-   S-shaped (sigmoid) curve
-   Converts linear predictions into probabilities

## Odds and Log-Odds

Logistic regression models log-odds as a linear function

***Odds**:* $$\frac{P}{1 - P}$$

***Log-odds (logit)***:

$$\log\left(\frac{P}{1 - P}\right)$$

## Equivalent Model Equation

$$P(y = 1 \mid x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x)}}$$

$$
\log\left(\frac{P(y=1)}{1 - P(y=1)}\right) = \beta_0 + \beta_1 x
$$

-   Linear in parameters
-   Non-linear in terms of prediction
-   We have **linked** the two sides with the `logit` function

## Fitting the Model {.smaller}

```{r, echo=TRUE}
# Example in R
data(mtcars)
mtcars$am <- factor(mtcars$am)

model <- glm(am ~ mpg, data = mtcars, family = binomial)
summary(model)
```

-   Uses **maximum likelihood estimation**
-   Output includes coefficients for log-odds

## Interpreting Coefficients

-   Coefficients are in **log-odds**
-   To interpret: exponentiate

```{r}
exp(coef(model))
```

-   $\exp(\beta_1)$: multiplicative change in odds for a 1-unit increase in predictor

## Making Predictions {.smaller}

```{r}
predict(model, newdata = data.frame(mpg = 20), type = "response")
```

-   Returns the **probability** that `am = 1` given `mpg = 20`

## Model Evaluation Metrics

To assess the quality of a logistic regression model, we often use:

-   **Confusion matrix**
-   **Accuracy**
-   **Precision**
-   **Recall**
-   **AUC-ROC**

## Confusion Matrix

A 2x2 table comparing predicted vs. actual values:

|                     | Predicted Positive  | Predicted Negative  |
|---------------------|---------------------|---------------------|
| **Actual Positive** | True Positive (TP)  | False Negative (FN) |
| **Actual Negative** | False Positive (FP) | True Negative (TN)  |

Helps visualize performance across all classification outcomes.

## Accuracy

-   Proportion of **correct predictions** out of all predictions:

$$
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
$$

-   Simple but may be **misleading** with imbalanced data.

## Precision

-   Of all predicted positives, how many are actually positive?

$$
\text{Precision} = \frac{TP}{TP + FP}
$$

-   High precision = **low false positive rate**

## Recall (Sensitivity)

-   Of all actual positives, how many were predicted correctly?

$$
\text{Recall} = \frac{TP}{TP + FN}
$$

-   High recall = **low false negative rate**

## Precision vs. Recall

-   **Trade-off**: improving one can reduce the other
-   Use **F1 score** to balance both:

$$
F1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
$$

## AUC-ROC

-   **ROC Curve** plots TPR (Recall) vs. FPR across thresholds
-   **AUC** (Area Under Curve) summarizes performance:
-   Probability the model ranks a random positive higher than a random negative
-   Ranges from **0.5 (random)** to **1.0 (perfect)**

------------------------------------------------------------------------

## R INTERLUDE \| Logistic regression

## Dataset Variable Descriptions {.smaller}

| Variable | Type | Description |
|---------------------------|----------------------|------------------------|
| `biomaterial` | Categorical | Type of biomaterial used in the scaffold (`Material_A`, `Material_B`, `Material_C`) |
| `stiffness` | Numeric | Mechanical stiffness of the scaffold (in kilopascals, kPa) |
| `age_days` | Integer | Age of the scaffold at implantation (in days; range: 20–60) |
| `success` | Binary | Outcome of tissue integration (`1` = successful, `0` = unsuccessful) |

## Fitting the model {.smaller}

```{r}
#| echo: true
#| eval: false
bio_data <- read.csv("BioE_logistic_data.csv", header = TRUE)
model <- glm(success ~ biomaterial + stiffness + age_days, 
             data = bio_data, 
             family = binomial)

summary(model)
```

## Fitting the model {.smaller}

```{r}
#| echo: false
#| eval: true
bio_data <- read.csv("BioE_logistic_data.csv", header = TRUE)
model <- glm(success ~ biomaterial + stiffness + age_days, 
             data = bio_data, 
             family = binomial)

summary(model)
```

## Plotting the model {.smaller}

```{r}
#| echo: true
#| eval: true

ggplot(bio_data, aes(x = stiffness, y = success, color = biomaterial)) +
  geom_point(alpha = 0.4, shape = 1) +
  geom_smooth(method = "glm", method.args = list(family = "binomial"), 
              formula = success ~ biomaterial + stiffness + age_days, se = FALSE, size = 1.2) +
  labs(title = "Predicted Probability of Success",
       x = "Stiffness (kPa)",
       y = "Predicted Probability",
       color = "Biomaterial") +
  theme_minimal()
```

------------------------------------------------------------------------

# Poisson Regression

## What is Poisson Regression?

-   A type of **Generalized Linear Model (GLM)**
-   Used to model **count data** (e.g., number of visits, accidents, events)
-   The outcome variable is assumed to follow a **Poisson distribution**

## The Poisson Distribution

$$ P(Y = y) = \frac{e^{-\lambda} \lambda^y}{y!} $$

-   Mean = Variance = $\lambda$
-   Suitable for modeling counts of **rare events**

## Model Structure

$$ Y_i \sim \text{Poisson}(\lambda_i) $$

$$
\log(\lambda_i) = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}
$$

-   The **log link** ensures ( \lambda\_i \> 0 )
-   Linear model for the **log of expected counts**

## Why Not Use Linear Regression?

-   Linear regression may predict **negative counts**
-   It assumes **constant variance**, but Poisson variance = mean
-   Poisson regression accounts for the distributional structure of count data

## Example

```{r}
#| echo: true
#| eval: false
model <- glm(y ~ x1 + x2, data = something, family = poisson)
```

## Interpreting Coefficients

-   Coefficients are on the **log scale**
-   To interpret, **exponentiate** them:

```{r}
#| eval: false
exp(coef(model))
```

-   $\exp(\beta_j)$: multiplicative change in expected count for a 1-unit increase in predictor $x_j$

## Model Assumptions

1.  **Counts are Poisson-distributed**
2.  **Mean equals variance** (can be relaxed in quasi-Poisson or negative binomial models)
3.  **Log-linear relationship** between predictors and the outcome

## Model Evaluation

-   **Deviance residuals**
-   **Akaike Information Criterion (AIC)**
-   **Overdispersion checks**
-   **Pseudo-**$R^2$

## Overdispersion

-   When variance \> mean
-   Can inflate type I errors
-   Check with:

$$
\text{Dispersion} = \frac{\text{Residual deviance}}{\text{df}}
$$

-   Consider alternatives:

    -   **Quasi-Poisson**
    -   **Negative Binomial Regression**

## Goodness-of-Fit

-   **Deviance**: compares model to saturated model
-   **Pearson residuals**: detect outliers and overdispersion
-   Use `AIC` to compare models

## R INTERLUDE \| Log-linear (Poisson) regression

## Dataset Variable Descriptions (Poisson Model) {.smaller}

| Variable | Type | Description |
|--------------------------------|-------------------|---------------------|
| `biomaterial` | Categorical | Type of biomaterial used in the scaffold |
| `stiffness` | Numeric | Mechanical stiffness in kilopascals (kPa) |
| `age_days` | Integer | Age of scaffold at implantation (20–60 days) |
| `integration_events` | Count | Number of successful tissue integration events (simulated) |

## Fit the model {.smaller}

```{r}
#| echo: true
#| eval: false

# Load data
bio_data <- read.csv("BioE_poisson_data.csv")

# Fit log-linear (Poisson) model
model <- glm(integration_events ~ biomaterial + stiffness + age_days,
             family = poisson, data = bio_data)

summary(model)
```

## Fit the model {.smaller}

```{r}
#| echo: false
#| eval: true

# Load data
bio_data <- read.csv("BioE_poisson_data.csv")

# Fit log-linear (Poisson) model
model <- glm(integration_events ~ biomaterial + stiffness + age_days,
             family = poisson, data = bio_data)

summary(model)
```

## Make a plot {.smaller}

```{r}
#| echo: true
#| eval: false
#| 
library(ggplot2)
library(dplyr)

# Add predicted counts
bio_data$predicted_counts <- predict(model, type = "response")

# Plot predicted vs actual across stiffness for each biomaterial
ggplot(bio_data, aes(x = stiffness, y = predicted_counts, color = biomaterial)) +
  geom_point(aes(y = integration_events), alpha = 0.5, shape = 1) +
  geom_line(size = 1.2) +
  labs(title = "Predicted Integration Events",
       x = "Stiffness (kPa)",
       y = "Predicted Count",
       color = "Biomaterial") +
  theme_minimal()


```

------------------------------------------------------------------------

# 
