---
title: "Week 9b - Statistics for Bioengineering"
author: "Bill Cresko"
format: 
  revealjs:
    transition: fade
    transition-speed: slow
editor: visual
---

```{r}
library(tidyverse)
library(gt)
library(readxl)
library(ggplot2)
library(tibble)
```

## This week

-   Chi-square tests of association and goodness of fit

-   Connections to confusion matrices

-   Basics of statistical (classical) machine learning

-   **Today** - brief presentation to the class of your dataset

-   **Next week** - finish statistical (classical) machine learning including decision trees and random forests

# Machine Learning

## What is Machine Learning?

-   Machine learning is a subset of **artificial intelligence** that enables computers to learn and make decisions from data without being explicitly programmed for every task.

-   **Two main paradigms:**

    -   Statistical (classical) Machine Learning
    -   Deep Learning

## Statistical Machine Learning

-   Traditional approach using statistical methods and algorithms that require manual feature engineering.

-   **Key characteristics:**

    -   Explicit feature extraction
    -   Relatively simple model architectures
    -   Interpretable results
    -   Works well with smaller datasets

## Deep Learning

-   Modern approach using artificial neural networks with multiple layers to automatically learn hierarchical representations.

-   **Key characteristics:**

    -   Automatic feature learning
    -   Complex, multi-layered architectures
    -   Often requires large datasets
    -   High computational requirements

## Common Classical ML Algorithms

-   **Supervised Learning:**
    -   Linear/Logistic Regression
    -   Decision Trees
    -   Random Forest
    -   Support Vector Machines (SVM)
    -   Naive Bayes
    -   K-Nearest Neighbors (KNN)
-   **Unsupervised Learning:**
    -   K-Means Clustering
    -   Hierarchical Clustering
    -   Principal Component Analysis (PCA)

## Common Deep Learning Architectures

-   **Neural Networks:**
    -   Feedforward Neural Networks
    -   Convolutional Neural Networks (CNNs)
    -   Recurrent Neural Networks (RNNs)
    -   Transformer Networks
-   **Applications:**
    -   Computer Vision
    -   Natural Language Processing
    -   Speech Recognition
    -   Generative Models

## 

![](images/Disc_vs_Gen_AI.png)

## Feature Engineering

::::: columns
::: {.column width="50%"}
### Classical ML

-   **Manual feature extraction**
-   Domain expertise required
-   Time-intensive process
-   Features explicitly defined
:::

::: {.column width="50%"}
### Deep Learning

-   **Automatic feature learning**
-   Networks learn features during training
-   Hierarchical feature representation
-   No manual feature engineering needed
:::
:::::

## Model Complexity

::::: columns
::: {.column width="50%"}
### Classical ML

-   **Simpler architectures**
-   Linear models, tree-based methods
-   Logistic Regression, Random Forest, SVM
-   Fewer parameters (hundreds to thousands)
-   Easier to understand and debug
:::

::: {.column width="50%"}
### Deep Learning

-   **Complex architectures**
-   Multiple hidden layers
-   CNNs, RNNs, Transformers
-   Millions to billions of parameters
-   "Black box" nature
:::
:::::

## Data Requirements

::::: columns
::: {.column width="50%"}
### Classical ML

-   **Works with smaller datasets**
-   Hundreds to thousands of samples
-   Can handle tabular data effectively
-   Less prone to overfitting with small data
-   Good performance with limited data
:::

::: {.column width="50%"}
### Deep Learning

-   **Requires large datasets**
-   Thousands to millions of samples
-   Needs big data to avoid overfitting
-   Data-hungry algorithms
-   Performance improves with more data
:::
:::::

# Statistical Machine Learning

## When to Use Statistical ML

-   Small to medium datasets (\< 10,000 samples)
-   Structured/tabular data
-   Need for interpretability
-   Limited computational resources
-   Quick prototyping
-   Regulatory requirements for explainability

## Two General Goals of Statistical ML

-   **Prediction**
    -   generally using linear models such as regression
    -   outcome is a prediction of the response variable value and associated error
-   **Classification**
    -   use logistic regression, random forest, Bayes classifiers, etc...
    -   outcome is a statement about what category each observation belongs

## The Learning Process

1.  **Collect Data**: Features (X) and target (Y)
2.  **Split Data**:
    -   Training set
    -   Validation set
3.  **Select Model**: Choose algorithm, e.g.
    -   linear regression
    -   logistic regression
    -   decision trees
4.  **Train Model**: Fit model to training data
5.  **Evaluate**: Use *independent test data* to assess performance
6.  **Deploy**: Use model to make future predictions

## 

![](images/ml_steps.png)

## Overfitting vs. Underfitting

-   **Underfitting**: Model too simple → high bias
-   **Overfitting**: Model too complex → high variance
-   Use **cross-validation** to estimate test error and tune model complexity
-   Use **independent test data** to estimate generalizability of model

## Model Accuracy Metrics

-   **Prediction**:
    -   Mean Squared Error (MSE)
    -   R²
-   **Classification**:
    -   Accuracy
    -   Precision
    -   Recall
    -   AUC
-   Split data and thus error into:
    -   **Training error**: fit to training set
    -   **Validation error**: fit to the validation set
    -   **Test error**: error generalized to new data

------------------------------------------------------------------------

# Binary classification using logistic regression

## Simulated binary data

```{r}
#| echo: true
#| eval: false

metal_exposure <- runif(200, -2, 2)
p <- 1 / (1 + exp(-(-0.5 + 2 * metal_exposure)))
cancer_diagnosis <- rbinom(200, 1, prob = p)
logit_model <- glm(cancer_diagnosis ~ metal_exposure, family = binomial)
summary(logit_model)
```

## Simulated binary data {.smaller}

```{r}
#| echo: false
#| eval: true

metal_exposure <- runif(200, -2, 2)
p <- 1 / (1 + exp(-(-0.5 + 2 * metal_exposure)))
cancer_diagnosis <- rbinom(200, 1, prob = p)
logit_model <- glm(cancer_diagnosis ~ metal_exposure, family = binomial)
summary(logit_model)
```

## Plotting the Logistic Regression Curve

```{r}
#| echo: true
#| eval: false

cancer_df <- data.frame(metal_exposure, cancer_diagnosis)
cancer_df$prob <- predict(logit_model, type = "response")

ggplot(cancer_df, aes(metal_exposure, cancer_diagnosis)) +
  geom_point(alpha = 0.4) +
  geom_line(aes(y = prob), color = "red") +
  labs(title = "Logistic Regression Fit", 
       x= "Heavy Metal Exposure", 
       y = "Probability of Cancer") +
  theme_minimal()
```

## Plotting Logistic Regression Curve

```{r}
#| echo: false
#| eval: true

cancer_df <- data.frame(metal_exposure, cancer_diagnosis)
cancer_df$prob <- predict(logit_model, type = "response")

ggplot(cancer_df, aes(metal_exposure, cancer_diagnosis)) +
  geom_point(alpha = 0.4) +
  geom_line(aes(y = prob), color = "red") +
  labs(title = "Logistic Regression Fit", 
       x= "Heavy Metal Exposure", 
       y = "Probability of Cancer") +
  theme_minimal()
```

# Performing machine learning using logistic regression

## Making predictions using the Iris dataset {.smaller}

-   we're using the `tidymodels` package for splitting the data
-   Here we're just creating binary variable for whether the plant is or isn't a *setosa*

```{r}
#| echo: true
#| eval: true

library(tidymodels)

iris <- iris %>% 
  mutate(setosa = as.integer(Species == "setosa"))

```

## Split data into training and validation sets {.smaller}

-   This function allows you to split the data into test vs. training data objects
-   Note how we make the `split` data object which contains both groups of data
-   Then we pull those out into two different data sets

```{r}
#| echo: true
#| eval: true

split <- initial_split(iris, 
                       prop = .80, 
                       strata = setosa) 
iris_train <- training(split)
iris_test <- testing(split)

```

-   Take a look at those new data objects in R Studio. How many observations are in each, and how many observations are there in the original data set?

## Visualize sepal length to prob of setosa

-   Note that this is another way to visualize the logit function
-   Does it look like sepal length can predict whether a plant is a setosa?

```{r}
#| echo: true
#| eval: false

ggplot(iris_train, aes(x = Sepal.Length,
                       y = setosa)) +
  geom_jitter(height = .05, 
              alpha = .5) +
  geom_smooth(method = "glm",
              method.args = list(family = "binomial"),
              se = TRUE) +
  theme_minimal()
```

## Visualize sepal length to prob of setosa

```{r}
#| echo: false
#| eval: true

ggplot(iris_train, aes(x = Sepal.Length,
                       y = setosa)) +
  geom_jitter(height = .05, 
              alpha = .5) +
  geom_smooth(method = "glm",
              method.args = list(family = "binomial"),
              se = TRUE) +
  theme_minimal()
```

## Build a model with training data

```{r}
#| echo: true
#| eval: false

training_model <- glm(setosa ~ Sepal.Length,
             data = iris_train,
             family = "binomial")
summary(training_model)
```

## Build a model with training data {.smaller}

```{r}
#| echo: false
#| eval: true

training_model <- glm(setosa ~ Sepal.Length,
             data = iris_train,
             family = "binomial")
summary(training_model)
```

## Evaluate the model

-   First we create predictions of the validation data from the training model
-   Note that the output is a probability, but we just turn it into a binary classification

```{r}
#| echo: true
#| eval: true
#| 
iris_test <- iris_test %>% 
  mutate(setosa_prob = predict(training_model,
                               iris_test,
                               type = "response"),
         setosa_pred = ifelse(setosa_prob > .5, 1, 0))
```

-   Take a look at the `iris_test` data object

-   You should now see the `setosa_pred` variable

## Evaluate the model

-   Now let's create a confusion matrix to see how well the trained model predicted on the validation data set

```{r}
#| echo: true
#| eval: false

setosa_conf_matrix <- table(iris_test$setosa,
           iris_test$setosa_pred)

as.data.frame.matrix(setosa_conf_matrix) %>%
  gt(rownames_to_stub = TRUE) %>%
  tab_header(title = "Setosa Confusion Matrix") %>%
  tab_stubhead(label = "Actual") %>%
  tab_spanner(label = "Predicted", columns = everything())
```

## Evaluate the model

```{r}
#| echo: false
#| eval: true

setosa_conf_matrix <- table(iris_test$setosa,
           iris_test$setosa_pred)

as.data.frame.matrix(setosa_conf_matrix) %>%
  gt(rownames_to_stub = TRUE) %>%
  tab_header(title = "Setosa Confusion Matrix") %>%
  tab_stubhead(label = "Actual") %>%
  tab_spanner(label = "Predicted", columns = everything())
```

## What Is a Confusion Matrix?

-   A **confusion matrix** summarizes the outcomes of a classification task.
-   It compares **predicted** labels to **actual** (true) labels.
-   Helps identify **where** the model is getting things right or wrong.
-   Example for binary classification:

## Confusion Matrix Layout (Binary)

|                      | **Predicted: Positive** | **Predicted: Negative** |
|----------------------|-------------------------|-------------------------|
| **Actual: Positive** | True Positive (TP)      | False Negative (FN)     |
| **Actual: Negative** | False Positive (FP)     | True Negative (TN)      |

\

-   **TP**: Correctly predicted positive case
-   **TN**: Correctly predicted negative case
-   **FP**: Incorrectly predicted positive (Type I error)
-   **FN**: Incorrectly predicted negative (Type II error)

## Key Metrics for Classification {.smaller}

$$ 
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} 
$$\

$$
\text{Precision} = \frac{TP}{TP + FP}
$$\

$$
\text{Sensitivity} = \frac{TP}{TP + FN}
$$\

$$
\text{Specificity} = \frac{TN}{TN + FP}
$$\

$$
\text{F1_score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
$$

## Key Metrics for Classification

::: callout-warning

-   There is no single best evaluation metric for model fit for prediction

-   How important is it to identify true positives and what are the costs of false positives?

-   How important is it to identify true negatives, and what are the costs of false negatives?

-   Both of these need to be determined based upon the use of the classification
:::

::: callout-note
Different types of evaluation are used for prediction (e.g. regression) models
:::


## Evaluate the model

-   Now let's calculate various metrics for the setosa analysis

```{r}
#| echo: true
#| eval: false

tn <- setosa_conf_matrix[1,1]  # True Negative
fp <- setosa_conf_matrix[1,2]  # False Positive  
fn <- setosa_conf_matrix[2,1]  # False Negative
tp <- setosa_conf_matrix[2,2]  # True Positive

# Calculate metrics
metrics_df <- data.frame(
  Metric = c("Accuracy", "Precision", "Recall", "Specificity", "F1-Score"),
  Value = c(
    (tp + tn) / (tp + tn + fp + fn),           # Accuracy
    tp / (tp + fp),                            # Precision
    tp / (tp + fn),                            # Recall/Sensitivity
    tn / (tn + fp),                            # Specificity
    2 * (tp / (tp + fp)) * (tp / (tp + fn)) / 
    ((tp / (tp + fp)) + (tp / (tp + fn)))      # F1-Score
  )
)

# Create simple gt table
metrics_df %>%
  gt() %>%
  tab_header(title = "Classification Evaluation Metrics") %>%
  fmt_number(columns = Value, decimals = 3)

```

## Evaluate the model

```{r}
#| echo: false
#| eval: true

tn <- setosa_conf_matrix[1,1]  # True Negative
fp <- setosa_conf_matrix[1,2]  # False Positive  
fn <- setosa_conf_matrix[2,1]  # False Negative
tp <- setosa_conf_matrix[2,2]  # True Positive

# Calculate metrics
metrics_df <- data.frame(
  Metric = c("Accuracy", "Precision", "Recall", "Specificity", "F1-Score"),
  Value = c(
    (tp + tn) / (tp + tn + fp + fn),           # Accuracy
    tp / (tp + fp),                            # Precision
    tp / (tp + fn),                            # Recall/Sensitivity
    tn / (tn + fp),                            # Specificity
    2 * (tp / (tp + fp)) * (tp / (tp + fn)) / 
    ((tp / (tp + fp)) + (tp / (tp + fn)))      # F1-Score
  )
)

# Create simple gt table
metrics_df %>%
  gt() %>%
  tab_header(title = "Classification Evaluation Metrics") %>%
  fmt_number(columns = Value, decimals = 3)

```

------------------------------------------------------------------------

# Cross-Validation with the boot Library in R

## What is Cross-Validation?

Cross-validation is a statistical technique for assessing how well a model generalizes to new data by repeatedly splitting your dataset into training and testing portions.

**The Process:**

1.  Split your data into k groups (folds)
2.  Train your model on k-1 folds\
3.  Test on the remaining fold
4.  Repeat this process k times
5.  Average the results to get a robust estimate of model performance

## The boot Library

The `boot` library's main function is `cv.glm()`, which performs cross-validation for generalized linear models.

```{r}
library(boot)

# Example with linear regression
model <- glm(mpg ~ wt + hp, data = mtcars)

# Perform 10-fold cross-validation
cv_result <- cv.glm(mtcars, model, K = 10)

# Extract cross-validation error
cv_error <- cv_result$delta[1]
```

## Key Parameters

**K parameter** - Controls the number of folds:

-   `K = nrow(data)` → Leave-one-out cross-validation (LOOCV)
-   `K = 10` → 10-fold CV (common choice)
-   `K = 5` → 5-fold CV

**Custom cost functions** - Specify different loss functions:

```{r}
# Custom cost function (mean absolute error)
mae_cost <- function(y, yhat) mean(abs(y - yhat))

cv_result <- cv.glm(mtcars, model, K = 10, cost = mae_cost)
```

## Practical Example: Model Comparison

```{r}
library(boot)
data(mtcars)

# Fit different models
model1 <- glm(mpg ~ wt, data = mtcars)
model2 <- glm(mpg ~ wt + hp, data = mtcars)
model3 <- glm(mpg ~ wt + hp + disp, data = mtcars)

# Compare models using 10-fold CV
cv1 <- cv.glm(mtcars, model1, K = 10)
cv2 <- cv.glm(mtcars, model2, K = 10)
cv3 <- cv.glm(mtcars, model3, K = 10)

# Compare cross-validation errors
cv_errors <- c(cv1$delta[1], cv2$delta[1], cv3$delta[1])
names(cv_errors) <- c("Model 1", "Model 2", "Model 3")
print(cv_errors)
```

## Understanding the Output

The `cv.glm()` function returns:

-   **`delta[1]`**: Raw cross-validation estimate
-   **`delta[2]`**: Bias-corrected cross-validation estimate\
-   **`K`**: Number of folds used

**Interpretation:** Lower cross-validation error = better expected performance on new data

## Advantages of the boot Library

✓ Automatically handles data splitting

✓ Ensures each observation appears in exactly one test fold per iteration

✓ Provides both biased and bias-corrected estimates

✓ Well-suited for GLMs

✓ Integrates seamlessly with R's modeling framework

------------------------------------------------------------------------
