---
title: "Week 6a - Statistics for Bioengineering"
author: "Bill Cresko"
format: 
  revealjs:
    transition: fade
    transition-speed: slow
editor: visual
---

```{r}
library(tidyverse)
library(gt)
library(readxl)
library(ggplot2)
```

## This week

-   Model 1 and 2 regression

-   Multiple linear regression

-   Analysis of Variance (ANOVA)

-   Key principles of experimental design

-   How to report statistics in papers

# Linear Models and Regression

## General linear model - two or more continuous variables

$$ y_i = \beta_0 + \beta_1 x_1 + \epsilon_i $$

```{r, echo=FALSE, out.width='90%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_4b.008.jpeg")
```

$$ y_i = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_n x_n + \epsilon_i $$

$$ y_i = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1x_2 + \epsilon_i $$

## Generalized linear models - non-linear in the coefficients {.smaller}

-   **Polynomial regression** $$y = \beta_0 + \beta_1 x + \beta_2 x^2 +...+ \beta_n x^n +\epsilon$$

-   **Logistic regression** $$log(\frac{\pi_i}{1-\pi_1})=\beta_0 + \beta_1 x_1 + \beta_2 x_2 +...+ \beta_n x_n +\epsilon$$

-   **Poisson regression** $$log(y) = \beta_0 + \beta_1 x + \epsilon$$

-   **Exponential regression** $$y = \alpha^{\beta x} + \epsilon$$

-   **Power regression** $$y = \alpha x^\beta + \epsilon$$

# Model 1 and Model 2 regression

## Hypothesis tests in regression {.smaller}

$$H_0 : \beta_0 = 0$$ $$H_A : \beta_1 \neq 0$$

reduced model ($H_0$) -

$$y_i = \beta_0 + 0x_i + \epsilon_i$$

full model ($H_A$) -

$$y_i = \beta_0 + \beta_1x_i + \epsilon_i$$\
\

1.  fits a “reduced” model without slope term (H0)
2.  fits the “full” model with slope term added back
3.  compares fit of full and reduced models using an ***F test statistic***
4.  compares the residual variance of the full vs. reduced models

## 

```{r, echo=FALSE, out.width='90%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_5a.001.jpeg")
```

## 

```{r, echo=FALSE, out.width='90%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_5a.002.jpeg")
```

## 

```{r, echo=FALSE, out.width='90%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_5a.003.jpeg")
```

------------------------------------------------------------------------

## R INTERLUDE \| Model I and II regression {.smaller}

-   First we fit a standard model I ordinary least squares

```{r, echo=TRUE, eval=TRUE}

zfish_data <- read.table("Drerio_development_complete.csv", header=T, sep=",")

length <- zfish_data$Length_cm
weight <- zfish_data$Weight_mg

size_lm <- lm(weight~length)
summary(size_lm)
```

## R INTERLUDE \| Model I and II regression {.smaller}

-   We can fit the line

```{r, echo=TRUE, eval=TRUE}
library(ggplot2)

ggplot(size_lm, aes(x = weight, y = length)) +
  geom_point(color = "blue", size = 2, alpha = 0.2) +                  
  geom_smooth(method = "lm", color = "red", se = TRUE) +
  labs(
    title = "Linear regression of zebrafish weight on standard length",
    x = "zebrafish length in cm",
    y = "zebrafish weight in gm"
  ) +
  theme_minimal()
```

## R INTERLUDE \| Model I and II regression {.smaller}

-   We can fit a `LOESS` line

```{r, echo=TRUE, eval=TRUE}
library(ggplot2)

ggplot(size_lm, aes(x = weight, y = length)) +
  geom_point(color = "blue", size = 2, alpha = 0.2) +                  
  geom_smooth(method = "loess", color = "red", se = TRUE) +
  labs(
    title = "Linear regression of zebrafish weight on standard length",
    x = "zebrafish length in cm",
    y = "zebrafish weight in gm"
  ) +
  theme_minimal()
```

## Smoothers and local regression {.smaller}

-   **loess** (locally estimated scatterplot smoothing) - fit least squares regression lines to successive subsets of the observations weighted according to their distance from the focal observation.\
-   **kernel smoothers** - new smoothed y-values are computed as the weighted averages of points within a defined window of the original x-values. Weightings are determined by the type of kernel smoother specified, the larger the window, the greater the degree of smoothing.\
-   **splines** - joins together a series of polynomial fits that have been generated after the data is split up into a number of smaller windows.

## R INTERLUDE \| Model I and II regression {.smaller}

```{r, echo=TRUE, eval=TRUE}
library(lmodel2)

size_lm_ModII <- lmodel2(weight ~ length)
size_lm_ModII
```

-   You should also have generated `OLS (Ordinary Least Squares)`, `MA (Moving Average)`, and `SMA (Standard Major Axis)` regression models, and the last plots SMA line from parameter estimates

## R INTERLUDE - on your own {.smaller}

-   Say you measured green fluorescent protein abundance in cell culture across several key, carefully controlled temperatures.

-   You ultimately want to know whether protein expression changes as a function of temperature in your experiment.

-   Read in the `gfp_temp.tsv` data and perform a regression analysis.

-   Address the following questions

    -   Which is X and which is Y?
    -   Are residual assumptions met?
    -   What model of regression should we use in this case and why?
    -   What is our decision regarding our null hypothesis of no relationship?

# Mutliple Linear regression

## Multiple Linear Regression - Goals

-   To develop a better predictive model than is possible from models based on single independent variables.

-   To investigate the relative individual effects of each of the multiple independent variables above and beyond the effects of the other variables.

-   The individual effects of each of the predictor variables on the response variable can be depicted by single partial regression lines.

-   The slope of any single partial regression line (partial regression slope) thereby represents the rate of change or effect of that specific predictor variable (holding all the other predictor variables constant to their respective mean values) on the response variable.

## Multiple Linear Regression \| Additive and multiplicative models of 2 or more predictors

Additive model $$y_i = \beta_0 + \beta_1x_{1} + \beta_2x_{2} + ... + \beta_jx_{j} + \epsilon_i$$

\

Multiplicative model (with two predictors) $$y_i = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_1x_2 + \epsilon_i$$

## Multiple Linear Regression \| Additive and multiplicative models

<br>

```{r, echo=FALSE, out.width='100%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_5a.006.jpeg")
```

## Multiple linear regression assumptions

<br>

-   linearity
-   normality
-   homogeneity of variance
-   The absence of strong **multi-collinearity**
    -   a predictor variable must not be linearly predicted by - or correlated with - any combination of the other predictor variables.

## checking for multi-collinearity

```{r, echo=TRUE, eval=TRUE}

library(car)
zfish_data <- read.table("Drerio_development_complete.csv", header=T, sep=",")

length <- zfish_data$Length_cm
weight <- zfish_data$Weight_mg
age <- zfish_data$Age_Days
pigment <- zfish_data$Pigmentation


scatterplotMatrix(~length+pigment+age, diag="boxplot")
```

## R INTERLUDE \| multiple regression with 2 predictor variables {.smaller}

-   Read in the `RNAseq_lipid.tsv` and examine the continuous variables.
-   We are interested in whether Gene01 and/or Gene02 expression levels influence lipid content.
-   First plot $Y$ vs $Gene01$, $Y$ vs $Gene02$, then set up and test a multiplicative model.
-   What are the parameter estimates of interest in this case?
-   What are the outcomes from our hypothesis tests?

## R INTERLUDE \| multiple regression with 2 predictor variables {.smaller}

```{r, echo=TRUE, eval=TRUE}
RNAseq_Data <- read.table("RNAseq_lipid.tsv", header=T, sep="\t")

y <- RNAseq_Data$Lipid_Conc
g1 <- RNAseq_Data$Gene01
g2 <- RNAseq_Data$Gene02
g3 <- RNAseq_Data$Gene03
g4 <- RNAseq_Data$Gene04

Mult_lm <- lm(y ~ g1*g2)
summary(Mult_lm)
```

## R INTERLUDE \| multiple regression with 2 predictor variables {.smaller}

-   Now get rid of the interaction term, and set up a purely additive model
-   Did any of our estimates change? Why?
-   Did the degrees of freedom change? Why?

```{r, echo=TRUE, eval=FALSE}
Add_lm <- lm(y ~ g1+g2)
summary(Add_lm)
```

-   Also try different combinations of genes
-   Finally, see if you can make a partial regression plot of just one predictor and the response variable

## R INTERLUDE \| adding a polynomial {.smaller}

```{r, echo=TRUE, eval=TRUE}
Poly_lm_1 <- lm(y ~ poly(g1, 1))
summary(Poly_lm_1)
```

## R INTERLUDE \| adding a polynomial {.smaller}

```{r, echo=TRUE, eval=TRUE}
Poly_lm_2 <- lm(y ~ poly(g1, 2))
summary(Poly_lm_2)
```

## R INTERLUDE \| adding a polynomial {.smaller}

```{r, echo=TRUE, eval=TRUE}
Poly_lm_3 <- lm(y ~ poly(g1, 3))
summary(Poly_lm_3)
```

## R INTERLUDE \| adding a polynomial {.smaller}

```{r, echo=TRUE, eval=TRUE}
Poly_lm_10 <- lm(y ~ poly(g1, 10))
summary(Poly_lm_10)
```

# Model selection when you have many predictor variables

## Model selection

```{r, echo=FALSE, out.width='90%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_5b.004.jpeg")
```

## Model selection \| the problems

-   How to decide the complexity of polynomial: straight line regression, quadratic, cubic, ....

-   Which variables to keep/ discard when building a multiple regression model?

-   Selecting from candidate models representing different biological processes.

## Model selection \| a beetle example

```{r, echo=FALSE, out.width='80%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_5b.005.jpeg")
```

## Start with linear regression

```{r, echo=FALSE, out.width='80%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_5b.006.jpeg")
```

## Quadratic (2nd degree) polynomial?

```{r, echo=FALSE, out.width='80%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_5b.007.jpeg")
```

## Quadratic (3rd degree) polynomial?

```{r, echo=FALSE, out.width='80%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_5b.008.jpeg")
```

## A polynomial of degree 5?

```{r, echo=FALSE, out.width='80%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_5b.009.jpeg")
```

## A polynomial of degree 10?

```{r, echo=FALSE, out.width='80%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_5b.010.jpeg")
```

## The problem with this approach

-   The log likelihood of the model increases with the number of parameters
-   So does $r^2$
-   Isn't this good - the best fit to the data?

```{r, echo=FALSE, out.width='70%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_5b.011.jpeg")
```

## The problem with this approach

-   Does it violate a principle of parsimony
-   Fit no more parameters than is necessary.
-   If two or more models fit the data almost equally well, prefer the simpler model.

> "models should be pared down until they are minimal and adequate”

Crawley 2007, p325

## Let's consider our objectives

-   Model should predicts well

-   Approximates true relationship between the variables

-   Be able to evaluate a wider array of models. Not only or more “reduced” models.

-   But an overfitted model may not predict well in the future!

## A polynomial of degree 10?

```{r, echo=FALSE, out.width='80%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_5b.010.jpeg")
```

##  {.flexbox .vcenter}
