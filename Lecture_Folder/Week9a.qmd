---
title: "Week 9a - Statistics for Bioengineering"
author: "Bill Cresko"
format: 
  revealjs:
    transition: fade
    transition-speed: slow
editor: visual
---

```{r}
library(tidyverse)
library(gt)
library(readxl)
library(ggplot2)
library(tibble)
```

## This week

-   Chi-square tests of association and goodness of fit

-   Connections to confusion matrices

-   Basics of statistical (classical) machine learning

-   **Tomorrow** - homework 3 due

-   **On Thursday** - brief presentation to the class of your dataset

-   **Next week** - finish statistical (classical) machine learning

-   **Special session** - Git/GitHub and Talapas

## Analyzing categorical variables

- How do we test **frequencies** of a categorical variable against **what we expect**?

- How do we test for **correlation** among two or more categorical variables?

- How do we measure the **effect size of the correlation** of two or more categorical variables?

- These each involve **contingency tables** and the **$\chi^2$ distribution**


## What is the $\chi^2$ Distribution ?

-   How do we perform statistical tests on **one or more** categorical variables?

-   The **Chi-square distribution** (χ²) is a continuous probability distribution.

-   It arises when you **sum the squares** of independent standard normal variables.

-   It is widely used in **statistical tests** and in **machine learning** for evaluation

## $\chi^2$ Distribution Definition

-   Start with a set of variables $X_1, X_2, ..., X_k$
-   Then take the **z-scores** for each variable

If $Z_1, Z_2, ..., Z_k$ are independent standard normal variables, then:

$$
\chi^2_k = \sum_{i=1}^k Z_i^2
$$

-   $\chi^2_k$ denotes a chi-square distribution with $k$ degrees of freedom (df).
-   The shape depends on the degrees of freedom.
-   As the mean increases it approaches a normal distribution (via Central Limit Theorem)

## 

```{r}
#| label: chi-square-plot
#| echo: false
#| fig-cap: "Chi-square distribution for different degrees of freedom"
library(ggplot2)

df_vals <- c(1, 2, 5, 10)
x <- seq(0, 30, length.out = 300)
df_data <- lapply(df_vals, function(k) data.frame(x = x, y = dchisq(x, df = k), df = paste0("df = ", k)))
df_data <- do.call(rbind, df_data)

ggplot(df_data, aes(x = x, y = y, color = df)) +
  geom_line(size = 1.2) +
  labs(title = "Chi-square Distribution",
       x = "x", y = "Density") +
  theme_minimal()
```

## $\chi^2$ Distribution Properties

-   **Domain**: $x \in [0, \infty)$
-   **Mean**: $k$
-   **Variance**: $2k$
-   **Right-skewed** distribution

## $\chi^2$ Distribution Probability Density Function

$$
f(x; k) = \frac{1}{2^{k/2} \Gamma(k/2)} x^{k/2 - 1} e^{-x/2}, \quad x > 0
$$

-   Where $\Gamma$ is the gamma function.
-   This function defines the shape of the chi-square distribution.
-   The gamma function is linked to the gamma distribution

## **Gamma Distribution**

-   a continuous probability distribution
-   used to model the times that elapses before $\alpha$ occurrences of a randomly occurring event
-   e.g. calls to a pizza place or defects on a production line
-   such events are said to occur via a Poisson Process

## **Gamma Distribution**

-   Describes the waiting time until the $r^{th}$ event for a process that occurs randomly over time at a rate $\lambda$ :

<br>

$$f(x) = \frac{e^{-\lambda x}\lambda x^{r-1}}{(r-1)!}\lambda$$

<br>

$$ Mean =  \frac{r}{\lambda} $$ $$ Variance = \frac{r}{\lambda^2} $$

## Gamma Distribution

$$
f(x; \alpha, \lambda) = 
\frac{\lambda^{\alpha}}{\Gamma(\alpha)} x^{\alpha - 1} e^{-\lambda x}, \quad x > 0
$$

-   with shape parameter $\alpha$ \> 0 and rate parameter $\lambda$ \> 0

$$
f(x; \alpha, \theta) = 
\frac{1}{\Gamma(\alpha) \theta^\alpha} x^{\alpha - 1} e^{-x/\theta}, \quad x > 0
$$

-   with scale parameter $\theta = \frac{1}{\lambda}$

## Gamma Distribution

<br>

-   **Example**: If, in a PCR reaction, DNA polymerase synthesizes new DNA strands at a rate of 1 per millisecond, how long until 1000 new DNA strands are produced?

<br>

-   **Assume**: that DNA synthesis does not deplete the pool of primers or nucleotides in the chamber, so that each event is independent of other events in the PCR chamber.

## Uses of the $\chi^2$ distribution

-   **Goodness-of-fit tests**: Does observed data match expected proportions?

-   **Test of independence**: Are two categorical variables related?

-   **Test of homogeneity**: Do different groups have the same distribution?

_________________________

# Goodness of Fit Test

## What is a $\chi^2$ Goodness-of-Fit Test?

-   A **Chi-square Goodness-of-Fit Test** checks whether observed frequencies differ from expected frequencies.
-   Used when:
    -   You have one categorical variable.
    -   You want to test whether it fits a specified distribution.

## Hypotheses

-   **Null hypothesis (H₀)**: Observed data follows the expected distribution.
-   **Alternative hypothesis (H₁)**: Observed data does not follow the expected distribution.


## When to use this test

-   **Marketing**: preference among product categories
-   **Biology**: expected vs observed phenotypes
-   **Education**: survey responses by category

## Assumptions

-   Data are **counts** (not percentages or proportions).
-   Categories are **mutually exclusive**.
-   Expected count in each category **should be ≥ 5** .

## $\chi^2$ Test Statistic

The test statistic is:

$$
\chi^2 = \sum \frac{(O_i - E_i)^2}{E_i}
$$

Where:\
$O_i$ = Observed frequency\
$E_i$ = Expected frequency

Degrees of freedom:\
$$
df = k - 1
$$\
where ( k ) = number of categories

## Example Scenario

A geneticist expects a 3:1 Mendelian ratio in offspring genotypes.

Observed counts:

-   Dominant: 72
-   Recessive: 28

Expected proportions:

-   Dominant: 0.75
-   Recessive: 0.25

## $\chi^2$ Goodness of Fit by hand in R

```{r}
#| label: gof-example-show
#| echo: true
#| eval: false
#| message: false
#| warning: false

# Observed data
observed <- c(Dominant = 72, Recessive = 28)

# Expected proportions
expected_probs <- c(0.75, 0.25)

# Expected counts
expected <- sum(observed) * expected_probs

# Chi-square test manually
chisq_stat <- sum((observed - expected)^2 / expected)
chisq_stat  # Output: Chi-square statistic

# Degrees of freedom
df <- length(observed) - 1

# Critical value at alpha = 0.05
critical_val <- qchisq(0.95, df)
critical_val

# P-value
p_val <- pchisq(chisq_stat, df, lower.tail = FALSE)
p_val

# Alternatively, use chisq.test()
chisq.test(x = observed, p = expected_probs)
```

## $\chi^2$ Goodness of Fit by hand in R

```{r}
#| label: gof-example-do
#| echo: false
#| eval: true
#| message: false
#| warning: false

# Observed data
observed <- c(Dominant = 72, Recessive = 28)

# Expected proportions
expected_probs <- c(0.75, 0.25)

# Expected counts
expected <- sum(observed) * expected_probs

# Chi-square test manually
chisq_stat <- sum((observed - expected)^2 / expected)
chisq_stat  # Output: Chi-square statistic

# Degrees of freedom
df <- length(observed) - 1

# Critical value at alpha = 0.05
critical_val <- qchisq(0.95, df)
critical_val

# P-value
p_val <- pchisq(chisq_stat, df, lower.tail = FALSE)
p_val

# Alternatively, use chisq.test()
chisq.test(x = observed, p = expected_probs)
```

## $\chi^2$ Parametric Test

- rolling a six-sided die 60 times

```{r}
#| label: gof-example-dice-show
#| echo: true
#| eval: false

observed <- c(8, 9, 10, 12, 11, 10)

expected_probs <- rep(1/6, 6)

chisq.test(x = observed, p = expected_probs)
```

## $\chi^2$ Parametric Test

- rolling a six-sided die 60 times

```{r}
#| label: gof-example-dice-tell
#| echo: false
#| eval: true
#| message: false
#| warning: false
# Observed frequencies
observed <- c(8, 9, 10, 12, 11, 10)

# Expected probabilities for a fair die
expected_probs <- rep(1/6, 6)

# Perform the Chi-square goodness-of-fit test
chisq.test(x = observed, p = expected_probs)
```

## $\chi^2$ goodness of fit test using resampling

```{r}
#| label: gof-example-resampling-show
#| echo: true
#| eval: false

set.seed(123)  # For reproducibility

# Observed data
observed <- c(Dominant = 72, Recessive = 28)
n <- sum(observed)

# Expected probabilities
expected_probs <- c(0.75, 0.25)

# Number of simulations
n_sim <- 10000

# Simulate chi-square statistics under the null
sim_chisq <- replicate(n_sim, {
  simulated <- rmultinom(1, size = n, prob = expected_probs)
  expected <- n * expected_probs
  sum((simulated - expected)^2 / expected)
})

# Observed chi-square statistic
obs_chisq <- sum((observed - n * expected_probs)^2 / (n * expected_probs))

# Empirical p-value
p_value <- mean(sim_chisq >= obs_chisq)

# Output results
cat("Observed Chi-square:", obs_chisq, "\n")
cat("Empirical P-value:", p_value, "\n")
```

## $\chi^2$ goodness of fit test using resampling

```{r}
#| label: gof-example-resampling-tell
#| echo: false
#| eval: true

set.seed(123)  # For reproducibility

# Observed data
observed <- c(Dominant = 72, Recessive = 28)
n <- sum(observed)

# Expected probabilities
expected_probs <- c(0.75, 0.25)

# Number of simulations
n_sim <- 10000

# Simulate chi-square statistics under the null
sim_chisq <- replicate(n_sim, {
  simulated <- rmultinom(1, size = n, prob = expected_probs)
  expected <- n * expected_probs
  sum((simulated - expected)^2 / expected)
})

# Observed chi-square statistic
obs_chisq <- sum((observed - n * expected_probs)^2 / (n * expected_probs))

# Empirical p-value
p_value <- mean(sim_chisq >= obs_chisq)

# Output results
cat("Observed Chi-square:", obs_chisq, "\n")
cat("Empirical P-value:", p_value, "\n")
```

## $\chi^2$ goodness of fit null distribution

```{r}
hist(sim_chisq, breaks = 50, main = "Resampling Null Distribution",
     xlab = "Chi-square statistic", col = "lightblue", border = "white")
abline(v = obs_chisq, col = "red", lwd = 2)
legend("topright", legend = "Observed", col = "red", lwd = 2)
```

## On your own

-   Change around the data in the examples above and see how it affects the results
-   Take your term-long data set or one of the data sets we've examined in class
-   Create a hypothesis of expected proportions and test it
-   Create a side-by-side plot of observed and expected frequencies

------------------------------------------------------------------------

# ## $\chi^2$ test of association

## What is a $\chi^2$ Test of Association?

-   A statistical test to determine whether **two categorical variables** are associated (not independent).

-   **Null hypothesis (H₀)**: The two variables are **independent**.

-   **Alternative hypothesis (H₁)**: There is an **association** between the variables.

## Assumptions

-   Observations are independent.
-   Expected counts ≥ 5 in at least 80% of cells.
-   No more than 20% of cells have expected counts \< 5.

## Example Scenario

A researcher surveys students about their study preference:

|                | Morning | Evening |
|----------------|---------|---------|
| Undergraduates | 30      | 20      |
| Graduates      | 10      | 40      |

-   Are study time preference and academic level associated?

## Performing a $\chi^2$ test of association in R

```{r}
#| label: assoc-test-show
#| echo: true
#| eval: false
#| message: false
#| warning: false

# Create contingency table
study_table <- matrix(c(30, 20, 10, 40),
                      nrow = 2,
                      byrow = TRUE)

# Add row and column names
rownames(study_table) <- c("Undergraduate", "Graduate")
colnames(study_table) <- c("Morning", "Evening")

# View the table
study_table

# Perform Chi-square test of independence
chisq.test(study_table)

```

## Performing a $\chi^2$ test of association in R

```{r}
#| label: assoc-test-tell
#| echo: false
#| eval: true
#| message: false
#| warning: false

# Create contingency table
study_table <- matrix(c(30, 20, 10, 40),
                      nrow = 2,
                      byrow = TRUE)

# Add row and column names
rownames(study_table) <- c("Undergraduate", "Graduate")
colnames(study_table) <- c("Morning", "Evening")

# View the table
study_table

# Perform Chi-square test of independence
chisq.test(study_table)

```

## Paired Histogram

```{r}
#| label: assoc-plot-show
#| echo: true
#| eval: false
#| fig-cap: "Study Preferences by Student Type"
library(ggplot2)

df <- as.data.frame(as.table(study_table))
ggplot(df, aes(x = Var2, y = Freq, fill = Var1)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Study Time", y = "Number of Students", fill = "Student Type") +
  theme_minimal()
```

## Paired Histogram

```{r}
#| label: assoc-plot-tell
#| echo: false
#| eval: true
#| fig-cap: "Study Preferences by Student Type"
library(ggplot2)

df <- as.data.frame(as.table(study_table))
ggplot(df, aes(x = Var2, y = Freq, fill = Var1)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Study Time", y = "Number of Students", fill = "Student Type") +
  theme_minimal()
```

## Mosaic Plot

```{r}
#| label: mosaic-plot-show
#| echo: true
#| eval: false
#| fig-cap: "Mosaic Plot of Study Preference by Student Type"

mosaicplot(study_table,
           main = "Study Time Preference by Student Type",
           xlab = "Student Type",
           ylab = "Study Preference",
           color = TRUE)
```

## Mosaic Plot

```{r}
#| label: mosaic-plot-tell
#| echo: false
#| eval: true
#| fig-cap: "Mosaic Plot of Study Preference by Student Type"

mosaicplot(study_table,
           main = "Study Time Preference by Student Type",
           xlab = "Student Type",
           ylab = "Study Preference",
           color = TRUE)
```

## Calculating the effect size using Cramér's V

$$
V = \sqrt{\frac{\chi^2}{n(k-1)}}
$$ where

-   $\chi^2$ is the test statistic
-   n is the total number of observations
-   k is the min(number of rows, number of columns)
-   ranges from 0 to 1, with
    -   0.1 being a small effect
    -   0.3 being a medium effect
    -   0.5 being a large effect

## Calculating the effect size in R

```{r}
#| label: cramers_test_show
#| echo: true
#| eval: false

# Example 2x2 contingency table
tbl <- matrix(c(500, 10, 20, 40), nrow = 2, byrow = TRUE)
rownames(tbl) <- c("Group1", "Group2")
colnames(tbl) <- c("Outcome1", "Outcome2")

# Chi-square test of independence
test <- chisq.test(tbl)

# Extract test statistic
chisq_val <- test$statistic

# Total number of observations
n <- sum(tbl)

# Minimum of (rows, columns) for Cramér’s V formula
k <- min(nrow(tbl), ncol(tbl))

# Cramér’s V calculation
cramers_v <- sqrt(chisq_val / (n * (k - 1)))

# Print result
cramers_v
```

## Calculating the effect size in R

```{r}
#| label: cramers_test_tell
#| echo: false
#| eval: true


# Example 2x2 contingency table
tbl <- matrix(c(50, 10, 20, 40), nrow = 2, byrow = TRUE)
rownames(tbl) <- c("Group1", "Group2")
colnames(tbl) <- c("Outcome1", "Outcome2")

# Chi-square test of independence
test <- chisq.test(tbl)

# Extract test statistic
chisq_val <- test$statistic

# Total number of observations
n <- sum(tbl)

# Minimum of (rows, columns) for Cramér’s V formula
k <- min(nrow(tbl), ncol(tbl))

# Cramér’s V calculation
cramers_v <- sqrt(chisq_val / (n * (k - 1)))

# Print result
cat("Cramer's V:", cramers_v)
```

# Fisher's Exact test

## What is Fisher’s Exact Test?

-   A statistical test used to determine if there is a **nonrandom association** between two categorical variables.
-   Appropriate when:
    -   Sample sizes are small
    -   Expected frequencies are \< 5

## When to use a Fisher's Exact Test

-   2x2 tables (most common)
-   Very small sample sizes
-   When Chi-square assumptions are violated
-   Expected frequency \< 5 in \>20% of cells

## Caution and Notes

-   Not limited to 2x2 tables (but computationally expensive for larger ones)
-   More accurate than Chi-square in small samples
-   Cannot directly compute effect size (but odds ratio is available)

## Hypotheses

-   **H₀**: The row and column variables are **independent**.
-   **H₁**: There is an **association** between the variables.



## Example Scenario

A researcher asks 20 people whether they prefer coffee or tea.

|          | Coffee | Tea |
|----------|--------|-----|
| English  | 8      | 2   |
| American | 1      | 9   |

Is beverage preference independent of nationality?


## Creating the Table in R

```{r}
#| label: fisher-table
#| echo: true
#| message: false

# Create 2x2 contingency table
beverage_table <- matrix(c(8, 2, 1, 9),
                         nrow = 2,
                         byrow = TRUE)

rownames(beverage_table) <- c("American", "English")
colnames(beverage_table) <- c("Coffee", "Tea")

beverage_table
```

## Running the Fisher's exact test

```{r}
#| label: fisher-test
#| echo: true

# Perform Fisher's Exact Test
fisher.test(beverage_table)
```

## Paired Histogram

```{r}
#| label: fisher-bar-show
#| echo: true
#| eval: false
#| fig-cap: "Beverage Preference by Nationality"
library(ggplot2)

df <- as.data.frame(as.table(beverage_table))
ggplot(df, aes(x = Var2, y = Freq, fill = Var1)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Beverage", y = "Count", fill = "Nationality") +
  theme_minimal()
```

## Paired Histogram

```{r}
#| label: fisher-bar-tell
#| echo: false
#| eval: true
#| fig-cap: "Beverage Preference by Nationality"
library(ggplot2)

df <- as.data.frame(as.table(beverage_table))
ggplot(df, aes(x = Var2, y = Freq, fill = Var1)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Beverage", y = "Count", fill = "Nationality") +
  theme_minimal()
```

## Mosaic Plot

```{r}
#| label: fisher-mosaic-show
#| echo: true
#| eval: false
#| fig-cap: "Beverage Preference by Nationality"

mosaicplot(beverage_table,
           main = "Beverage preference by nationality",
           xlab = "Nationality",
           ylab = "Beverage",
           color = TRUE)
```

## Mosaic Plot

```{r}
#| label: fisher-mosaic-tell
#| echo: false
#| eval: true
#| fig-cap: "Beverage Preference by Nationality"

mosaicplot(beverage_table,
           main = "Beverage preference by nationality",
           xlab = "Nationality",
           ylab = "Beverage",
           color = TRUE)
```

_____________________________________________________________________

# Odds ratios

## What is an Odds Ratio?

-   An **odds** is the ratio of the probability an event occurs to the probability it does **not** occur.

-   The **odds ratio (OR)** compares the odds of an event between two groups.

-   Can think of this as an analogue of the effect size from linear models.

## Interpretation

-   Odds (linear scale) show how much more likely men are to prefer coffee than women.
-   Log(odds) scale:
    -   Makes multiplicative differences additive.
    -   Useful for regression and effect size comparison.
-   A large positive log(OR) indicates strong association.

## Example: Coffee Preference by Nationality

|          | Coffee | Tea | Total |
|----------|--------|-----|-------|
| American | 8      | 2   | 10    |
| English  | 1      | 9   | 10    |

-   Odds (Americans prefer Coffee) = $\frac{8}{2} = 4$
-   Odds (English prefer Coffee) = $\frac{1}{9} ≈ 0.111$
-   **Odds Ratio** of preferring coffee = $\frac{4}{0.111} ≈ 36$

## Interpreting the Odds Ratio

-   **OR = 1** → No association (odds are equal)
-   **OR \> 1** → Group 1 has higher odds of the event
-   **OR \< 1** → Group 1 has lower odds of the event

In this case, Americans are **36 times more likely** to prefer coffee compared to English — a strong association.

## Visualizing Odds and Log Odds Ratio

```{r}
#| label: odds-bars-show
#| echo: true
#| eval: false
#| fig-cap: "Odds and Log(Odds Ratio) for Coffee Preference by Nationality"
#| message: false
#| warning: false

library(ggplot2)
library(dplyr)

# Data setup
odds_data <- data.frame(
  Nationality = c("American", "English"),
  Coffee = c(8, 1),
  Tea = c(2, 9)
)

# Compute odds and log(odds)
odds_data <- odds_data |>
  mutate(
    Odds = Coffee / Tea,
    LogOdds = log(Odds)
  )

# Long format for bar plot
odds_long <- odds_data |>
  select(Nationality, Odds, LogOdds) |>
  tidyr::pivot_longer(cols = c(Odds, LogOdds), names_to = "Metric", values_to = "Value")

# Bar plot for odds and log(odds)
ggplot(odds_long, aes(x = Nationality, y = Value, fill = Nationality)) +
  geom_col(width = 0.6, show.legend = FALSE) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(title = "Comparison of Odds and Log(Odds)",
       y = "Value", x = "Nationality") +
  theme_minimal(base_size = 14)
```

## Visualizing Odds and Log Odds Ratio

```{r}
#| label: odds-bars-tell
#| echo: false
#| eval: true
#| fig-cap: "Odds and Log(Odds Ratio) for Coffee Preference by Nationality"
#| message: false
#| warning: false

library(ggplot2)
library(dplyr)

# Data setup
odds_data <- data.frame(
  Nationality = c("American", "English"),
  Coffee = c(8, 1),
  Tea = c(2, 9)
)

# Compute odds and log(odds)
odds_data <- odds_data |>
  mutate(
    Odds = Coffee / Tea,
    LogOdds = log(Odds)
  )

# Long format for bar plot
odds_long <- odds_data |>
  select(Nationality, Odds, LogOdds) |>
  tidyr::pivot_longer(cols = c(Odds, LogOdds), names_to = "Metric", values_to = "Value")

# Bar plot for odds and log(odds)
ggplot(odds_long, aes(x = Nationality, y = Value, fill = Nationality)) +
  geom_col(width = 0.6, show.legend = FALSE) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(title = "Comparison of Odds and Log(Odds)",
       y = "Value", x = "Nationality") +
  theme_minimal(base_size = 14)
```

# Confusion matrices are contingency tests

## What Is a Confusion Matrix?

-   A **confusion matrix** summarizes the outcomes of a classification task.
-   It compares **predicted** labels to **actual** (true) labels.
-   Helps identify **where** the model is getting things right or wrong.
-   Example for binary classification:

## Confusion Matrix Layout (Binary)

|                      | **Predicted: Positive** | **Predicted: Negative** |
|----------------------|-------------------------|-------------------------|
| **Actual: Positive** | True Positive (TP)      | False Negative (FN)     |
| **Actual: Negative** | False Positive (FP)     | True Negative (TN)      |

\

-   **TP**: Correctly predicted positive case
-   **TN**: Correctly predicted negative case
-   **FP**: Incorrectly predicted positive (Type I error)
-   **FN**: Incorrectly predicted negative (Type II error)

## Confusion Matrix = Contingency Table

-   A confusion matrix **is a special case** of a contingency table:
    -   **Predicted class** ↔ Variable 1
    -   **True class** ↔ Variable 2
-   Entries are **counts** of observations in each combination of categories.
-   It is just a **labeled 2×2 (or K×K)** contingency table.

# Why This Matters

-   **Confusion matrix** helps **evaluate models**.
-   **Contingency tables** help **test statistical relationships**.
-   This duality allows:
    -   Calculation of classification metrics (accuracy, precision, etc.)
    -   Application of statistical tests (Chi-square, Fisher’s Exact)

## Key Metrics {.smaller}

### Accuracy

$$ \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} $$

### Precision (Positive Predictive Value)

$$
\text{Precision} = \frac{TP}{TP + FP}
$$

### Recall (Sensitivity, True Positive Rate)

$$
\text{Recall} = \frac{TP}{TP + FN}
$$

### F1 Score

$$
\text{F1} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
$$

## An example confusion matrix

|                     | Predicted Positive | Predicted Negative |
|---------------------|--------------------|--------------------|
| **Actual Positive** | TP = 50            | FN = 10            |
| **Actual Negative** | FP = 5             | TN = 35            |

## Calculating the confusion matrix metrics

```{r}
#| label: confusion-matrix-calc-show
#| eval: false
#| echo: true

TP <- 50; FN <- 10; FP <- 5; TN <- 35

accuracy <- (TP + TN) / (TP + TN + FP + FN)
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)
f1 <- 2 * precision * recall / (precision + recall)

c(Accuracy = accuracy, Precision = precision, Recall = recall, F1 = f1)

```

## Calculating the confusion matrix metrics

```{r}
#| label: confusion-matrix-calc-tell
#| eval: true
#| echo: false

# Calculate metrics
TP <- 50; FN <- 10; FP <- 5; TN <- 35

accuracy <- (TP + TN) / (TP + TN + FP + FN)
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)
f1 <- 2 * precision * recall / (precision + recall)

c(Accuracy = accuracy, Precision = precision, Recall = recall, F1 = f1)

```

## Visualizing the Confusion Matrix in R

```{r}
#| label: confusion-matrix-vis-show
#| echo: true
#| eval: true

# Create confusion matrix
conf_mat <- matrix(c(50, 10, 5, 35), nrow = 2,
                   dimnames = list("Actual" = c("Positive", "Negative"),
                                   "Predicted" = c("Positive", "Negative")))

conf_mat
```

## Applying Statistical Tests to a Confusion Matrix {.smaller}

-   You can apply a **Chi-square test of independence** to a confusion matrix:
    -   Tests if predicted and actual labels are **statistically independent**.
-   For small sample sizes or sparse data, use **Fisher’s Exact Test**.

```{r}
#| label: conf-matrix-test
#| echo: true
#| eval: true

conf_matrix <- matrix(c(50, 10, 5, 35), nrow = 2)
dimnames(conf_matrix) <- list(
  "Actual" = c("Positive", "Negative"),
  "Predicted" = c("Positive", "Negative")
)

chisq.test(conf_matrix)
```

## On your own

-   calculate the odds ratios for this confusion matrix
-   can you see how odds ratios can be used to evaluate fit?
-   Particularly in the case of training vs. test data sets?
