---
title: "Week 4a - Statistics for Bioengineering"
author: "Bill Cresko"
format: 
  revealjs:
    footer: BioE_Stats_2025 - Knight Campus
    transition: fade
    transition-speed: slow
editor: visual
---

```{r}
library(tidyverse)
library(gt)
library(readxl)
library(nycflights23)
theme_set(theme_minimal())
```

# Sampling, parameter estimation, error and power

## This week

-   Finish parameter estimation

-   t-test

-   Hypothesis testing

-   Power

-   Start linear models

## **Poisson Probability Distribution** {.smaller}

-   Another common situation in bioengineering is when each trial is discrete, but the number of observations of each outcome is observed/counted

-   Some examples are

    -   counts of snails in several plots of land
    -   observations of the firing of a neuron in a unit of time
    -   the number of muscle contractions during a bout of exercise
    -   count of genes in a genome binned to units of 500kb

-   Just like before you have 'successes', but

    -   now you count them for each replicate
    -   the replicates now are units of area or time
    -   the values can now range from 0 to a large number

## Poisson Probability Distribution {.smaller}

<br>

-   For example, you can examine 100 participants
    -   count the number of jumps they can perform in 2 minutes
    -   what is the probability of observing a particpant with `r` jumps?
-   `Pr(Y=r)` is the probability that the number of occurrences of an event `y` equals a count `r` in the total number of trials

<br>

$$Pr(Y=r) = \frac{e^{-\mu}\mu^r}{r!}$$

## Poisson Probability Distribution {.smaller}

-   Note that the Poisson is a single parameter function because $\mu = \sigma^2$
-   The two together are often just represented by $\lambda$

$$Pr(y=r) = \frac{e^{-\lambda}\lambda^r}{r!}$$

-   This means that for a variable that is truly Poisson distributed:
    -   the mean and variance should be equal to one another
    -   variables that are approximately Poisson distributed but have a larger variance than mean are often called 'overdispersed'
    -   quite common in RNA-seq and microbiome data

## Poisson Probability Distribution with gene length by bins of 500 nucleotides {.smaller}

```{r, echo=FALSE, out.width='80%', fig.align='center'}
knitr::include_graphics("images/week_2.004.jpeg")
```

## Poisson Probability Distribution with increasing parameter values of $\lambda$ {.smaller}

```{r, echo=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics("images/week_2.005.jpeg")
```

## Testing Poisson Distributions {.smaller}

Number of counts (x) given a mean and variance of lambda

```{r, echo=TRUE, out.width="40%", fig.align="center"}
dpois(x=2, lambda=1)
plot(dpois(x=1:10, lambda=3))
```

------------------------------------------------------------------------

# Some other common probability distributions

## **Geometric Distribution** {.smaller}

-   If a single event has two possible outcomes the probability of having to observe `k` trials before the first "one" appears is given by the **geometric distribution**
-   The probability that the first "one" would appear on the first trial is `p`, but the probability that the first "one" appears on the second trial is `(1-p)*p`
-   By generalizing this procedure, the probability that there will be `k-1` failures before the first success is:

$$P(X=k)=(1-p)^{k-1}p$$

-   mean = $\frac{1}{p}$
-   variance = $\frac{(1-p)}{p^2}$

## **Geometric Distribution** {.smaller}

-   Example: If the probability of extinction of an endangered population is estimated to be 0.1 every year, what is the expected time until extinction?
-   The distribution gives the probability of extinction in a given year (requiring that the population did not go extinct in all of the years prior)
-   If we want to know the probability of the population going exticnt by year 4, we simply add up the probabilities for years 1-3 using "or" rules

```{r, echo=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics("images/prob.017.jpeg")
```

## Testing Geometric Distributions {.smaller}

-   dgeom gives the density (probability) of an event (p) after (x) failures

```{r, echo=TRUE, out.width="40%", fig.align="center"}
dgeom(x=20, p=0.1)
plot(dgeom(1:20,0.1))
```

## Testing Geometric Distributions {.smaller}

-   pgeom gives the cumulative probability of event (p) in (q) trials

```{r, echo=TRUE}
pgeom(q=20, p=0.1)
plot(pgeom(q=1:20, p=0.1))
```

------------------------------------------------------------------------

## **Negative Binomial Distribution** {.smaller}

-   Extension of the geometric distribution describing the waiting time until `r` "ones" have appeared.
-   Generalizes the geometric distribution
-   Probability of the $r^{th}$ "one" appearing on the $k^{th}$ trial:

$$P(X=k)=(\frac{k-1}{r-1})p^{r-1}(1-p)^{k-r}p$$

<br>

which simplifies to

$$P(X=k)=(\frac{k-1}{r-1})p^{r}(1-p)^{k-r}$$

-   mean = $\frac{r}{p}$
-   variance = $r(1-p)/p^2$

## **Negative Binomial Distribution** {.smaller}

-   Example: If a predator must capture 10 prey before it can grow large enough to reproduce
-   What would the mean age of onset of reproduction be if the probability of capturing a prey on any given day is 0.1?
-   Notice that the variance is quite high (\~1000) and that the distribution looks quite skewed

```{r, echo=FALSE, out.width='50%', fig.align='center'}
knitr::include_graphics("images/prob.018.jpeg")
```

# Continuous Probability Distributions

## **Continuous probability distributions** {.smaller}

P(observation lies within dx of x) = f(x)dx

$$P(a\leq X \leq b) = \int_{a}^{b} f(x) dx$$

Remember that the indefinite integral sums to one

$$\int_{-\infty}^{\infty} f(x) dx = 1$$

## Continuous probabilities {.smaller}

<br>

`E[X]` may be found by integrating the product of `x` and the probability density function over all possible values of `x`:

$$E[X] = \int_{-\infty}^{\infty} xf(x) dx $$

<br>

$Var(X) = E[X^2] - (E[X])^2$, where the expectation of $X^2$ is

$$E[X^2] = \int_{-\infty}^{\infty} x^2f(x) dx $$

## **Uniform Distribution** {.smaller}

<br>

$$E[X] = \int_{a}^{b} x\frac{1}{b-a} dx = \frac{(a+b)}{2} $$

<br>

```{r, echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/prob.019.jpeg")
```

## **Uniform Distribution** {.smaller}

-   Means that the probability is equal for all possible outcomes
-   Like drawing m&m out of a bag with equal proportions of colors

```{r, echo=TRUE}
dunif(x=1,min=0, max=10)
plot(dunif(1:10, 0, 10))
```

## Exponential Distribution {.smaller}

<br>

$$f(x)=\lambda e^{-\lambda x}$$

<br>

-   `E[X]` can be found be integrating $xf(x)$ from 0 to infinity

<br>

-   leading to the result that

<br>

-   $E[X] = \frac{1}{\lambda}$
-   $E[X^2] = \frac{1}{\lambda^2}$

## Exponential Distribution {.smaller}

-   For example, let equal the instantaneous death rate of an individual.
-   The lifespan of the individual would be described by an exponential distribution (assuming that does not change over time).

```{r, echo=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics("images/prob.020.jpeg")
```

## Exponential Distribution {.smaller}

```{r, echo=TRUE}
dexp(10, rate = 0.1)
plot(dexp(1:100, rate = 0.1))
```

------------------------------------------------------------------------

## **Gamma Distribution** {.smaller}

-   The gamma distribution generalizes the exponential distribution.
-   It describes the waiting time until the $r^{th}$ event for a process that occurs randomly over time at a rate $\lambda$ :

<br>

$$f(x) = \frac{e^{-\lambda x}\lambda x^{r-1}}{(r-1)!}\lambda$$

<br>

$$ Mean =  \frac{r}{\lambda} $$ $$ Variance = \frac{r}{\lambda^2} $$

## Gamma Distribution {.smaller}

-   **Example**: If, in a PCR reaction, DNA polymerase synthesizes new DNA strands at a rate of 1 per millisecond, how long until 1000 new DNA strands are produced?

-   Assume that DNA synthesis does not deplete the pool of primers or nucleotides in the chamber, so that each event is independent of other events in the PCR chamber.

-   The gamma distribution generalizes the exponential distribution.

-   It describes the waiting time until the $r^{th}$ event for a process that occurs randomly over time at a rate $\lambda$

# Normal (Gaussian) probability distribution {.smaller}

## Normal PDF - ($\mu$ and $\sigma$) {.smaller}

$$f(x) = \frac{1}{\sqrt{2\pi\sigma}} \, \mathrm{e}^{-\frac{(x - \mu)^2}{2\sigma^2}}$$

where $$\large \pi \approx 3.14159$$

$$\large \mathrm{e} \approx 2.71828$$

To write that a variable (v) is distributed as a normal distribution with mean $\mu$ and variance $\sigma^2$, we write the following:

$$\large v \sim \mathcal{N} (\mu,\sigma^2)$$

## 

```{r, echo=FALSE, out.width='90%', fig.align='center'}
knitr::include_graphics("images/week_2.011.jpeg")
```

## 

```{r, echo=FALSE, out.width='50%', fig.align='center'}
knitr::include_graphics("images/week_2.012.jpeg")
```

## Estimates of mean and variance {.smaller}

Estimate of the mean from a single sample

$$\Large \bar{x} = \frac{1}{n}\sum_{i=1}^{n}{x_i} $$

Estimate of the variance from a single sample

$$\Large s^2 = \frac{1}{n-1}\sum_{i=1}^{n}{(x_i - \bar{x})^2} $$

## Normal PDF {.smaller}

```{r, echo=FALSE, out.width='90%', fig.align='center'}
knitr::include_graphics("images/week_2.010.jpeg")
```

## Why is the Normal special in biosciences? {.smaller}

```{r, echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/week_2.013.jpeg")
```

## Why is the Normal special in biosciences? {.smaller}

```{r, echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/week_2.015.jpeg")
```

## Why is the Normal special in biosciences? {.smaller}

```{r, echo=FALSE, out.width='60%', fig.align='center'}
knitr::include_graphics("images/week_2.014.jpeg")
```

## Parent-offspring resemblance {.smaller}

```{r, echo=FALSE, out.width='45%', fig.align='center'}
knitr::include_graphics("images/week_2.016.jpeg")
```

## Genetic model of complex traits {.smaller}

```{r, echo=FALSE, out.width='90%', fig.align='center'}
knitr::include_graphics("images/week_2.017.jpeg")
```

## Distribution of $F_2$ genotypes \| really just binomial sampling {.smaller}

```{r, echo=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics("images/week_2.018.jpeg")
```

## Why else is the Normal special?

-   It is the basis of estimation and precision of the expected value of **all distributions**

-   Provides a mathematical basis for moving from **single samples to point estimates**.

-   Provides a way to use simulation to generate **empirical sample and test distributions** through Monte Carlo approaches

## z-scores of normal variables {.smaller}

-   Often we want to make variables more comparable to one another
-   For example, consider measuring the leg length of mice and of elephants
    -   Which animal has longer legs in absolute terms?
    -   Proportional to their body size?
-   A good way to answer these last questions is to use 'z-scores'

## z-scores of normal variables {.smaller}

-   z-scores are standardized to a mean of 0 and a standard deviation of 1
-   We can modify any normal distribution to have
    -   a mean of 0 and
    -   a standard deviation of 1
-   Another term for this is the standard normal distribution

$$\huge z_i = \frac{(x_i - \bar{x})}{s}$$

## Common theme in R for distributions {.smaller}

+-------------+---------------------------+-------------------------+-------------------+------------------------------+
|             | d                         | p                       | q                 | r                            |
|             |                           |                         |                   |                              |
|             | probability mass function | cumulative distribution | quantile function | pseudorandom number generate |
+=============+:=========================:+:=======================:+:=================:+:============================:+
| binomial    | dbinom                    | pbinom                  | qbinom            | rbinom                       |
+-------------+---------------------------+-------------------------+-------------------+------------------------------+
| poisson     | dpois                     | ppois                   | qpois             | rpois                        |
+-------------+---------------------------+-------------------------+-------------------+------------------------------+
| exponential | dexp                      | pexp                    | qexp              | rexp                         |
+-------------+---------------------------+-------------------------+-------------------+------------------------------+
| normal      | dnorm                     | pnorm                   | qnorm             | rnorm                        |
+-------------+---------------------------+-------------------------+-------------------+------------------------------+

------------------------------------------------------------------------

# Sampling and estimation

## Parameter Estimation {.smaller}

-   Estimation is the process of inferring a population parameter from sample data
-   The value of one sample estimate is almost never the same as the population parameter because of random sampling error
-   Most will be close, but some will be far away
-   Sampling distribution of an estimate
    -   all values we might have obtained from our sample
    -   probabilities of occurrence
-   Standard error of an estimate
    -   standard deviation of a sampling distribution
    -   measures the precision of the parameter estimate
    -   NO ESTIMATE IS USEFUL WITHOUT IT!

## Parameter Estimation {.smaller}

-   **Parametric** (a few special exceptions, like the sample mean and its standard error)
-   **Resampling** - bootstrapping and randomization to create empirical null distributions
-   **Ordinary Least Squares (OLS)** - optimized procedure that produces one definitive result, easy to use but no estimates of confidence
-   **Maximum Likelihood (ML)** - Can provide model-based estimates with confidence, but harder to calculate
-   **Bayesian Approaches** - Incorporates prior information into ML estimation

## Accuracy vs. Precision {.smaller}

```{r, echo=FALSE, out.width='50%', fig.align='center'}
knitr::include_graphics("images/week_2.024.jpeg")
```

## Accuracy vs. Precision {.smaller}

-   **Accuracy** is the closeness of an estimated value to its true value
-   **Precision** is the closeness of repeated estimates to one another
-   Our goal is to have **unbiased estimates** that are the most precise
-   We have to **estimate parameters** and **test hypotheses** by taking **samples** that approximate the underlying distribution
-   The goal of **replication** is to quantify variation at as many levels in a study as possible
-   The goal of **randomization** is to avoid bias as much as possible

# The Central Limit Theorem {.smaller}

## The Central Limit Theorem {.smaller}

-   For most real world data sets we can’t empirically determine a sampling distribution by taking many actual samples, because we often have just the one sample.
-   Fortunately, we can rely on the **Central Limit Theorem** to make some assumptions about sampling distributions, particularly when estimating a mean from a single sample, or when estimating most any parameter using a “pseudo” or re-sampling process we refer to as “**bootstrapping**”


## Standard Error of the Mean (SEM) {.smaller}

$$\huge \sigma_{\bar{x}} \approx s_{\bar{x}} = \frac{s}{\sqrt{n}} $$

-   where $s_{\bar{x}}$ is the estimated standard error of the distribution of the mean estimates
-   which is usually just referred to as the 'standard error of the mean (SEM)
-   note that this **is not** the standard deviation of the original distribution
-   importantly, the SEM will go down as the sample size increases

## Calculating the Standard Error of the Mean (SEM) {.smaller}

-   Think conceptually - how will SEM change as sample size increases?

```{r, echo=TRUE}
set.seed(32)
true_pop <- rnorm(n=1000, mean=2, sd=5)
samps_var <- replicate(n = 50, sample(true_pop, size = 5))
samps_var_means <- apply(samps_var, 2, mean)
sem = sd(samps_var_means)/ sqrt(length(samps_var_means))
print(sem)
```

```{r, echo=TRUE}
set.seed(32)
true_pop <- rpois(n=1000, lambda = 5)
samps_var <- replicate(n = 50, sample(true_pop, size = 5))
samps_var_means <- apply(samps_var, 2, mean)
sem = sd(samps_var_means)/ sqrt(length(samps_var_means))
print(sem)
```

```{r, echo=TRUE}
set.seed(32)
true_pop <- rpois(n=1000, lambda = 5)
samps_var <- replicate(n = 50, sample(true_pop, size = 50))
samps_var_means <- apply(samps_var, 2, mean)
sem = sd(samps_var_means)/ sqrt(length(samps_var_means))
print(sem)
```

## Estimation and confidence intervals {.vcenter .flexbox}

```{r, echo=FALSE, out.width='90%', fig.align='center'}
knitr::include_graphics("images/week_2.026.jpeg")
```

## Estimation and confidence intervals {.vcenter .flexbox}

```{r, echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/week_2.027.jpeg")
```

## Estimation and confidence intervals {.vcenter .flexbox}

```{r, echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/week_2.028.jpeg")
```

## Confidence Intervals

-   A confidence interval is a range of values about a parameter estimate, such that we are X% certain that the true population parameter value lies within that interval.
-   For now, know that for a normally distributed sample, a confidence interval about the population mean can be calculated using the t.test() function in base R.
-   The 95% confidence interval is commonly reported in statistical analysis results, by convention, but other values are occasionally reported as well.

## Coefficient of Variation

-   To make standard deviations comparable across populations with very different means, we can instead compare a standardized metric called the “coefficient of variation” (CV), which is simply the sample standard deviation divided by the sample mean (and usually expressed as a % by multiplying by 100).

# Basics of bootstrapping via re-sampling

## Bootstrapping

-Unfortunately, most other kinds of estimates (anything not the mean) do not have this amazing property, but we can rely on another approach to calculate the standard error. - This involves generating your own sampling distribution for the estimate using the “**bootstrap**,” a method invented by Efron (1979). - We call the bootstrap, and other methods that do not rely on distributional assumptions of the variable itself, “**nonparametric**” approaches.

## The etymology of the term 'bootstrap' {.smaller}

```{r, echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/week_2.030.jpeg")
```

## Why the bootstrap is good {.smaller}

-   Can be applied to almost any sample statistic
    -   This includes means, proportions, correlations, regression
-   Works when there is no ready formula for a standard error
    -   For example the median, trimmed mean, correlation, eigenvalue, etc.
-   Is nonparametric, so doesn’t require normally-distributed data
-   Works for estimates based on complicated sampling procedures or calculations

## Easy steps for bootstrapping in R {.smaller}

1.  Take a random sample (with replacement) from your sample data
2.  Calculate the estimate using the measurements in the bootstrap sample (step 1). This is the first bootstrap replicate estimate
3.  Repeat steps 1 and 2 a large number of times (1000 times is reasonable)
4.  Calculate the sample standard deviation of all the bootstrap replicate estimates obtained in step 3 (SSD = sd(sample)/sqrt(sample size))

## Bootstrapping {.smaller}

-   The resulting quantity is called the “bootstrap standard error”
-   The bootstrap can be applied to almost any sample statistic, including means, proportions, correlations, and regression parameters.
-   It works when there is no ready formula for a standard error, for example when estimating the median, trimmed mean, correlation, eigenvalue, etc.
-   It is nonparametric, so doesn’t require normally-distributed data, as mentioned. 
- It works well for parameter estimates that are based on complicated sampling procedures or calculations. For example, it is used to assess confidence in local relationships within phylogenetic trees.


# Loops in R - a good way to create even more complicated null distributions

## Loops in R

-   R is very good at performing repetitive tasks.
-   If we want a set of operations to be repeated several times we use what’s known as a loop.
-   When you create a loop, R will execute the instructions in the loop a specified number of times or until a specified condition is met.
-   There are two common types of loop in R: the `for loop` and the `while loop`

## For loops

-   The most commonly used loop structure when you want to repeat a task a defined number of times is the for loop. The most basic example of a for loop is:
-   How does this appear to be working?

```{r,  echo=T}
# Notice the sequence of parentheses and brackets used in this example
for (i in 1:5) {
  print(i)
}
```

## While loops

-   Another type of loop that you may use is the while loop.
-   The while loop is used when you want to keep looping until a specific logical condition is satisfied.

```{r,  echo=T}
i <- 0
while (i <= 4) {
  i <- i + 1
  print(i)
}
```

## If and Else Statements

-   Conditional statements are how you inject some logic into your code.
-   The most commonly used conditional statement is `if.`
    -   Whenever you see an `if` statement, read it as ‘If X is TRUE, then do a thing’.
-   Another statement is `else`, which extends the logic to ‘If X is TRUE, do a thing, or else do something different’.

## A programming joke for conditional statements {.smaller}

A programmer’s partner says: ‘Please go to the store and buy a carton of milk and if they have eggs, get six.’

The programmer returned with 6 cartons of milk.

When the partner sees this, and exclaims ‘Why the heck did you buy six cartons of milk?’

The programmer replied ‘They had eggs’

```{r,  echo=T}
eggs <- TRUE # Whether there were eggs in the store

  if (eggs == TRUE) { # If there are eggs
  n.milk <- 6 # Get 6 cartons of milk
    } else { # If there are not eggs
  n.milk <- 1 # Get 1 carton of milk
  }
```

------------------------------------------------------------------------

## R INTERLUDE - Simulate a population and sample it! {.smaller}

## Simulations to compare parameter estimates {.smaller}

-   Let's use our distribution functions from last time to set up some data to play with
-   Let's imagine our data is made up of counts, with an average of 3 counts - which distribution would fit that data best?
-   Ex: number of hours per day spent doing homework by UO undergraduates

```{r,  echo=TRUE, out.width="50%"}
set.seed(32)
true_pop <- rpois(n=10000, lambda = 3)
hist(true_pop, xlim = c(0,16))
```

## Calculating parameters

-   How would we calculate the mean and range for this population?

```{r, echo=TRUE, eval=TRUE}
mean(true_pop)
range(true_pop)
median(true_pop)
```

-   how about the variance and the standard deviation?

## Sampling Exercise

-   Since we are working with simulated data, we can also afford to simulate our sampling!
-   Try taking a sample from our `true_pop` dataset and change the sample size, then calculate the mean and range for your sample and see how it compares to the true values.
-   How many college students are you including in your survey?

## Randomness in Sampling

-   Because of the randomness of sampling, you may get close to the true estimates even with a small sample size 
-   But your results will change each time you take a new sample of the same size
-   How do we get a feel for how accurate each sample size is? Or which sample size is recommended?

## Surveying your Sampling

Step 1 - take 50 samples of size 10
Step 2 - Calculate the mean from each sample
Step 3 - Plot the distribution of sample mean estimates

```{r, echo = TRUE, out.width="50%"}
set.seed(32)
true_pop <- rpois(n=10000, lambda = 3)
samps_var <- replicate(50, sample(true_pop, 10))
samps_var_means <- apply(samps_var, 2, mean)
```

## Histograms

:::: {.columns}

::: {.column width="50%"}
```{r}
hist(true_pop)
```
:::

:::{.column width="50%"}
```{r}
hist(samps_var_means)
```
:::

::::

## Surveying your Sampling

-   This sampling variation is what we have to deal with, and account for, as empirical scientists.
-   If this had been a real-world scenario, we likely would be basing our estimate for the true mean on just a single sample mean.
-   Getting close to the idea of **power** - does our experimental design have the power to detect the parameters we are interested in?

## CHALLENGE - Bootstrapping to produce a confidence interval {.smaller}

-   The 2.5th and 97.5th percentiles of the bootstrap sampling distribution are a passable 95% confidence interval
-   Note that no transformations or normality assumptions needed
-   You can use the `quantile()` function to calculate these
-   You can also use `qnorm`
-   Use R to figure out the bootstrap distribution for other parameters (such as variance).
-   Try to produce bootstrap distributions for the mean and variance of gene expresssion of one gene from the stickleback data set
-   Plot the resulting distributions
-   Determine the value of the 2.5th and 97.5th percentiles

------------------------------------------------------------------------

# Hypothesis testing, test statistics, p-values {.smaller}

## What is a hypothesis {.vcenter .flexbox}

-   A statement of belief about the world
-   Need a **critical** test to
    -   accept or reject the hypothesis
    -   compare the relative merits of different models
-   This is where **statistical sampling distributions** come into play

## What is a hypothesis? {.vcenter .flexbox}

-   We can compare estimated parameters, while also including measures of uncertainty, using frequentist methods
-   Requires statistical hypotheses: a statement of belief about the world
-   Need a **critical** test to
    -   accept or reject the hypothesis
    -   compare the relative merits of different models

## What is a hypothesis? {.vcenter .flexbox}

-   This is where **statistical sampling distributions** come into play
    -   Statistical distributions are built upon sampling distributions
    -   Will tell us how "frequently" we expect to see a result as extreme as ours

## Hypothesis tests {.vcenter .flexbox}

$H_0$ : *Null hypothesis* : Ponderosa pine trees are the same height on average as Douglas fir trees

$H_A$ : *Alternative Hypothesis*: Ponderosa pine trees are not the same height on average as Douglas fir trees

## Hypothesis tests {.vcenter .flexbox}

-   What is the probability that we would reject a **true null hypothesis**?

-   What is the probability that we would accept a **false null hypothesis**?

-   How do we **decide** when to reject a null hypothesis and support an alternative?

-   What can we **conclude** if we fail to reject a null hypothesis?

-   What **parameter estimates of distributions** are important to test hypotheses?


## Null and alternative hypotheses \| population distributions {.smaller}

-   Which scenario do you think will be easier to find a difference?
-   What factor(s) will make our statistical tests better?

```{r, echo=FALSE, out.width='40%', fig.align='center'}
knitr::include_graphics("images/week_3.002.jpeg")
```

# Statistical sampling distributions {.vcenter .flexbox}

-   Statistical tests provide a way to perform **critical tests** of hypotheses
-   Just like raw data, statistics are **random variables** and depend on sampling distributions of the underlying data
-   The particular **form of the statistical distribution** depends on the test statistic and parameters such as the degrees of freedom that are determined by sample size.
    -   Essentially: we get a distribution for statistics values too! How frequently do we see a statistics value as large/small as this one?

## Statistical sampling distributions {.vcenter .flexbox}

-   In many cases we create a **null statistical distribution** that models the distribution of a test statistic under the **null hypothesis**.
-   Similar to point estimates, we calculate an **observed test statistic value** for our data.
-   Then see how probable it was by comparing against **the null distribution.**
-   The probability of seeing that value or greater is called the **p-value** of the statistic.

## Statistical sampling distributions

-   An example of a test statistic is the t-statistic.
-   The t-statistic is a standardized difference between two sample means
    -   t = 0 indicates no difference between population means
    -   t-distribution is Normal, with the center and peak at 0
-   We can evaluate the t-statistic for our sample data and see whether it falls far enough away from zero - then we reject the null hypothesis



## Null and alternative hypotheses \| population distributions {.smaller}

```{r, echo=FALSE, out.width='90%', fig.align='center'}
knitr::include_graphics("images/week_3.001.jpeg")
```

## Null and alternative hypotheses \| population distributions {.smaller}

```{r, echo=FALSE, out.width='60%', fig.align='center'}
knitr::include_graphics("images/week_3.002.jpeg")
```


## Four common statistical distributions {.smaller}

```{r, echo=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics("images/week_3.003.jpeg")
```

## The t-test and t sampling distribution {.smaller}

$$\huge t = \frac{(\bar{y}_1-\bar{y}_2)}{s_{\bar{y}_1-\bar{y}_2}} $$

where

```{r, echo=FALSE, out.width='60%', fig.align='center'}
knitr::include_graphics("images/week_3.016.jpeg")
```

which is the calculation for the standard error of the mean difference

## The t-test and t sampling distribution \| under different degrees of freedom {.smaller}

```{r, echo=FALSE, out.width='90%', fig.align='center'}
knitr::include_graphics("images/week_3.004.jpeg")
```

## The t-test and t sampling distribution \| one-tailed test {.smaller}

```{r, echo=FALSE, out.width='90%', fig.align='center'}
knitr::include_graphics("images/week_3.005_1_tailed.jpeg")
```

## The t-test and t sampling distribution \| two-tailed test {.smaller}

```{r, echo=FALSE, out.width='60%', fig.align='center'}
knitr::include_graphics("images/week_3.006.jpeg")
```

## The t-test and t sampling distribution {.vcenter .flexbox}

$H_0$ : Null hypothesis : Ponderosa pine trees are the same height on average as Douglas fir trees

$$H_0 : \mu_1 = \mu_2$$

$H_A$ : Alternative Hypothesis: Ponderosa pine trees are not the same height as Douglas fir trees

$$H_A : \mu_1 \neq \mu_2$$

## The t-test and t sampling distribution {.smaller}

where

```{r, echo=FALSE, out.width='60%', fig.align='center'}
knitr::include_graphics("images/week_3.016.jpeg")
```

which is the calculation for the standard error of the mean difference


## Assumptions of parameteric t-tests {.vcenter .flexbox}

<br>

-   The theoretical t-distributions for each degree of freedom were calculated for populations that are:
    -   normally distributed
    -   have equal variances (if comparing two means)
    -   observations are independent (randomly drawn)
-   This is an example of a **parametric** test
-   What do you do if the there is non-normality?
    -   nonparametric tests such as Mann-Whitney-Wilcoxon
    -   randomization tests to create a null distribution

## A tangent on degrees of freedom

-   In statistics, degrees of freedom refers to the number of values in a calculation that are free to vary.
-   Example: you have a set of numbers, like 1, 2, 3, and 4. If you know the average (mean) of these numbers is 3, you can actually choose three of the numbers freely, but the fourth number will be determined by the other three. So in this case, you have three degrees of freedom.
-   This answer comes from ChatGPT!

## Degrees of freedom exercise

-   Try out this concept in R
    -   Randomly sample from a list of numbers (1-10) 5 numbers
    -   Calculate the mean for that group of numbers
    -   If you were to add a sixth number, what would it have to be for the mean to = 7?

## On a scale of Murica to 10, how many degrees of freedom do you have?

```{r, echo=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics("images/week7_murica.jpeg")
```


## Type 1 and Type 2 errors {.smaller}

<br>

```{r, echo=FALSE, out.width='90%', fig.align='center'}
knitr::include_graphics("images/week_3.007.jpeg")
```

## Components of hypothesis testing

-   **p-value** = the long run probability of rejecting a true null hypothesis
-   $\alpha$ = critical value of p-value cutoff for experiments. The Type I error we are willing to tolerate.
-   $\beta$ = cutoff for probability of accepting a false null hypothesis
-   **Power** = the probability that a test will reject a false null hypothesis (1 - beta). It depends on effect size, sample size, chosen alpha, and population standard deviation
-   **Multiple testing** = performing the same or similar tests multiple times - need to correct alpha value
-   Can correct multiple testing using a tax (e.g. **Bonferonni**) or directly estimating a **False Discovery Rate (FDR)**

## Statistical power

<br>

-   Type 1 error - $\alpha$ - incorrectly rejecting a true null hypothesis
    -   This is saying that there is an effect when there isn’t
-   Type 2 error - $\beta$ - incorrectly accepting a false null hypothesis
    -   This is saying that there isn’t an effect when there is
-   Power is the probability of rejecting a false null hypothesis
-   Mostly we shoot for a power of around 80%
-   Power can be calculated *post hoc* or *a priori*


## Why do we use $\alpha = 0.5$ as a cutoff? {.smaller}

```{r, echo=FALSE, out.width='90%', fig.align='center'}
knitr::include_graphics("images/week_3.008.jpeg")
```


## Components of hypothesis testing

-   **Multiple testing** = performing the same or similar tests multiple times - need to correct alpha value
-   Can correct multiple testing using a tax (e.g. **Bonferonni**) or directly estimating a **False Discovery Rate (FDR)**

## Statistical power

-   Type 1 error - $\alpha$ - incorrectly rejecting a true null hypothesis
    -   This is saying that there is an effect when there isn’t
-   Type 2 error - $\beta$ - incorrectly accepting a false null hypothesis
    -   This is saying that there isn’t an effect when there is
-   Power is the probability of rejecting a false null hypothesis
-   Mostly we shoot for a power of around 80%
-   Power can be calculated *post hoc* or *a priori*

## Power \| the things one needs to know


$$ Power \propto \frac{(ES)(\alpha)(\sqrt n)}{\sigma}$$

-   Power is proportional to the combination of these parameters

    -   **ES** - effect size; how large is the change of interest?
    -   $\alpha$ - significance level (usually 0.05)
    -   **n** - sample size
    -   $\sigma$ - standard deviation among experimental units within the same group.

## Power \| what we usually want to know {.flexbox .vcenter}

<br>

```{r, echo=FALSE, out.width='90%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_6b.002.jpeg")
```


## Power \| rough calculation

```{r, echo=FALSE, out.width='90%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_6b.003.jpeg")
```


# Power Analyses

## What is a power analysis?

-   Type 1 error - $\alpha$ - incorrectly rejecting a true null hypothesis
    -   This is saying that there is an effect when there isn’t
-   Type 2 error - $\beta$ - incorrectly accepting a false null hypothesis
    -   This is saying that there isn’t an effect when there is
-   Power is the probability of rejecting a false null hypothesis
-   Mostly we shoot for a power of around 80%
-   Power can be calculated *post hoc* or *a priori*

## Statistical power

-   What factors can affect the power of our experiment - our ability to avoid a Type 2 error?

## Statistical power

-   What factors can affect the power of our experiment - our ability to avoid a Type 2 error?
    -   Sample size
    -   Effect size (difference between the groups)
    -   Variance (range of values for this trait/measure)

## A priori Power analyses

-   Before we start an experiment, we are interested in what sample size we should collect
-   We can use simulations to test different sample sizes

## An example

-   Let's say we're studying college students again, and we're interested in seeing if there's a difference in study hours between freshman and seniors
-   How many students should we sample?
    -   This will depend on our predictions about the effect size of this measurement

## Steps to power analysis

1.  Simulate the true distributions of our populations (decide on effect size, distribution type, and variance)
2.  Draw random samples of different sizes from those populations
3.  Perform our statistical test (t-test) on these samples
4.  Repeat 2 & 3 \~1000 times
5.  Plot our resulting p-values against sample size

## Step 1: simulating our true populations

-   What is the distribution type?
-   What is the effect size: difference in means between populations?
-   What is the variance?

## Step 1: simulating our true populations

```{r}
senior <- rpois(5000, lambda = 10)
fresh <- rpois(5000, lambda = 12)
```

## Step 1: simulating our true populations

```{r, echo=FALSE, out.width="40%"}
hist(senior)
hist(fresh)
```

## Step 2: drawing a sample

```{r}
sample_s <- sample(senior, size = 10, replace = FALSE)
sample_f <- sample(fresh, size = 10, replace = FALSE)
```

## Step 2: drawing a sample

```{r, echo=FALSE, out.width="40%"}
hist(sample_s)
hist(sample_f)
```

## Step 3: statistical test

```{r}
t.test(sample_f, sample_s)
```

## Step 4: setting up our replicates

-   Take a look at the "samps_var" vectors, how are they arranged? How would we begin conducting t-tests using each replicate from the two populations?

```{r}
## sample size of 10
samps_var_s <- replicate(n = 100, sample(senior, size = 10))
samps_var_f <- replicate(n = 100, sample(fresh, size = 10))

```

## Step 4: Testing our replicates

```{r}
# setting up a "test" dataframe
tests <- data.frame(1:100)
tests$SampleSize <- rep("10", 100)

for (i in 1:ncol(samps_var_f)){
  tests$result[i] <- t.test(samps_var_s[,i], samps_var_f[,i])$p.value
}
table(tests$result < 0.05)
```

## Step 4 contd: changing the sample size

## Step 4 contd: multiple sample sizes

-   Requires a more complex for loop

```{r, echo=FALSE, eval = FALSE}


results <- data.frame()


# using seq to set up the sample sizes


for (x in seq(10,100, by = 10)){
  samps_var_s <- replicate(n = 100, sample(senior, size = x))
  samps_var_f <- replicate(n = 100, sample(fresh, size = x))
  
  tests <- data.frame(1:100)
  tests$SampleSize <- rep(x, 100)
  for (i in 1:100){
    tests$p.value[i] <- t.test(samps_var_s[,i], samps_var_f[,i])$p.value
  }
  results <- rbind(results, tests)
}
```



## R Interlude \| Complete Exercises 3.2-3.4

<br>

```{r, echo=FALSE, out.width='75%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/cats/two.jpg")
```

## Components of hypothesis testing

-   **p-value** = the long run probability of rejecting a true null hypothesis
-   **alpha** = critical value of p-value cutoff for experiments. The Type I error we are willing to tolerate.
-   **beta** = cutoff for probability of accepting a false null hypothesis
-   **Power** = the probability that a test will reject a false null hypothesis (1 - beta). It depends on effect size, sample size, chosen alpha, and population standard deviation
-   **Multiple testing** = performing the same or similar tests multiple times - need to correct

## Null distributions and p-values {.flexbox .vcenter}

```{r, echo=FALSE, out.width='90%', fig.align='center'}
knitr::include_graphics("images/week_3.009.jpeg")
```

## Why do we use $\alpha = 0.5$ as a cutoff? {.smaller}

```{r, echo=FALSE, out.width='90%', fig.align='center'}
knitr::include_graphics("images/week_3.008.jpeg")
```

# The t-test and t sampling distribution {.smaller}

-   Now we're going to talk about one example - the t statistic

## The t-test and t sampling distribution {.smaller}

-   To calculate the t-statistic for two populations:

$$\large t = \frac{(\bar{y}_1-\bar{y}_2)}{s_{\bar{y}_1-\bar{y}_2}} $$

where s is the standard error of the mean difference

## The t-test and t sampling distribution \| one-tailed test {.smaller}

```{r, echo=FALSE, out.width='90%', fig.align='center'}
knitr::include_graphics("images/week_3.005_1_tailed.jpeg")
```

## The t-test and t sampling distribution \| two-tailed test {.smaller}

```{r, echo=FALSE, out.width='60%', fig.align='center'}
knitr::include_graphics("images/week_3.006.jpeg")
```

## Assumptions of parameteric t-tests {.vcenter .flexbox}

-   The theoretical t-distributions for each degree of freedom were calculated for populations that are:
    -   normally distributed
    -   have equal variances (if comparing two means)
    -   observations are independent (randomly drawn)
-   This is an example of a **parametric** test


## Let's try it out

-   Use the below code to set up two simulated populations:

```{r}
set.seed(518)
pop1 <- sample(rnorm(n=10000, mean=2, sd=0.5), size = 100)
pop2 <- sample(rnorm(n=10000, mean=5, sd=0.5), size = 100)

```

## LEt's try it out

```{r, echo=FALSE, out.width="40%"}
hist(pop1)
hist(pop2)
```

## First, is the distribution/spread the same?

-   Use the function `var.test()` in R
-   How do we interpret this?

```{r}
var.test(pop1, pop2)
```

## Now for the t-test!

-   Use the function `t.test()` in R
-   How do we interpret this? How would you write this conclusion as a sentence?

```{r}
t.test(pop1, pop2)
```

# Nonparametric tests

-   What if your data does not meet the requirements of a parametric t-test?
-   What do you do if the there is non-normality?
    -   nonparametric tests such as Mann-Whitney-Wilcoxon
    -   randomization tests to create a null distribution

## Mann-Whitney-Wilcoxon Tests

-   The Mann-Whitney U (also called “Mann-Whitney-Wilcoxon) Test tests for distributional differences between the ranks of two samples.
-   In R the function wilcox.test() can be used to perform it, in much the same way the t.test() function is used.

## Let's try it out

```{r, out.width = "50%"}
set.seed(518)
pop1 <- sample(rnorm(n=10000, mean=2, sd=0.5), size = 100)
pop2 <- sample(rnorm(n=10000, mean=5, sd=1.5), size = 100)
hist(pop2)
```

## Test the variance

-   Use the function we used before to see if the variance is the same or different between pop1 and pop2
-   How do we interpret this?

## The Mann Whit U Test

-   Use the function `wilcox.test()` in R to compare pop1 and pop2
-   How do we interpret that result? How would you write it as a sentence for a paper?

## The Mann Whit U Test

```{r}
wilcox.test(pop1, pop2)
```

## Null distributions and p-values

-   We can also create a null statistical distribution that models the distribution of a test statistic under the null hypothesis
-   To create the null distribution we can use either randomization or resampling

## Creating a null distribution through randomization

1.  Combine values from both populations into a single vector
2.  Randomly shuffle the vector using the sample() function
3.  Calculate a t statistic based on the first n1 and n2 observations as our “pseudo samples” from “populations” 1 and 2, respectively, and save the value
4.  Repeat steps 2 and 3 many times (e.g. 1000)
5.  Calculate the proportion of pseudo replicates in which t is ≥ to our original, observed value of t. This proportion is our estimated p-value for the test.

## Let's do it - the starting populations

```{r }
set.seed(56)
pop_1 <- rnorm(n=50, mean=20.1, sd=2)#simulate population 1 for this example
pop_2 <- rnorm(n=50, mean=19.3, sd=2)#simulate population 2 for this example
```

```{r, echo=F, out.width="40%"}
hist(pop_1)
hist(pop_2)
```

## Calculate the t-test

```{r}
# Store the t statistic calculated from our samples, using t.test()
t_obs <- t.test(x=pop_1, y=pop_2, alternative="greater")$statistic
```

## Combine the populations, sample from that, and calculate t-tests

```{r}
# Combine both population vectors into one
pops_comb <- c(pop_1, pop_2)

# Randomly shuffle and calculate t statistic 1000 times
t_rand <- replicate(1000, {
  pops_shuf <- sample(pops_comb)
  t.test(x=pops_shuf[1:50], y=pops_shuf[51:100], alternative="greater")$statistic
  })
```

## Plot the null distribution

```{r}

# Plot the "null distribution" from the randomization-based t-values
hist(t_rand)
```

## Null distributions and p-values {.flexbox .vcenter}

```{r, echo=FALSE, out.width='90%', fig.align='center'}
knitr::include_graphics("images/week_3.009.jpeg")
```

## What's our result?

-   How do we interpret this?

```{r}
# Calculate the p-value for the test as the number of randomization t-values greater
# than or equal to our actual t-value observed from the data
p <- sum(t_rand>=t_obs)/1000

p
```





## R INTERLUDE \| Parametric t-test in R {.smaller}

1.  Make a dummy data set that contains one continuous and one categorical value (with two levels). Draw individuals of the two different factor levels from normal distributions with slightly different means. Take 100 obsv. per factor level.

2.  Now, perform a t-test to see if the two populations are statistically different from one another

```{r, eval=FALSE, echo=TRUE}
boxplot(continuous_variable~cat_variable, dataset name) 
t.test(continuous_variable~cat_variable, dataset name) 
```

3.  Repeat steps 1 and 2 above but use different sample sizes (e.g. 10 , then 100, then 1000, then 10,000 obsv. per factor level)

4.  Repeat steps 1 and 2 above but now test between a categorical and continuous variable in the perchlorate data set (or any of your favorite data sets)

## R INTERLUDE \| Parametric t-test in R {.smaller}

-   Now, apply the equations for the t-statistic above to your simulated sample to perform the same test, but using a resampling approach
    -   hint: go back to your Bootstrapping script
-   Use this approach to calculate a distribution of random t statistics assuming the the null hypothesis of no difference is true.
-   Now, compare your observed value to your created null distribution.
-   What is the probability of that value occurring by chance under the null?
    -   This is your p-value! (Assume you’re doing a one-tailed test)
    -   hint: Proportion of values in the null distribution equal to or larger than the observed t-value = p-value

## R INTERLUDE \| Permutation t-test in R

-   Randomization test example 1
-   Data: The number of successful broods of pseudoscorpion females that were mated twice to either a single male (SM) or two different males (DM).
    -   SM: `4 0 3 1 2 3 4 2 4 2 0 2 0 1 2 6 0 2 3 3`
    -   DM: `2 0 2 6 4 3 4 4 2 7 4 1 6 3 6 4`
-   Observed difference in the means between the two
    -   $\bar{Y}_{SM}$ − $\bar{Y}_{DM}$ = 2.2−3.625 = −1.425

## R INTERLUDE \| Permutation t-test in R {.smaller}

-   Steps of the randomization test
-   First, create a randomized data set in which the measurements are randomly reassigned (without replacement) to the two groups
    -   For example, the following
    -   SM: `4 0 7 4 2 2 2 1 4 4 0 3 3 4 6 2 4 6 0 0`
    -   DM: `2 2 3 3 2 3 3 4 1 2 1 4 6 6 2 0`
-   Second, calculate the test statistic measuring the association between variables
    -   difference between group means
    -   this is the first bootstrap replicate
-   Repeat steps 1 and 2 many times to produce many bootstrap replicates

## R INTERLUDE \| Permutation t-test in R {.smaller}

-   Below is the **null distribution** of the test statistic from 10,000 replications
    -   This is produced by the randomized assignment of values to each group (SM or DM)
    -   Thus this is the distribution we'd expect under the null hypothesis of no difference
-   The observed value from the data is –1.425
-   The area in red is the tail beyond this observed value and is therefore the **bootstrap p-value**

```{r, echo=FALSE, out.width='40%', fig.align='center'}
knitr::include_graphics("images/week_3.010.jpeg")
```

## R INTERLUDE \| Permutation t-test in R

-   Of these 10,000 randomizations, only 176 had a test statistic (difference between means) equal to or less than the observed value, –1.425.
-   You can use the simulated null distribution in the same way as t or F distribution in conventional tests.
-   Proportion of values in the null distribution equal or larger than the observed value of the test statistic is 176/10000 = 0.0176.

## R INTERLUDE \| Perform a one-way ANOVA with power

-   Read in the perchlorate data again
-   Perform an ANOVA to test Strain on T4_Hormone_Level, but log-transform (base 10) the T4 variable

```{r, eval=FALSE, echo=TRUE}
perc <- read.table('perchlorate_data.tsv', header=T, sep='\t')
x <- perc$Strain
y <- log10(perc$T4_Hormone_Level)

MyANOVA <- aov(y ~ x)
summary (MyANOVA)
boxplot(y ~ x)
```

## R INTERLUDE \| Perform a one-way ANOVA with power

-   Consider the parameters of this test related to power:
    -   The per-group sample sizes
    -   The standard deviation (use the higher within-group sd)
    -   The effect size (\|difference between means\| / within-grp sd)
-   For more complex ANOVA power calculations (\>2 groups):
    -   The total variance
    -   The within-group variance (use the higher one)

## R INTERLUDE \| post hoc and a priori power analyses

Based on your results, calculate the power for your ANOVA.

```{r, eval=FALSE, echo=TRUE }
   pwr.t2n.test(n1=xxx, n2=xxx, d=xxx, sig.level=.05, power=NULL)
```

Check out the functions in the ‘pwr’ library (Unnecessary in this case, but could use ANOVA version):

```{r, eval=FALSE, echo=TRUE}
   pwr.anova.test(k=2, n=190, f=0.25, sig.level=.05, power=NULL)
```

## R INTERLUDE \| post hoc and a priori power analyses

-   effect size approximations:
    -   f=0.1 (small)
    -   f=0.25 (medium)
    -   f=0.4 (large)
-   see http://www.statmethods.net/stats/power.html

## R INTERLUDE \| post hoc and a priori power analyses {.flexbox .vcenter}

-   Let’s say you have to repeat the experiment, but your IACUC wants you to get by by using fewer fish.
-   You want to be able to detect a minimum mean difference of 1.3 T4 units (about 0.114 on the log10 scale), at a power of 90%.
-   First, divide 0.114 by std.dev. of transformed WK values (the higher std. dev. of the two groups) to get a conservative “d”.
-   What kind of sample size for the WK group would you need???
-   (Again use the pwr.t2n.test() function, but this time specify the WK sample size as the unknown parameter)



















