---
title: "Week 4a - Statistics for Bioengineering"
author: "Bill Cresko"
format: 
  revealjs:
    footer: BioE_Stats_2025 - Knight Campus
    transition: fade
    transition-speed: slow
editor: visual
---

```{r}
library(tidyverse)
library(gt)
library(readxl)
library(nycflights23)
theme_set(theme_minimal())
```

# Sampling, parameter estimation, error and power

## This week

-   Finish parameter estimation

-   t-test

-   Hypothesis testing

-   Power

-   Start linear models

## **Poisson Probability Distribution** {.smaller}

-   Another common situation in bioengineering is when each trial is discrete, but the number of observations of each outcome is observed/counted

-   Some examples are

    -   counts of snails in several plots of land
    -   observations of the firing of a neuron in a unit of time
    -   the number of muscle contractions during a bout of exercise
    -   count of genes in a genome binned to units of 500kb

-   Just like before you have 'successes', but

    -   now you count them for each replicate
    -   the replicates now are units of area or time
    -   the values can now range from 0 to a large number

## Poisson Probability Distribution {.smaller}

<br>

-   For example, you can examine 100 participants
    -   count the number of jumps they can perform in 2 minutes
    -   what is the probability of observing a particpant with `r` jumps?
-   `Pr(Y=r)` is the probability that the number of occurrences of an event `y` equals a count `r` in the total number of trials

<br>

$$Pr(Y=r) = \frac{e^{-\mu}\mu^r}{r!}$$

## Poisson Probability Distribution {.smaller}

-   Note that the Poisson is a single parameter function because $\mu = \sigma^2$
-   The two together are often just represented by $\lambda$

$$Pr(y=r) = \frac{e^{-\lambda}\lambda^r}{r!}$$

-   This means that for a variable that is truly Poisson distributed:
    -   the mean and variance should be equal to one another
    -   variables that are approximately Poisson distributed but have a larger variance than mean are often called 'overdispersed'
    -   quite common in RNA-seq and microbiome data

## Poisson Probability Distribution with gene length by bins of 500 nucleotides {.smaller}

```{r, echo=FALSE, out.width='80%', fig.align='center'}
knitr::include_graphics("images/week_2.004.jpeg")
```

## Poisson Probability Distribution with increasing parameter values of $\lambda$ {.smaller}

```{r, echo=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics("images/week_2.005.jpeg")
```

## Testing Poisson Distributions {.smaller}

Number of counts (x) given a mean and variance of lambda

```{r, echo=TRUE, out.width="40%", fig.align="center"}
dpois(x=2, lambda=1)
plot(dpois(x=1:10, lambda=3))
```

------------------------------------------------------------------------

# Some other common probability distributions

## **Geometric Distribution** {.smaller}

-   If a single event has two possible outcomes the probability of having to observe `k` trials before the first "one" appears is given by the **geometric distribution**
-   The probability that the first "one" would appear on the first trial is `p`, but the probability that the first "one" appears on the second trial is `(1-p)*p`
-   By generalizing this procedure, the probability that there will be `k-1` failures before the first success is:

$$P(X=k)=(1-p)^{k-1}p$$

-   mean = $\frac{1}{p}$
-   variance = $\frac{(1-p)}{p^2}$

## **Geometric Distribution** {.smaller}

-   Example: If the probability of extinction of an endangered population is estimated to be 0.1 every year, what is the expected time until extinction?
-   The distribution gives the probability of extinction in a given year (requiring that the population did not go extinct in all of the years prior)
-   If we want to know the probability of the population going exticnt by year 4, we simply add up the probabilities for years 1-3 using "or" rules

```{r, echo=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics("images/prob.017.jpeg")
```

## Testing Geometric Distributions {.smaller}

-   dgeom gives the density (probability) of an event (p) after (x) failures

```{r, echo=TRUE, out.width="40%", fig.align="center"}
dgeom(x=20, p=0.1)
plot(dgeom(1:20,0.1))
```

## Testing Geometric Distributions {.smaller}

-   pgeom gives the cumulative probability of event (p) in (q) trials

```{r, echo=TRUE}
pgeom(q=20, p=0.1)
plot(pgeom(q=1:20, p=0.1))
```

------------------------------------------------------------------------

## **Negative Binomial Distribution** {.smaller}

-   Extension of the geometric distribution describing the waiting time until `r` "ones" have appeared.
-   Generalizes the geometric distribution
-   Probability of the $r^{th}$ "one" appearing on the $k^{th}$ trial:

$$P(X=k)=(\frac{k-1}{r-1})p^{r-1}(1-p)^{k-r}p$$

<br>

which simplifies to

$$P(X=k)=(\frac{k-1}{r-1})p^{r}(1-p)^{k-r}$$

-   mean = $\frac{r}{p}$
-   variance = $r(1-p)/p^2$

## **Negative Binomial Distribution** {.smaller}

-   Example: If a predator must capture 10 prey before it can grow large enough to reproduce
-   What would the mean age of onset of reproduction be if the probability of capturing a prey on any given day is 0.1?
-   Notice that the variance is quite high (\~1000) and that the distribution looks quite skewed

```{r, echo=FALSE, out.width='50%', fig.align='center'}
knitr::include_graphics("images/prob.018.jpeg")
```

# Continuous Probability Distributions

## **Continuous probability distributions** {.smaller}

P(observation lies within dx of x) = f(x)dx

$$P(a\leq X \leq b) = \int_{a}^{b} f(x) dx$$

Remember that the indefinite integral sums to one

$$\int_{-\infty}^{\infty} f(x) dx = 1$$

## Continuous probabilities {.smaller}

<br>

`E[X]` may be found by integrating the product of `x` and the probability density function over all possible values of `x`:

$$E[X] = \int_{-\infty}^{\infty} xf(x) dx $$

<br>

$Var(X) = E[X^2] - (E[X])^2$, where the expectation of $X^2$ is

$$E[X^2] = \int_{-\infty}^{\infty} x^2f(x) dx $$

## **Uniform Distribution** {.smaller}

<br>

$$E[X] = \int_{a}^{b} x\frac{1}{b-a} dx = \frac{(a+b)}{2} $$

<br>

```{r, echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/prob.019.jpeg")
```

## **Uniform Distribution** {.smaller}

-   Means that the probability is equal for all possible outcomes
-   Like drawing m&m out of a bag with equal proportions of colors

```{r, echo=TRUE}
dunif(x=1,min=0, max=10)
plot(dunif(1:10, 0, 10))
```

## Exponential Distribution {.smaller}

<br>

$$f(x)=\lambda e^{-\lambda x}$$

<br>

-   `E[X]` can be found be integrating $xf(x)$ from 0 to infinity

<br>

-   leading to the result that

<br>

-   $E[X] = \frac{1}{\lambda}$
-   $E[X^2] = \frac{1}{\lambda^2}$

## Exponential Distribution {.smaller}

-   For example, let equal the instantaneous death rate of an individual.
-   The lifespan of the individual would be described by an exponential distribution (assuming that does not change over time).

```{r, echo=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics("images/prob.020.jpeg")
```

## Exponential Distribution {.smaller}

```{r, echo=TRUE}
dexp(10, rate = 0.1)
plot(dexp(1:100, rate = 0.1))
```

------------------------------------------------------------------------

## **Gamma Distribution** {.smaller}

-   The gamma distribution generalizes the exponential distribution.
-   It describes the waiting time until the $r^{th}$ event for a process that occurs randomly over time at a rate $\lambda$ :

<br>

$$f(x) = \frac{e^{-\lambda x}\lambda x^{r-1}}{(r-1)!}\lambda$$

<br>

$$ Mean =  \frac{r}{\lambda} $$ $$ Variance = \frac{r}{\lambda^2} $$

## Gamma Distribution {.smaller}

-   **Example**: If, in a PCR reaction, DNA polymerase synthesizes new DNA strands at a rate of 1 per millisecond, how long until 1000 new DNA strands are produced?

-   Assume that DNA synthesis does not deplete the pool of primers or nucleotides in the chamber, so that each event is independent of other events in the PCR chamber.

-   The gamma distribution generalizes the exponential distribution.

-   It describes the waiting time until the $r^{th}$ event for a process that occurs randomly over time at a rate $\lambda$

# Normal (Gaussian) probability distribution {.smaller}

## Normal PDF - ($\mu$ and $\sigma$) {.smaller}

$$f(x) = \frac{1}{\sqrt{2\pi\sigma}} \, \mathrm{e}^{-\frac{(x - \mu)^2}{2\sigma^2}}$$

where $$\large \pi \approx 3.14159$$

$$\large \mathrm{e} \approx 2.71828$$

To write that a variable (v) is distributed as a normal distribution with mean $\mu$ and variance $\sigma^2$, we write the following:

$$\large v \sim \mathcal{N} (\mu,\sigma^2)$$

## 

```{r, echo=FALSE, out.width='90%', fig.align='center'}
knitr::include_graphics("images/week_2.011.jpeg")
```

## 

```{r, echo=FALSE, out.width='50%', fig.align='center'}
knitr::include_graphics("images/week_2.012.jpeg")
```

## Estimates of mean and variance {.smaller}

Estimate of the mean from a single sample

$$\Large \bar{x} = \frac{1}{n}\sum_{i=1}^{n}{x_i} $$

Estimate of the variance from a single sample

$$\Large s^2 = \frac{1}{n-1}\sum_{i=1}^{n}{(x_i - \bar{x})^2} $$

## Normal PDF {.smaller}

```{r, echo=FALSE, out.width='90%', fig.align='center'}
knitr::include_graphics("images/week_2.010.jpeg")
```

## Why is the Normal special in biosciences? {.smaller}

```{r, echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/week_2.013.jpeg")
```

## Why is the Normal special in biosciences? {.smaller}

```{r, echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/week_2.015.jpeg")
```

## Why is the Normal special in biosciences? {.smaller}

```{r, echo=FALSE, out.width='60%', fig.align='center'}
knitr::include_graphics("images/week_2.014.jpeg")
```

## Parent-offspring resemblance {.smaller}

```{r, echo=FALSE, out.width='45%', fig.align='center'}
knitr::include_graphics("images/week_2.016.jpeg")
```

## Genetic model of complex traits {.smaller}

```{r, echo=FALSE, out.width='90%', fig.align='center'}
knitr::include_graphics("images/week_2.017.jpeg")
```

## Distribution of $F_2$ genotypes \| really just binomial sampling {.smaller}

```{r, echo=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics("images/week_2.018.jpeg")
```

## Why else is the Normal special?

-   It is the basis of estimation and precision of the expected value of **all distributions**

-   Provides a mathematical basis for moving from **single samples to point estimates**.

-   Provides a way to use simulation to generate **empirical sample and test distributions** through Monte Carlo approaches

## z-scores of normal variables {.smaller}

-   Often we want to make variables more comparable to one another
-   For example, consider measuring the leg length of mice and of elephants
    -   Which animal has longer legs in absolute terms?
    -   Proportional to their body size?
-   A good way to answer these last questions is to use 'z-scores'

## z-scores of normal variables {.smaller}

-   z-scores are standardized to a mean of 0 and a standard deviation of 1
-   We can modify any normal distribution to have
    -   a mean of 0 and
    -   a standard deviation of 1
-   Another term for this is the standard normal distribution

$$\huge z_i = \frac{(x_i - \bar{x})}{s}$$

## Common theme in R for distributions {.smaller}

+-------------+---------------------------+-------------------------+-------------------+------------------------------+
|             | d                         | p                       | q                 | r                            |
|             |                           |                         |                   |                              |
|             | probability mass function | cumulative distribution | quantile function | pseudorandom number generate |
+=============+:=========================:+:=======================:+:=================:+:============================:+
| binomial    | dbinom                    | pbinom                  | qbinom            | rbinom                       |
+-------------+---------------------------+-------------------------+-------------------+------------------------------+
| poisson     | dpois                     | ppois                   | qpois             | rpois                        |
+-------------+---------------------------+-------------------------+-------------------+------------------------------+
| exponential | dexp                      | pexp                    | qexp              | rexp                         |
+-------------+---------------------------+-------------------------+-------------------+------------------------------+
| normal      | dnorm                     | pnorm                   | qnorm             | rnorm                        |
+-------------+---------------------------+-------------------------+-------------------+------------------------------+

------------------------------------------------------------------------

# Sampling and estimation

## Parameter Estimation {.smaller}

-   Estimation is the process of inferring a population parameter from sample data
-   The value of one sample estimate is almost never the same as the population parameter because of random sampling error
-   Most will be close, but some will be far away
-   Sampling distribution of an estimate
    -   all values we might have obtained from our sample
    -   probabilities of occurrence
-   Standard error of an estimate
    -   standard deviation of a sampling distribution
    -   measures the precision of the parameter estimate
    -   NO ESTIMATE IS USEFUL WITHOUT IT!

## Parameter Estimation {.smaller}

-   **Parametric** (a few special exceptions, like the sample mean and its standard error)
-   **Resampling** - bootstrapping and randomization to create empirical null distributions
-   **Ordinary Least Squares (OLS)** - optimized procedure that produces one definitive result, easy to use but no estimates of confidence
-   **Maximum Likelihood (ML)** - Can provide model-based estimates with confidence, but harder to calculate
-   **Bayesian Approaches** - Incorporates prior information into ML estimation

## Accuracy vs. Precision {.smaller}

```{r, echo=FALSE, out.width='50%', fig.align='center'}
knitr::include_graphics("images/week_2.024.jpeg")
```

## Accuracy vs. Precision {.smaller}

-   **Accuracy** is the closeness of an estimated value to its true value
-   **Precision** is the closeness of repeated estimates to one another
-   Our goal is to have **unbiased estimates** that are the most precise
-   We have to **estimate parameters** and **test hypotheses** by taking **samples** that approximate the underlying distribution
-   The goal of **replication** is to quantify variation at as many levels in a study as possible
-   The goal of **randomization** is to avoid bias as much as possible

# The Central Limit Theorem {.smaller}

## The Central Limit Theorem {.smaller}

-   For most real world data sets we can’t empirically determine a sampling distribution by taking many actual samples, because we often have just the one sample.
-   Fortunately, we can rely on the **Central Limit Theorem** to make some assumptions about sampling distributions, particularly when estimating a mean from a single sample, or when estimating most any parameter using a “pseudo” or re-sampling process we refer to as “**bootstrapping**”

## Standard Error of the Mean (SEM) {.smaller}

$$\huge \sigma_{\bar{x}} \approx s_{\bar{x}} = \frac{s}{\sqrt{n}} $$

-   where $s_{\bar{x}}$ is the estimated standard error of the distribution of the mean estimates
-   which is usually just referred to as the 'standard error of the mean (SEM)
-   note that this **is not** the standard deviation of the original distribution
-   importantly, the SEM will go down as the sample size increases

## Calculating the Standard Error of the Mean (SEM) {.smaller}

-   Think conceptually - how will SEM change as sample size increases?

```{r, echo=TRUE}
set.seed(32)
true_pop <- rnorm(n=1000, mean=2, sd=5)
samps_var <- replicate(n = 50, sample(true_pop, size = 5))
samps_var_means <- apply(samps_var, 2, mean)
sem = sd(samps_var_means)/ sqrt(length(samps_var_means))
print(sem)
```

```{r, echo=TRUE}
set.seed(32)
true_pop <- rpois(n=1000, lambda = 5)
samps_var <- replicate(n = 50, sample(true_pop, size = 5))
samps_var_means <- apply(samps_var, 2, mean)
sem = sd(samps_var_means)/ sqrt(length(samps_var_means))
print(sem)
```

```{r, echo=TRUE}
set.seed(32)
true_pop <- rpois(n=1000, lambda = 5)
samps_var <- replicate(n = 50, sample(true_pop, size = 50))
samps_var_means <- apply(samps_var, 2, mean)
sem = sd(samps_var_means)/ sqrt(length(samps_var_means))
print(sem)
```

## Estimation and confidence intervals {.vcenter .flexbox}

```{r, echo=FALSE, out.width='90%', fig.align='center'}
knitr::include_graphics("images/week_2.026.jpeg")
```

## Estimation and confidence intervals {.vcenter .flexbox}

```{r, echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/week_2.027.jpeg")
```

## Estimation and confidence intervals {.vcenter .flexbox}

```{r, echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/week_2.028.jpeg")
```

## Confidence Intervals

-   A confidence interval is a range of values about a parameter estimate, such that we are X% certain that the true population parameter value lies within that interval.
-   For now, know that for a normally distributed sample, a confidence interval about the population mean can be calculated using the t.test() function in base R.
-   The 95% confidence interval is commonly reported in statistical analysis results, by convention, but other values are occasionally reported as well.

## Coefficient of Variation

-   To make standard deviations comparable across populations with very different means, we can instead compare a standardized metric called the “coefficient of variation” (CV), which is simply the sample standard deviation divided by the sample mean (and usually expressed as a % by multiplying by 100).

# Basics of bootstrapping via re-sampling

## Bootstrapping

-Unfortunately, most other kinds of estimates (anything not the mean) do not have this amazing property, but we can rely on another approach to calculate the standard error. - This involves generating your own sampling distribution for the estimate using the “**bootstrap**,” a method invented by Efron (1979). - We call the bootstrap, and other methods that do not rely on distributional assumptions of the variable itself, “**nonparametric**” approaches.

## The etymology of the term 'bootstrap' {.smaller}

```{r, echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/week_2.030.jpeg")
```

## Why the bootstrap is good {.smaller}

-   Can be applied to almost any sample statistic
    -   This includes means, proportions, correlations, regression
-   Works when there is no ready formula for a standard error
    -   For example the median, trimmed mean, correlation, eigenvalue, etc.
-   Is nonparametric, so doesn’t require normally-distributed data
-   Works for estimates based on complicated sampling procedures or calculations

## Easy steps for bootstrapping in R {.smaller}

1.  Take a random sample (with replacement) from your sample data
2.  Calculate the estimate using the measurements in the bootstrap sample (step 1). This is the first bootstrap replicate estimate
3.  Repeat steps 1 and 2 a large number of times (1000 times is reasonable)
4.  Calculate the sample standard deviation of all the bootstrap replicate estimates obtained in step 3 (SSD = sd(sample)/sqrt(sample size))

## Bootstrapping {.smaller}

-   The resulting quantity is called the “bootstrap standard error”
-   The bootstrap can be applied to almost any sample statistic, including means, proportions, correlations, and regression parameters.
-   It works when there is no ready formula for a standard error, for example when estimating the median, trimmed mean, correlation, eigenvalue, etc.
-   It is nonparametric, so doesn’t require normally-distributed data, as mentioned.
-   It works well for parameter estimates that are based on complicated sampling procedures or calculations. For example, it is used to assess confidence in local relationships within phylogenetic trees.

# Loops in R - a good way to create even more complicated null distributions

## Loops in R

-   R is very good at performing repetitive tasks.
-   If we want a set of operations to be repeated several times we use what’s known as a loop.
-   When you create a loop, R will execute the instructions in the loop a specified number of times or until a specified condition is met.
-   There are two common types of loop in R: the `for loop` and the `while loop`

## For loops

-   The most commonly used loop structure when you want to repeat a task a defined number of times is the for loop. The most basic example of a for loop is:
-   How does this appear to be working?

```{r,  echo=T}
# Notice the sequence of parentheses and brackets used in this example
for (i in 1:5) {
  print(i)
}
```

## While loops

-   Another type of loop that you may use is the while loop.
-   The while loop is used when you want to keep looping until a specific logical condition is satisfied.

```{r,  echo=T}
i <- 0
while (i <= 4) {
  i <- i + 1
  print(i)
}
```

## If and Else Statements

-   Conditional statements are how you inject some logic into your code.
-   The most commonly used conditional statement is `if.`
    -   Whenever you see an `if` statement, read it as ‘If X is TRUE, then do a thing’.
-   Another statement is `else`, which extends the logic to ‘If X is TRUE, do a thing, or else do something different’.

## A programming joke for conditional statements {.smaller}

A programmer’s partner says: ‘Please go to the store and buy a carton of milk and if they have eggs, get six.’

The programmer returned with 6 cartons of milk.

When the partner sees this, and exclaims ‘Why the heck did you buy six cartons of milk?’

The programmer replied ‘They had eggs’

```{r,  echo=T}
eggs <- TRUE # Whether there were eggs in the store

  if (eggs == TRUE) { # If there are eggs
  n.milk <- 6 # Get 6 cartons of milk
    } else { # If there are not eggs
  n.milk <- 1 # Get 1 carton of milk
  }
```

------------------------------------------------------------------------

## R INTERLUDE - Simulate a population and sample it! {.smaller}

## Simulations to compare parameter estimates {.smaller}

-   Let's use our distribution functions from last time to set up some data to play with
-   Let's imagine our data is made up of counts, with an average of 3 counts - which distribution would fit that data best?
-   Ex: number of hours per day spent doing homework by UO undergraduates

```{r,  echo=TRUE, out.width="50%"}
set.seed(32)
true_pop <- rpois(n=10000, lambda = 3)
hist(true_pop, xlim = c(0,16))
```

## Calculating parameters

-   How would we calculate the mean and range for this population?

```{r, echo=TRUE, eval=TRUE}
mean(true_pop)
range(true_pop)
median(true_pop)
```

-   how about the variance and the standard deviation?

## Sampling Exercise

-   Since we are working with simulated data, we can also afford to simulate our sampling!
-   Try taking a sample from our `true_pop` dataset and change the sample size, then calculate the mean and range for your sample and see how it compares to the true values.
-   How many college students are you including in your survey?

## Randomness in Sampling

-   Because of the randomness of sampling, you may get close to the true estimates even with a small sample size
-   But your results will change each time you take a new sample of the same size
-   How do we get a feel for how accurate each sample size is? Or which sample size is recommended?

## Surveying your Sampling

Step 1 - take 50 samples of size 10 Step 2 - Calculate the mean from each sample Step 3 - Plot the distribution of sample mean estimates

```{r, echo = TRUE, out.width="50%"}
set.seed(32)
true_pop <- rpois(n=10000, lambda = 3)
samps_var <- replicate(50, sample(true_pop, 10))
samps_var_means <- apply(samps_var, 2, mean)
```

## Histograms

::::: columns
::: {.column width="50%"}
```{r}
hist(true_pop)
```
:::

::: {.column width="50%"}
```{r}
hist(samps_var_means)
```
:::
:::::

## Surveying your Sampling

-   This sampling variation is what we have to deal with, and account for, as empirical scientists.
-   If this had been a real-world scenario, we likely would be basing our estimate for the true mean on just a single sample mean.
-   Getting close to the idea of **power** - does our experimental design have the power to detect the parameters we are interested in?

## CHALLENGE - Bootstrapping to produce a confidence interval {.smaller}

-   The 2.5th and 97.5th percentiles of the bootstrap sampling distribution are a passable 95% confidence interval
-   Note that no transformations or normality assumptions needed
-   You can use the `quantile()` function to calculate these
-   You can also use `qnorm`
-   Use R to figure out the bootstrap distribution for other parameters (such as variance).
-   Try to produce bootstrap distributions for the mean and variance of gene expresssion of one gene from the stickleback data set
-   Plot the resulting distributions
-   Determine the value of the 2.5th and 97.5th percentiles

------------------------------------------------------------------------

#  {.smaller}
