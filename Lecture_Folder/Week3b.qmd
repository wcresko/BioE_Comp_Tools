---
title: "Week 3b - Statistics for Bioengineering"
author: "Bill Cresko"
format: 
  revealjs:
    footer: BioE_Stats_2025 - Knight Campus
    transition: fade
    transition-speed: slow
editor: visual
---

```{r}
library(tidyverse)
library(gt)
library(readxl)
library(nycflights23)
theme_set(theme_minimal())
```

------------------------------------------------------------------------

# Sampling, parameter estimation, error and power

## Key dplyr verbs

There are five key dplyr verbs that you need to learn.

1.  `filter`: Filter (i.e. subset) observations (rows) based on their values.\
2.  `select`: Select (i.e. subset) variables (columns) by their names:\
3.  `arrange`: Arrange (i.e. reorder) rows based on their values.\
4.  `mutate`: Create new columns.\
5.  `summarise`: Collapse multiple rows into a single summary value.<sup>1</sup>

## `filter()`, `arrange()` & `select()`

```{r, echo=T, eval=F}
filter(flights, month == 11 | month == 12)
```

```{r, echo=T, eval=F}
arrange(flights, year, month, day)
```

```{r, echo=T, eval=F}
select(flights, year)
```

```{r, echo=FALSE, out.width='60%', fig.align='center'}
knitr::include_graphics("images/week_3.015.jpeg")
```

## conditionals {.smaller}

-   `==` equals exactly

-   `<, <=` is smaller than, is smaller than or equal to

-   `>`, \>=\` is bigger than, is bigger than or equal to

-   `!=` not equal to

-   `!` NOT operator, to specify things that should be omitted

-   `&` AND operator, allows you to chain two conditions which must both be met

-   `\|` OR operator, to chains two conditions when at least one should be met

-   `%in%` belongs to one of the following (usually followed by a vector of possible values)

The `AND (`&`) and the OR (`\|\`) operators are also super useful when you want to separate data based on multiple conditions.

## `mutate()` & `transmutate()`

```{r, echo=T, eval=F}
library(nycflights23)

mutate(flights,
  gain = arr_delay - dep_delay,
  hours = air_time / 60,
  gain_per_hour = gain / hours
)
```

```{r, echo=T, eval=F}
flights %>%
  mutate(
  gain = arr_delay - dep_delay,
  hours = air_time / 60,
  gain_per_hour = gain / hours
)
```

```{r, echo=T, eval=F}
flights_updated <- flights %>%
  mutate(
  gain = arr_delay - dep_delay,
  hours = air_time / 60,
  gain_per_hour = gain / hours
)
```

## `group_by( )` & `summarize( )` {.smaller}

This first function allows you to aggregate data by values of categorical variables

```{r, echo=T, eval=F}
by_day <- group_by(flights, year, month, day)
```

Once you have done this aggregation, you can then calculate values (in this case the mean) of other variables split by the new aggregated levels of the categorical variable

```{r, echo=T, eval=F}
summarise(by_day, delay = mean(dep_delay, na.rm = TRUE))
```

-   Note - you can get a lot of missing values!
-   That’s because aggregation functions obey the usual rule of missing values:
    -   if there’s any missing value in the input, the output will be a missing value.
    -   fortunately, all aggregation functions have an na.rm argument which removes the missing values prior to computation

## Other dplyr goodies {.smaller}

-   `group_by` and `ungroup`: For (un)grouping. - Particularly useful with the `summarise` and `mutate` commands, as we've already seen.\

-   `slice`: Subset rows by position rather than filtering by values. - E.g. `starwars %>% slice(c(1, 5))`\

-   `pull`: Extract a column from as a data frame as a vector or scalar. - E.g. `starwars %>% filter(gender=="female") %>% pull(height)`\

-   `count` and `distinct`: Number and isolate unique observations. - E.g. `starwars %>% count(species)`, or `starwars %>% distinct(species)`\

-   You could also use a combination of `mutate`, `group_by`, and `n()`, e.g. `starwars %>% group_by(species) %>% mutate(num = n())`.

------------------------------------------------------------------------

##  {.smaller}

# Probability, distributions and sampling

## Random variables & probability {.smaller}

-   **Probability** is the expression of belief in some future outcome

-   A **random variable** can take on different values with different probabilities

-   The **sample space** of a random variable is the universe of all possible values

-   The **sample space** can be represented by a

    -   **probability mass distribution** (discrete)
    -   **probability density function (PDF)** (continuous)
    -   algebra and calculus are used for each respectively
    -   probabilities of a sample space **always sum to 1.0**

-   How does it make sense that a sample space will always sum to 1?

## Bernoulli distribution {.smaller}

-   Describes the expected outcome of a single event with probability `p`

-   Example of flipping of a **fair** coin once

$$Pr(X=\text{Head}) = \frac{1}{2} = 0.5 = p $$

$$Pr(X=\text{Tails}) = \frac{1}{2} = 0.5 = 1 - p = q $$

## Bernoulli distribution {.smaller}

-   If the coin isn't fair then $p \neq 0.5$
-   However, the probabilities still sum to 1

$$ p + (1-p) = 1 $$ $$ p + q = 1 $$

-   Same is true for other binary possibilities
    -   success or failure
    -   yes or no answers
    -   choosing an allele from a population based upon allele frequencies (Hardy-Weinberg ring any bells??)

## Probability rules {.smaller}

-   Flip a coin twice
-   Represent the first flip as ‘X’ and the second flip as ‘Y’

$$ Pr(\text{X=H and Y=H}) = p*p = p^2 $$ $$ Pr(\text{X=H and Y=T}) = p*q = pq = p^2 $$ $$ Pr(\text{X=T and Y=H}) = q*p = pq $$ $$ Pr(\text{X=T and Y=T}) = q*q = q^2 $$

## Probability rules {.smaller}

-   Probability that the `H` and `T` can occur in any order

$$ \text{Pr(X=H) or Pr(X=T)} = p+q=1$$

$$ \text{Pr(X=H and Y=T) or Pr(X=T and Y=H)} = $$ $$ (p*q) + (p*q) = 2pq $$

-   These are the **'and'** and **'or'** rules of probability
    -   'and' means multiply the probabilities
    -   'or' means sum the probabilities
    -   most probability distributions can be built up from these simple rules

## Let's simulate some coin flips {.smaller}

```{r, echo=TRUE}
# tossing a fair coin
coin <- c("heads", "tails")

sample(coin)

```

## Let's simulate some coin flips {.smaller}

-   What happens when we change the probabilities or the sample size? How confident are we that our coin is fair?

```{r, echo=TRUE, out.width="50%"}
flips <- sample(coin, prob = c(0.5, 0.5), size=13, replace=TRUE)
barplot(table(flips))
```

## Joint probability {.smaller}

$$Pr(X,Y) = Pr(X) * Pr(Y)$$

-   Note that this is true for two **independent** events
-   However, for two non-independent events we also have to take into account their **covariance**
-   To do this we need **conditional probabilities**

## Conditional probability {.smaller}

-   For two **independent** variables: Probability of Y, given X, or the probability of X, given Y.

$$Pr(Y|X) = Pr(Y)\text{ and }Pr(X|Y) = Pr(X)$$

-   For two **non-independent** variables

$$Pr(Y|X) \neq Pr(Y)\text{ and }Pr(X|Y) \neq Pr(X)$$

-   Variables that are non-independent have a shared variance, which is also known as the **covariance**
-   Covariance standardized to a mean of zero and a unit standard deviation is **correlation**

## Expectation and Moments of Distributions {.smaller}

-   Distributions have **moments** that can be estimated
-   **1st moment** - The expectation or mean of a discrete random variable X is:

$$E[X] = \sum_{\text{all x}}^{}xP(X=x) = \mu$$

-   Often we want to know how dispersed the random variable is around its mean.
-   **2nd moment** - the measure of dispersion is the variance

$$Var(X) = E[X^2] = \sigma^2$$ - and the standard deviation is just the square root of the variance

$$ \sqrt{\sigma^2}$$ - There are higher moments of distributions (e.g. skew and kurtosis)

## What is Likelihood vs. Probability? {.smaller}

-   The **probability** of an event is the proportion of times that the event would occur if we repeated a random trial over and over again under the same conditions.

-   The **likelihood** is a conditional probability of a parameter value given a set of data

-   The likelihood of a population parameter **equaling a specific value, given the data**

`L[parameter|data] = Pr[data|parameter]`

-   **Likelihood function** is the range of likelihoods over the parameter space

-   **Maximum likelihood** is the highest value of the likelihood function

-   What is a **Bayesian estimate**? - the use of prior distribution to update a posterior distribution

------------------------------------------------------------------------

## **Binomial Distribution** {.smaller}

-   A **binomial distribution** results from the **combination** of several independent Bernoulli events

-   **Example**

    -   Pretend that you flip 20 fair coins
        -   or collect alleles from a heterozygote
    -   Now repeat that process and record the number of heads
    -   We expect that most of the time we will get approximately 10 heads
    -   Sometimes we get many fewer heads or many more heads

## Binomial Distribution {.smaller}

-   The distribution of probabilities for each combination of outcomes is

$$\large f(k) = {n \choose k} p^{k} (1-p)^{n-k}$$

-   `n` is the total number of trials
-   `k` is the number of successes
-   `p` is the probability of success
-   `q` is the probability of not success
-   For binomial as with the Bernoulli `p = 1-q`

## Binomial Probability Distribution {.smaller}

-   Note that the binomial function incorporates both the 'and' and 'or' rules of probability
-   This part is the probability of each outcome (multiplication)

$$\large p^{k} (1-p)^{n-k}$$

This part (called the binomial coefficient) is the number of different ways each combination of outcomes can be achieved (summation)

$$\large {n \choose k}$$ Together they equal the probability of a specified number of successes

## Binomial Probability Distribution {.smaller}

```{r, echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/week_2.003.jpeg")
```

## Testing Binomial Distributions {.smaller}

-   dbinom gives the density (probability) of number successes (x) in number trials (size), with (prob) probability between 0-1

```{r, echo=TRUE, out.width="40%", fig.align="center"}
# dbinom(x=5, size=10, p=0.5)
# 0.246
plot(dbinom(x=1:10, size=10, p=0.5))
```

## Testing Binomial Distributions {.smaller}

-   pbinom gives the cumulative probability of reaching at least (q) number of successes after (size) number of trials

```{r}
plot(pbinom(q=1:100, size=100, p=0.5))
```

------------------------------------------------------------------------

## **Poisson Probability Distribution** {.smaller}

-   Another common situation in bioengineering is when each trial is discrete, but the number of observations of each outcome is observed/counted

-   Some examples are

    -   counts of snails in several plots of land
    -   observations of the firing of a neuron in a unit of time
    -   count of genes in a genome binned to units of 500kb

-   Just like before you have 'successes', but

    -   now you count them for each replicate
    -   the replicates now are units of area or time
    -   the values can now range from 0 to a large number

## Poisson Probability Distribution {.smaller}

<br>

-   For example, you can examine 1000 genes
    -   count the number of base pairs in the coding region of each gene
    -   what is the probability of observing a gene with 'r' bp?
-   `Pr(Y=r)` is the probability that the number of occurrences of an event `y` equals a count `r` in the total number of trials

<br>

$$Pr(Y=r) = \frac{e^{-\mu}\mu^r}{r!}$$

## Poisson Probability Distribution {.smaller}

-   Note that this is a single parameter function because $\mu = \sigma^2$
-   The two together are often just represented by $\lambda$

$$Pr(y=r) = \frac{e^{-\lambda}\lambda^r}{r!}$$

-   This means that for a variable that is truly Poisson distributed:
    -   the mean and variance should be equal to one another
    -   variables that are approximately Poisson distributed but have a larger variance than mean are often called 'overdispersed'
    -   quite common in RNA-seq and microbiome data

## Poisson Probability Distribution \| gene length by bins of 500 nucleotides {.smaller}

```{r, echo=FALSE, out.width='80%', fig.align='center'}
knitr::include_graphics("images/week_2.004.jpeg")
```

## Poisson Probability Distribution \| increasing parameter values of $\lambda$ {.smaller}

```{r, echo=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics("images/week_2.005.jpeg")
```

## Testing Poisson Distributions {.smaller}

Number of counts (x) given a mean and variance of lambda

```{r, echo=TRUE, out.width="40%", fig.align="center"}
dpois(x=2, lambda=1)
plot(dpois(x=1:10, lambda=3))
```

------------------------------------------------------------------------

# Some other common probability distributions

## **Geometric Distribution** {.smaller}

-   If a single event has two possible outcomes the probability of having to observe `k` trials before the first "one" appears is given by the **geometric distribution**
-   The probability that the first "one" would appear on the first trial is `p`, but the probability that the first "one" appears on the second trial is `(1-p)*p`
-   By generalizing this procedure, the probability that there will be `k-1` failures before the first success is:

$$P(X=k)=(1-p)^{k-1}p$$

-   mean = $\frac{1}{p}$
-   variance = $\frac{(1-p)}{p^2}$

## **Geometric Distribution** {.smaller}

-   Example: If the probability of extinction of an endangered population is estimated to be 0.1 every year, what is the expected time until extinction?
-   The distribution gives the probability of extinction in a given year (requiring that the population did not go extinct in all of the years prior)
-   If we want to know the probability of the population going exticnt by year 4, we simply add up the probabilities for years 1-3 using "or" rules

```{r, echo=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics("images/prob.017.jpeg")
```

## Testing Geometric Distributions {.smaller}

-   dgeom gives the density (probability) of an event (p) after (x) failures

```{r, echo=TRUE, out.width="40%", fig.align="center"}
dgeom(x=20, p=0.1)
plot(dgeom(1:20,0.1))
```

## Testing Geometric Distributions {.smaller}

-   pgeom gives the cumulative probability of event (p) in (q) trials

```{r, echo=TRUE}
pgeom(q=20, p=0.1)
plot(pgeom(q=1:20, p=0.1))
```

------------------------------------------------------------------------

## **Negative Binomial Distribution** {.smaller}

-   Extension of the geometric distribution describing the waiting time until `r` "ones" have appeared.
-   Generalizes the geometric distribution
-   Probability of the $r^{th}$ "one" appearing on the $k^{th}$ trial:

$$P(X=k)=(\frac{k-1}{r-1})p^{r-1}(1-p)^{k-r}p$$

<br>

which simplifies to

$$P(X=k)=(\frac{k-1}{r-1})p^{r}(1-p)^{k-r}$$

-   mean = $\frac{r}{p}$
-   variance = $r(1-p)/p^2$

## **Negative Binomial Distribution** {.smaller}

-   Example: If a predator must capture 10 prey before it can grow large enough to reproduce
-   What would the mean age of onset of reproduction be if the probability of capturing a prey on any given day is 0.1?
-   Notice that the variance is quite high (\~1000) and that the distribution looks quite skewed

```{r, echo=FALSE, out.width='50%', fig.align='center'}
knitr::include_graphics("images/prob.018.jpeg")
```

# Continuous Probability Distributions

## **Continuous probability distributions** {.smaller}

P(observation lies within dx of x) = f(x)dx

$$P(a\leq X \leq b) = \int_{a}^{b} f(x) dx$$

Remember that the indefinite integral sums to one

$$\int_{-\infty}^{\infty} f(x) dx = 1$$

## Continuous probabilities {.smaller}

<br>

`E[X]` may be found by integrating the product of `x` and the probability density function over all possible values of `x`:

$$E[X] = \int_{-\infty}^{\infty} xf(x) dx $$

<br>

$Var(X) = E[X^2] - (E[X])^2$, where the expectation of $X^2$ is

$$E[X^2] = \int_{-\infty}^{\infty} x^2f(x) dx $$

## **Uniform Distribution** {.smaller}

<br>

$$E[X] = \int_{a}^{b} x\frac{1}{b-a} dx = \frac{(a+b)}{2} $$

<br>

```{r, echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/prob.019.jpeg")
```

## **Uniform Distribution** {.smaller}

-   Means that the probability is equal for all possible outcomes
-   Like drawing m&m out of a bag with equal proportions of colors

```{r, echo=TRUE}
dunif(x=1,min=0, max=10)
plot(dunif(1:10, 0, 10))
```

## Uniform Distribution {.smaller}

```{r, echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/prob.019.jpeg")
```

## Exponential Distribution {.smaller}

<br>

$$f(x)=\lambda e^{-\lambda x}$$

<br>

-   `E[X]` can be found be integrating $xf(x)$ from 0 to infinity

<br>

-   leading to the result that

<br>

-   $E[X] = \frac{1}{\lambda}$
-   $E[X^2] = \frac{1}{\lambda^2}$

## Exponential Distribution {.smaller}

-   For example, let equal the instantaneous death rate of an individual.
-   The lifespan of the individual would be described by an exponential distribution (assuming that does not change over time).

```{r, echo=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics("images/prob.020.jpeg")
```

## Exponential Distribution {.smaller}

```{r, echo=TRUE}
dexp(10, rate = 0.1)
plot(dexp(1:100, rate = 0.1))
```

------------------------------------------------------------------------

## **Gamma Distribution** {.smaller}

-   The gamma distribution generalizes the exponential distribution.
-   It describes the waiting time until the $r^{th}$ event for a process that occurs randomly over time at a rate $\lambda$ :

<br>

$$f(x) = \frac{e^{-\lambda x}\lambda x^{r-1}}{(r-1)!}\lambda$$

<br>

$$ Mean =  \frac{r}{\lambda} $$ $$ Variance = \frac{r}{\lambda^2} $$

## Gamma Distribution {.smaller}

-   **Example**: If, in a PCR reaction, DNA polymerase synthesizes new DNA strands at a rate of 1 per millisecond, how long until 1000 new DNA strands are produced?

-   Assume that DNA synthesis does not deplete the pool of primers or nucleotides in the chamber, so that each event is independent of other events in the PCR chamber.

-   The gamma distribution generalizes the exponential distribution.

-   It describes the waiting time until the $r^{th}$ event for a process that occurs randomly over time at a rate $\lambda$

------------------------------------------------------------------------

## Common theme in R for distributions {.smaller}

+-------------+---------------------------+-------------------------+-------------------+------------------------------+
|             | d                         | p                       | q                 | r                            |
|             |                           |                         |                   |                              |
|             | probability mass function | cumulative distribution | quantile function | pseudorandom number generate |
+=============+:=========================:+:=======================:+:=================:+:============================:+
| binomial    | dbinom                    | pbinom                  | qbinom            | rbinom                       |
+-------------+---------------------------+-------------------------+-------------------+------------------------------+
| poisson     | dpois                     | ppois                   | qpois             | rpois                        |
+-------------+---------------------------+-------------------------+-------------------+------------------------------+
| exponential | dexp                      | pexp                    | qexp              | rexp                         |
+-------------+---------------------------+-------------------------+-------------------+------------------------------+
| normal      | dnorm                     | pnorm                   | qnorm             | rnorm                        |
+-------------+---------------------------+-------------------------+-------------------+------------------------------+

