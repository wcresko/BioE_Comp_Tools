---
title: "Week 5b - Statistics for Bioengineering"
author: "Bill Cresko"
format: 
  revealjs:
    transition: fade
    transition-speed: slow
editor: visual
---

```{r}
library(tidyverse)
library(gt)
library(readxl)
library(ggplot2)
```

# Linear Models and Regression

## A linear model to relate two variables

$$ y_i = \beta_0 + \beta_1 x_1 + \epsilon_i $$

```{r, echo=FALSE, out.width='90%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_4b.008.jpeg")
```

## Bivariate normality

```{r, echo=FALSE, out.width='90%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_4b.005.jpeg")
```

## What is a linear model? {.smaller}

All an be written in the form

`response_var = intercept + parameter*(explanatory_var) + error`

\
\

$$ y_i = \beta_0 + \beta_1 x_1 + \epsilon_i $$

$$ y_i = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_n x_n + \epsilon_i $$

$$ y_i = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1x_2 + \epsilon_i $$

-   Functions in
    -   `lm()`
    -   `aov()`
    -   `glm()`

## Many approaches are linear models

-   Linear regression
-   Single factor ANOVA
-   Analysis of covariance
-   Multiple regression
-   Multi-factor ANOVA
-   Repeated-measures ANOVA

## Classes of linear models

-   General Linear Model (**GLM**) - two or more continuous variables

-   General Linear Mixed Model (**GLMM**) - a continuous response variable with a mix of continuous and categorical predictor variables

-   *Generalized Linear Model* - a GLMM that doesn’t assume normality of the response (we’ll get to this later)

## linear model parameters

```{r, echo=FALSE, out.width='90%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_4b.009.jpeg")
```

## What is a non-linear model? {.smaller}

-   ***parametric non-linear model***: assumes that the relationship between the dependent and independent variables can be modeled using a specfic mathematical function

    -   **Logistic regression** $log(\frac{\pi_i}{1-\pi_1})=\beta_0 + \beta_1 x_1 + \beta_2 x_2 +...+ \beta_n x_n +\epsilon$
    -   **Polynomial regression** $y = \beta_0 + \beta_1 x + \beta_2 x^2 +...+ \beta_n x^n +\epsilon$
    -   **Exponential regression** $y = \alpha^{\beta x} + \epsilon$
    -   **Poisson regression** $log(y) = \beta_0 + \beta_1 x + \epsilon$
    -   **Power regression** $y = \alpha x^\beta + \epsilon$

-   ***non-parameteric non-linear regression***: doesn't make this assumption, but instead uses machine learning algorhithms to larn the relationship

    -   Kernel smoothing
    -   Local polynomial regression
    -   Nearest neighbor regression

# Simple Linear Regression

## Simple Linear Regression

$$ y_i = B_0 + B_1 x_1 + e_i $$

-   Relate two continuous variables to one another

-   Often use ordinary least squares (OLS) to fit a model

-   The estimates ($\beta_0$ and $\beta_1$) are measured with normally distributed error

-   Parametric regression involves assumptions about the data

    -   need to do a 'residual analysis' to check assumptions

    -   there are ways to perform robust non-parametric linear regression

## Origin of the term `regression`?

```{r, echo=FALSE, out.width='150%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_4b.002.jpeg")
```

# Assumptions?

-   Independent errors (residuals)
-   Equal variance of residuals in all groups
-   Normally-distributed residuals
-   Robustness to departures from these assumptions is improved when sample size is large and design is balanced

## Residuals

```{r, echo=FALSE, out.width='90%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_4b.016.jpeg")
```

## Residuals

```{r, echo=FALSE, out.width='90%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_4b.015.jpeg")
```

## Hypothesis tests in regression {.smaller}

$$H_0 : \beta_0 = 0$$ $$H_A : \beta_1 \neq 0$$

reduced model ($H_0$) -

$$y_i = \beta_0 + 0x_i + \epsilon_i$$

full model ($H_A$) -

$$y_i = \beta_0 + \beta_1x_i + \epsilon_i$$\
\

1.  fits a “reduced” model without slope term (H0)
2.  fits the “full” model with slope term added back
3.  compares fit of full and reduced models using an ***F test statistic***
4.  compares the residual variance of the full vs. reduced models

## Hypothesis tests in regression

```{r, echo=FALSE, out.width='90%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_4b.011.jpeg")
```

## Estimation with squared errors

-   Sum of Squares ($SS$) is the sum of the squared difference

-   Sum of Squares Total ($SS_{total}$ or $SST$) is

    -   the total variability in the data
    -   variance in Y in simple linear regression

-   Sum of Squares Model ($SS_{model}$ or $SSM$) is the variability explained by the model

-   Sum of Squares Error ($SS_{error}$ or $SSE$) is the unexplained variability once the model is fit

$$SST = SSR + SSE$$

## $R^2$ as a measure of model fit

$$R^2 = 1 - \frac{SSE}{SST}$$ or

$$R^2 = \frac{SSR}{SST}$$

$R^2$ is the proportion of the variance in Y that is explained by X

## Hypothesis tests with mean squared errors

-   Mean square ($MS$) = divide the sum of squares by the degrees of freedom

-   $MS$ standardizes by the number of data points

-   $MST$, $MSM$ and $MSE$ are the mean squares total, model and error respectively

-   What are the degrees of freedom for simple linear regression

    -   numerator degrees of freedom: 1 (#variables - 1)
    -   denominator degrees of freedom: n-2 (#observations - 2)

## Hypothesis tests in linear regression {.smaller}

```{r, echo=FALSE, out.width='90%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_4b.013.jpeg")
```

## Hypothesis tests in linear regression {.smaller}

```{r, echo=FALSE, out.width='90%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_4b.012.jpeg")
```

------------------------------------------------------------------------

# R INTERLUDE {.smaller}

## linear models in R

-   In general we need to fit the model and then 'read' the output

```{r, echo=TRUE, eval=FALSE}
my_lm <- lm(dataset$variable1 ~ dataset$variable2)
summary(my_lm)
```

-   Now write a script to read in the zebrafish data set.
-   Now, add the code to perform a linear model of two continuous variables (length and weight).
-   Notice how the output of the linear model is specified to a new variable.

## 

```{r, echo=TRUE, eval=TRUE}
Zebrafish_data <- read.table("Drerio_development_complete.csv", header=T, sep=",")
head(Zebrafish_data)
```

##  {.smaller}

```{r, echo=TRUE}
x <- Zebrafish_data$Length_cm
y <- Zebrafish_data$Weight_mg


zfish_lm <- lm(y ~ x)
summary (zfish_lm)
```

## Making a plot of the model fit

```{r, echo=TRUE}
plot(y ~ x, col = "blue")
abline(zfish_lm, col = "red")
```

## Same plot in GGPlot2

```{r, echo=TRUE}
ggplot(zfish_lm, aes(x = x, y = y)) +
  geom_point(color = "blue", size = 2, alpha = 0.2) +                  
  geom_smooth(method = "lm", color = "red", se = TRUE) +
  labs(
    title = "Linear regression of zebrafish weight on standard length",
    x = "zebrafish length in cm",
    y = "zebrafish weight in gm"
  ) +
  theme_minimal()

```

## Looking at the model object {.smaller}

#### Names of variables in the model object

```{r, echo=TRUE}
names(zfish_lm)
```

\

#### Pulling out estimates and confidence intervals

```{r, echo=TRUE}
coef(zfish_lm)
confint(zfish_lm)
```

\

#### Formatting the output in a nice table

```{r, echo=TRUE}
library(gtsummary)
library(ggplot2)
tbl_regression(zfish_lm)
```

## Using your fitted model to make predictions

```{r, echo=TRUE}
predict(zfish_lm, data.frame(x = c(0.2, 0.4, 0.8, 1)), interval="confidence")
```

# Residual Analysis

## Did we meet our assumptions?

-   Independent errors (residuals)
-   Equal variance of residuals in all groups
-   Normally-distributed residuals

## Handling violations of the assumptions of linear models

-   What if your residuals aren’t normal because of outliers?

-   Nonparametric methods exist, but these don’t provide parameter estimates with CIs.

-   Robust regression (rlm)

-   Randomization tests

## Anscombe's quartet again \| what would residual plots look like for these?

```{r, echo=FALSE, out.width='90%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_4b.017.jpeg")
```

## Anscombe's quartet again \| what would residual plots look like for these?

```{r, echo=FALSE, out.width='90%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_4b.018.jpeg")
```

## Residual Plots \| Spotting assumption violations

```{r, echo=FALSE, out.width='90%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_4b.020.jpeg")
```

## Residuals \| leverage and influence {.smaller}

```{r, echo=FALSE, out.width='90%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_4b.021.jpeg")
```

-   1 is an outlier for both Y and X
-   2 is not an outlier for either Y or X but has a high residual
-   3 is an outlier in just X - and thus a high residual - and therefore has high influence as measured by Cook's D

## Residuals \| leverage and influence {.smaller}

-   **Leverage** - a measure of how much of an outlier each point is in x-space (on x-axis) and thus only applies to the predictor variable. (Values \> 2\*(2/n) for simple regression are cause for concern)

-   **Residuals** - As the residuals are the differences between the observed and predicted values along a vertical plane, they provide a measure of how much of an outlier each point is in y-space (on y-axis). The patterns of residuals against predicted y values (residual plot) are also useful diagnostic tools for investigating linearity and homogeneity of variance assumptions

-   **Cook’s D** statistic is a measure of the influence of each point on the fitted model (estimated slope) and incorporates both leverage and residuals. Values ≥ 1 (or even approaching 1) correspond to highly influential observations.

## R INTERLUDE \| residual analyses {.smaller}

Let’s look at the residuals in the zebrafish data.

```{r, echo=TRUE, eval=TRUE}
hist(residuals(zfish_lm), breaks=30)
```

## residuals vs. original x variable

```{r, echo=TRUE, eval=TRUE}
plot (residuals(zfish_lm) ~ x)
```

## residuals vs. predicted (fitted) values

```{r, echo=TRUE, eval=TRUE}
plot (residuals(zfish_lm) ~ fitted.values(zfish_lm))
```

## Residual analyses {.smaller}

Or apply the plot() function to the linear model object directly

```{r, echo=TRUE, eval=TRUE}
plot(zfish_lm)
```

Figure out what these plots are telling you

## Or you can use this nifty little package

```{r, echo=TRUE}
library(ggfortify)
autoplot(zfish_lm, which = 1:6, ncol = 2, label.size = 3)
```

## Influence measures

-   How about using the influence.measures function???

```{r, echo=TRUE, eval=FALSE}
influence.measures(zfish_lm)
```

-   Do we have any high leverage observations we need to worry about???

------------------------------------------------------------------------

# The Eugenics History of Statistics

## Galton as the Father of Eugenics

![](images/week6_Galton.jpeg){fig.align="center"}

-   Francis Galton: Darwin's half cousin
-   Studied human variation and genetic inheritance
    -   Human height, fingerprints, intelligence
    -   Correlation, regression toward the mean, and "nature versus nurture"
    -   Pioneered twin studies

## Galton as the Father of Eugenics

-   Believed that intelligence was hereditary based on surveying prominent academics in Europe
-   Used the ideas of **correlation** and **regression towards the mean** to argue that the upper class should breed amongst themselves to keep those "good genes" pure
-   Wanted to provide monetary incentives for "good" couples to marry and reproduce as a way to avoid the upper class being genetically muddied by the lower class

## A common sight at state fairs around the U.S. in the 1930s

-   Competitions for the "perfect family" to encourage public consciousness and support for eugenics

```{r, echo=FALSE, out.width='50%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/IMAGES2.013.jpeg")
```

## Galton, Charles Davenport & G. Stanley Hall

```{r, echo=FALSE, out.width='50%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/IMAGES2.014.jpeg")
```

## Logo of the US eugenics society

```{r, echo=FALSE, out.width='80%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/IMAGES2.015.jpeg")
```

-   American Eugenics Record Office (ERO) founded in Cold Springs Harbor

## Eugenics societies in America

-   Advocated for state laws to ban interracial marriages and promote sterilization of "unfit" individuals (negative eugenics) - especially black, Latinx, and Native American women
-   30 states passed laws to force mental institution patients to be sterilized
-   Between 1907 and 1963, over 64,000 individuals were forcibly sterilized under eugenic legislation in the United States

## RA Fisher and Eugenics in London

-   Developer of Fishers exact test, analysis of variance (ANOVA), null hypothesis, p values, maximum likelihood, probability density functions
-   Founding Chairman of the University of Cambridge Eugenics Society

```{r, echo=FALSE, out.width='20%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/week6_fisher.jpeg")
```

## RA Fisher and Eugenics

-   1/3rd of his work "The Genetical Theory of Natural Selection" discussed eugenics and his theory that the fall of civilizations was due to the fertility of their upper classes being diminished
-   Used these statistical methods to test data on human variation to prove biological differences between human races
-   Eugenics and racism were the primary motivators for many of these statistical tests that we use today

## A direct line to Hitler and Nazism

-   Eugenics existed in America (and England) before it became popular in Germany.
-   By 1933, California had subjected more people to forceful sterilization than all other U.S. states combined.
-   The forced sterilization program engineered by the Nazis was partly inspired by California's.

```{r, echo=FALSE, out.width='70%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/IMAGES2.016.jpeg")
```

## Sterilizations continue in America

-   Our history books paint Nazi Germany as the primary evil of that time, while we try to cover up our significant role in eugenics
-   It wasn't until 1978 that the US passed regulations on sterilization procedures
-   California only passed a bill to outlaw sterilization of inmates in 2014
-   Certain members of the genetic engineering community threaten to bring back eugenics ideas
-   Our current President repeats eugenics talking points

## So, what do we do from here?

-   The statistical methods that Galton, Fisher, and others developed are useful science tools
-   Important to use these tools for good - improving our planet, human health, and technology
-   Important to acknowledge and not forget the history of science - educate others to avoid repeating history

## Interested in learning more?

```{r, echo=FALSE, out.width='20%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/IMAGES2.017.jpeg")
```

-   See also
    -   "The Gene" by Siddhartha Mukherjee
    -   "Control" by Adam Rutherford
