---
title: "Week 2b - Statistics for Bioengineering"
author: "Bill Cresko"
format: 
  revealjs:
    footer: BioE_Stats_2025 - Knight Campus
    transition: fade
    transition-speed: slow
editor: visual
---

```{r}
library(tidyverse)
library(gt)
library(readxl)
```


## Today we will

- Finish Markdown, Latex
- Exploratory data analysis with ggplot2
- Probability and distributions

:::callout-note
Homeworks will be assigned this evening and be due in 2 weeks
:::


# Markdown and LaTeX

## What is markdown? {.smaller}

-   Lightweight *formal* markup languages are used to add formatting to plaintext documents
    -   Adding basic syntax to the text will make elements look different once rendered/knit
    -   Available in many base editors
-   You then need a markdown application with a markdown processor/parser to render your text files into something more exciting
    -   Static and dynamic outputs!
    -   pdf, HTML, presentations, websites, scientific articles, books etc

## Formatting text

```{r, eval=FALSE, echo=TRUE}
*Italic* or _Italic_
**Bold** or __Bold__
```

-   *Italic* or *Italic*
-   **Bold** or **Bold**

## Formatting text {.smaller}

```{r, eval=FALSE, echo=TRUE}
> "You know the greatest danger facing us is ourselves, an irrational fear of the unknown. 
But there’s no such thing as the unknown — only things temporarily hidden, temporarily not understood."
>
> --- Captain James T. Kirk
```

> "You know the greatest danger facing us is ourselves, an irrational fear of the unknown. But there’s no such thing as the unknown — only things temporarily hidden, temporarily not understood."
>
> --- Captain James T. Kirk

## Formatting lists {.smaller}

```{r, eval=FALSE, echo=TRUE}
- list_element
    - sub_list_element #double tab to indent
    - sub_list_element #double tab to indent
    - sub_list_element #double tab to indent
- list_element
    - sub_list_element #double tab to indent
#note the space after each dash- this is important!
```

-   list_element
    -   sub_list_element
    -   sub_list_element
    -   sub_list_element
-   list_element
    -   sub_list_element

## Formatting lists

```{r, eval=FALSE, echo=TRUE}
1. One
2. Two
3. Three
4. Four
```

1.  One
2.  Two
3.  Three
4.  Four

## Inserting images or URLs {.smaller}

```{r, eval=FALSE, echo=TRUE}
[Link](https://commonmark.org/help/)
![Image](https://i1.wp.com/evomics.org/wp-content/uploads/2012/07/20120115-IMG_0297.jpg)
```

[Link](https://commonmark.org/help/) ![Image](https://i1.wp.com/evomics.org/wp-content/uploads/2012/07/20120115-IMG_0297.jpg)

## Including code chunks {.smaller}

```{R, echo=TRUE}

x <- 2
x^2

```

## What is LaTeX? {.smaller}

-   Pronounced «Lah-tech» or «Lay-tech» (to rhyme with «Bertolt Brecht»)
-   A document preparation system for high-quality typesetting
-   It is most often used for medium-to-large technical or scientific documents
-   Can be used for almost any form of publishing.
-   Typesetting journal articles, technical reports, books, and slide presentations
-   Allows for precise mathematical statements
-   https://www.latex-project.org
-   **Importantly, LaTeX can be included right into Markdown documents**

## Operators and Symbols

```{r, eval=FALSE, echo=TRUE}
$$ \large a^x, \sqrt[n]{x}, \vec{\jmath}, \tilde{\imath}$$
```

$$ \large a^x, \sqrt[n]{x}, \vec{\jmath}, \tilde{\imath}$$

```{r, eval=FALSE, echo=TRUE}
$$ \large \alpha, \beta, \gamma$$
```

$$ \large \alpha, \beta, \gamma$$

## Operators and Symbols

```{r, eval=FALSE, echo=TRUE}
$$ \large\approx, \neq, \nsim $$
```

$$ \large\approx, \neq, \nsim $$

```{r, eval=FALSE, echo=TRUE}
$$\large \partial, \mathbb{R}, \flat$$
```

$$\large \partial, \mathbb{R}, \flat$$

## Equations {.smaller}

Binomial sampling equation

```{r, eval=FALSE, echo=TRUE}
$$\large f(k) = {n \choose k} p^{k} (1-p)^{n-k}$$
```

$$\large f(k) = {n \choose k} p^{k} (1-p)^{n-k}$$

Poisson Sampling Equation

```{r, eval=FALSE, echo=TRUE}
$$\large Pr(Y=r) = \frac{e^{-\mu}\mu^r}{r!}$$
```

$$\large Pr(Y=r) = \frac{e^{-\mu}\mu^r}{r!}$$

## Differential Equations {.smaller}

```{r, eval=FALSE, echo=TRUE}
$$\iint xy^2\,dx\,dy =\frac{1}{6}x^2y^3$$
```

$$\iint xy^2\,dx\,dy =\frac{1}{6}x^2y^3$$

## Matrix formulations {.smaller}

```{r, eval=FALSE, echo=TRUE}
$$	\begin{matrix}
		-2 & 1 & 0 & 0 & \cdots & 0  \\
		1 & -2 & 1 & 0 & \cdots & 0  \\
		0 & 1 & -2 & 1 & \cdots & 0  \\
		0 & 0 & 1 & -2 & \ddots & \vdots \\
		\vdots & \vdots & \vdots & \ddots & \ddots & 1  \\
		0 & 0 & 0 & \cdots & 1 & -2
	\end{matrix} $$
```

$$  \begin{matrix}
        -2 & 1 & 0 & 0 & \cdots & 0  \\
        1 & -2 & 1 & 0 & \cdots & 0  \\
        0 & 1 & -2 & 1 & \cdots & 0  \\
        0 & 0 & 1 & -2 & \ddots & \vdots \\
        \vdots & \vdots & \vdots & \ddots & \ddots & 1  \\
        0 & 0 & 0 & \cdots & 1 & -2
    \end{matrix} $$

## In-line versus fenced {.smaller}

```{r, eval=FALSE, echo=TRUE}
This equation, $y=\frac{1}{2}$, is included inline
```

This equation, $y=\frac{1}{2}$, is included inline

```{r, eval=FALSE, echo=TRUE}
Whereas this equation, $$y=\frac{1}{2}$$, is put on a separate line
```

Whereas this equation $$y=\frac{1}{2}$$ is put on a separate line

## Markdown is very flexible {.smaller}

-   You can import RMarkdown templates into RStudio and open as a new Rmarkdown file
-   Better yet there are packages that add functionality
    -   books
    -   journal articles
    -   slide shows (these slides!)
    -   interactive exercises

------------------------------------------------------------------------

# Exploratory Data Analysis with ggplot2

## Reading in and Exporting Data Frames {.flexbox .vcenter}

\
\

```{r, eval = FALSE, echo = TRUE}
YourFile <- read.table('yourfile.csv', header=T, row.names=1, sep=',')
YourFile <- read.table('yourfile.txt', header=T, row.names=1, sep='\t')
```

\
\

```{r, eval = FALSE, echo = TRUE}
write.table(YourFile, "yourfile.csv", quote=F, row.names=T, sep=",")
write.table(YourFile, "yourfile.txt", quote=F, row.names=T, sep="\t")
```

## Indexing in data frames {.flexbox .vcenter}

-   Next up - indexing just a subset of the data
-   This is a very important idea in R, that you can analyze just a subset of the data.
-   This is analyzing only the data in the file you made that has the factor value 'mixed'.

```{r, echo=TRUE, eval=FALSE}
print (YourFile[,2])
print (YourFile$variable)
print (YourFile[2,])
plot (YourFile$variable1, YourFile$variable2)
```


## Types of vectors of data {.smaller}

-   `int` stands for integers

-   `dbl` stands for doubles, or real numbers

-   `chr` stands for character vectors, or strings

-   `dttm` stands for date-times (a date + a time)

-   `lgl` stands for logical, vectors that contain only TRUE or FALSE

-   `fctr` stands for factors, which R uses to represent categorical variables with fixed possible values

-   `date` stands for dates

::: callout-note
-   Integer and double vectors are known collectively as numeric vectors.
-   In `R` numbers are doubles by default.
:::

## Types of vectors of data {.smaller}

-   Logical vectors can take only three possible values:
    -   `FALSE`
    -   `TRUE`
    -   `NA` which is 'not available'.
-   Integers have one special value: NA, while doubles have four:
    -   `NA`
    -   `NaN` which is 'not a number'
    -   `Inf`
    -   `-Inf`
    
# ggplot2

## Plotting using `ggplot2()` {.smaller}

-   Part of the `tidyverse` suite of packages
-   In most cases, you start with `ggplot2()`
-   Supply a dataset and aesthetic mapping with `aes()`
-   Determine the type of plot using `geom_point()` or `geom_histogram()` or others
-   Many more options and controls available!
-   More info: https://ggplot2.tidyverse.org/

## GGPlot2 and the Grammar of Graphics

-   GG stands for ‘Grammar of Graphics’
-   A good paragraph uses good grammar to convey information
-   A good figure uses good grammar in the same way
-   Seven general components can be used to create most figures

## GGPlot2 and the Grammar of Graphics

```{r, echo=FALSE, fig.cap="", out.width='90%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/images_4a.017.jpeg")
```

## Plotting using `ggplot()` {.smaller}

-   Install and load `ggplot2`

```{r, echo=TRUE}
# install.packages("ggplot2")
library("ggplot2")
```

## Scatterplots with `ggplot`

-   Use the preloaded `mpg` dataset available in RStudio

```{r, echo=TRUE, out.width='55%', fig.asp=.75, fig.align='center'}
ggplot(mpg, aes(displ, hwy, color = class)) + 
  geom_point()
```

## Boxplots in `ggplot`

```{r, echo=TRUE, out.width='100%', fig.asp=.3, fig.align='center'}
ggplot(mpg, aes(manufacturer, hwy, colour = class)) + 
  geom_boxplot() + 
  theme_classic() + 
  theme(axis.text.x = element_text(angle = 45, hjust=1))
```


## The `geom_bar` function {.flexbox .vcenter}

```{r, eval=FALSE, echo=TRUE}
ggplot(data=diamonds) +
  geom_bar(mapping=aes(x=cut))
```

Now try this...

```{r, eval=FALSE, echo=TRUE}
ggplot(data=diamonds) +
  geom_bar(mapping=aes(x=cut, color=cut))
```

and this...

```{r, eval=FALSE, echo=TRUE}
ggplot(data=diamonds) +
  geom_bar(mapping=aes(x=cut, fill=cut))
```

and finally this...

```{r, eval=FALSE, echo=TRUE}
ggplot(data=diamonds) +
  geom_bar(mapping=aes(x=cut, fill=clarity), position="dodge")
```

## The `geom_histogram` and `geom_freqpoly`function {.flexbox .vcenter}

With this function you can make a histogram

```{r, eval=FALSE, echo=TRUE}
ggplot(data=diamonds) +
  geom_histogram(mapping=aes(x=carat), binwidth=0.5)
```

This allows you to make a frequency polygram

```{r, eval=FALSE, echo=TRUE}
ggplot(data=diamonds) +
  geom_histogram(mapping=aes(x=carat), binwidth=0.5)
```

## The `geom_boxplot` function {.flexbox .vcenter}

Boxplots are very useful for visualizing data

```{r, eval=FALSE, echo=TRUE}
ggplot(data=diamonds, mapping=aes(x=cut, y=price)) +
  geom_boxplot()
```

\

```{r, eval=FALSE, echo=TRUE}
ggplot(data=mpg, mapping=aes(x=reorder(class, hwy, FUN=median), y=hwy)) +
  coordflip()
```

\

```{r, eval=FALSE, echo=TRUE}
ggplot(data=mpg, mapping=aes(x=class, y=hwy)) +
  geom_boxplot() +
  coordflip
```

## The `geom_point` & `geom_smooth` functions {.flexbox .vcenter}

```{r, eval=FALSE, echo=TRUE}
ggplot(data=diamonds2, mapping=aes(x=x, y=y)) +
  geompoint()
```

\

```{r, eval=FALSE, echo=TRUE}
ggplot(data=mpg) +
  geompoint(mapping=aes(x=displ, y=hwy)) +
  facet_wrap(~class, nrow=2)
```

\

```{r, eval=FALSE, echo=TRUE}
ggplot(data=mpg) +
  geompoint(mapping=aes(x=displ, y=hwy)) +
  facet_grid(drv~cyl)
```

\

```{r, eval=FALSE, echo=TRUE}
ggplot(data=mpg) +
  geomsmooth(mapping=aes(x=displ, y=hwy))
```

## Combining geoms {.flexbox .vcenter}

```{r, eval=TRUE, echo=TRUE}
ggplot(data=mpg) +
  geom_point(mapping=aes(x=displ, y=hwy)) +
  geom_smooth(mapping=aes(x=displ, y=hwy))
```

## Adding labels {.flexbox .vcenter}

```{r, eval=TRUE, echo=TRUE}
ggplot(data=mpg, aes(displ, hwy)) +
  geom_point(aes(color=class)) +
  geom_smooth(se=FALSE) +
  labs(
    title = "Fuel efficiency generally decreases with engine size",
    caption = "Data from fueleconomy.gov"
  )
```

## What type of plot do I use for each data type?


------------------------------------------------------------------------



________________________________________________________________________________



# Randomness in Statistics {.smaller}

-   We often want to know truths about the world, but the best we can do is estimate them - and uncertainty in those estimates is a given.
-   The process of statistics is largely about quantifying and managing uncertainty.
-   Frequently, we want to understand how likely a particular observation or set of observations is (e.g. from a sample of a population), given some expectation.
-   Expectations can be based on theoretical probability distributions

# Probability and distributions

## Different flavors of inferential statistics {.smaller}

-   **Frequentist Statistics**
    -   Classical or standard approaches
    -   Null hypothesis testing
\
\
-   **Hierarchical Probabilistic Modeling**
    -   Maximum Likelihood
    -   Bayesian Analyses
    -   Machine Learning

## What is probability {.smaller}

-   **Frequency interpretation**

"Probabilities are understood as mathematically convenient approximations to long run relative frequencies."

-   **Subjective (Bayesian) interpretation**

"A probability statement expresses the opinion of some individual regarding how certain an event is to occur."

## Random variables & probability {.smaller}

-   **Probability** is the expression of belief in some future outcome

-   A **random variable** can take on different values with different probabilities

-   The **sample space** of a random variable is the universe of all possible values

## Random variables & probability {.smaller}

-   The **sample space** can be represented by a
    -   **probability distribution** (discrete)
    -   **probability density function (PDF)** (continuous)
    -   algebra and calculus are used for each respectively
    -   probabilities of a sample space **always sum to 1.0**
-   How does it make sense that a sample space will always sum to 1?

## Bernoulli distribution {.smaller}

-   Describes the expected outcome of a single event with probability `p`

-   Example of flipping of a **fair** coin once

$$Pr(X=\text{Head}) = \frac{1}{2} = 0.5 = p $$

$$Pr(X=\text{Tails}) = \frac{1}{2} = 0.5 = 1 - p = q $$

## Bernoulli distribution {.smaller}

-   If the coin isn't fair then $p \neq 0.5$
-   However, the probabilities still sum to 1

$$ p + (1-p) = 1 $$ $$ p + q = 1 $$

-   Same is true for other binary possibilities
    -   success or failure
    -   yes or no answers
    -   choosing an allele from a population based upon allele frequencies (Hardy-Weinberg ring any bells??)

## Probability rules {.smaller}

-   Flip a coin twice
-   Represent the first flip as ‘X’ and the second flip as ‘Y’

$$ Pr(\text{X=H and Y=H}) = p*p = p^2 $$ $$ Pr(\text{X=H and Y=T}) = p*q = pq = p^2 $$ $$ Pr(\text{X=T and Y=H}) = q*p = pq $$ $$ Pr(\text{X=T and Y=T}) = q*q = q^2 $$

## Probability rules {.smaller}

-   Probability that the `H` and `T` can occur in any order

$$ \text{Pr(X=H) or Pr(X=T)} = p+q=1$$

$$ \text{Pr(X=H and Y=T) or Pr(X=T and Y=H)} = $$ $$ (p*q) + (p*q) = 2pq $$

-   These are the **'and'** and **'or'** rules of probability
    -   'and' means multiply the probabilities
    -   'or' means sum the probabilities
    -   most probability distributions can be built up from these simple rules

## Let's simulate some coin flips {.smaller}

```{r, echo=TRUE}
# tossing a fair coin
coin <- c("heads", "tails")

sample(coin)

```

## Let's simulate some coin flips {.smaller}

-   What happens when we change the probabilities or the sample size? How confident are we that our coin is fair?

```{r, out.width="50%"}
flips <- sample(coin, prob = c(0.5, 0.5), size=10, replace=TRUE)
barplot(table(flips))
```

## Expectation and Moments of Distributions {.smaller}

-   Distributions have **moments** that can be estimated
-   1st, 2nd, 3rd and 4th moments of a distribution?
-   The expectation or mean of a random variable X is:

$$E[X] = \sum_{\text{all x}}^{}xP(X=x) = \mu$$

-   Often we want to know how dispersed the random variable is around its mean.
-   One measure of dispersion is the variance

$$Var(X) = E[X^2] = \sigma^2$$

## Joint probability {.smaller}

$$Pr(X,Y) = Pr(X) * Pr(Y)$$

-   Note that this is true for two **independent** events
-   However, for two non-independent events we also have to take into account their **covariance**
-   To do this we need **conditional probabilities**

## Conditional probability {.smaller}

-   For two **independent** variables: Probability of Y, given X, or the probability of X, given Y.

$$Pr(Y|X) = Pr(Y)\text{ and }Pr(X|Y) = Pr(X)$$

-   For two **non-independent** variables

$$Pr(Y|X) \neq Pr(Y)\text{ and }Pr(X|Y) \neq Pr(X)$$

-   Variables that are non-independent have a shared variance, which is also known as the **covariance**
-   Covariance standardized to a mean of zero and a unit standard deviation is **correlation**

## What is Likelihood vs. Probability? {.smaller}

-   The **probability** of an event is the proportion of times that the event would occur if we repeated a random trial over and over again under the same conditions.

-   The **likelihood** is a conditional probability of a parameter value given a set of data

-   The likelihood of a population parameter **equaling a specific value, given the data**

`L[parameter|data] = Pr[data|parameter]`

-   **Likelihood function** which can have a maximum

## Extra Bayesian material {.smaller}

```{r, echo=FALSE, eval=FALSE}
## A Bayesian look at estimation

- A way of quantifying uncertainty of the population parameter, **not our estimate of it**, using prior information about the probability distribution of the parameter.



$$ P(\theta|d) = \frac{P(d|\theta)P(\theta)}{P(d)}$$

where


- $P(\theta|d)$ = posterior probability distribution 
- $P(d|\theta)$ = likelihood function for $\theta$ 
- $P(\theta)$ = prior probability distribution 
- $P(d)$ = mean of the likelihood function 
```

## Who wants to win a car? {.smaller}

-   Pretend that there are three doors, and behind one is a car. Behind the other doors are cats.

-   You choose one of the doors, and then Monte Hall opens one of the two remaining doors to reveal a cat.

-   At this point you have the choice of changing doors or staying with your original choice.

-   **What should you do?**

```{r, echo=FALSE, out.width='50%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/cats/two.jpg")
```

# Common probability distributions

## **Geometric Distribution** {.smaller}

-   If a single event has two possible outcomes the probability of having to observe `k` trials before the first "one" appears is given by the **geometric distribution**
-   The probability that the first "one" would appear on the first trial is `p`, but the probability that the first "one" appears on the second trial is `(1-p)*p`
-   By generalizing this procedure, the probability that there will be `k-1` failures before the first success is:

$$P(X=k)=(1-p)^{k-1}p$$

-   mean = $\frac{1}{p}$
-   variance = $\frac{(1-p)}{p^2}$

## **Geometric Distribution** {.smaller}

-   Example: If the probability of extinction of an endangered population is estimated to be 0.1 every year, what is the expected time until extinction?

```{r, echo=FALSE, out.width='80%', fig.align='center'}
knitr::include_graphics("images/prob.017.jpeg")
```

## **Geometric Distribution** {.smaller}

-   The distribution gives the probability of extinction in a given year (requiring that the population did not go extinct in all of the years prior)
-   If we want to know the probability of the population going exticnt by year 4, we simply add up the probabilities for years 1-3 using "or" rules

```{r, echo=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics("images/prob.017.jpeg")
```

## Testing Geometric Distributions {.smaller}

-   dgeom gives the density (probability) of an event (p) after (x) failures

```{r, echo=TRUE, out.width="40%", fig.align="center"}
#dgeom(x=20, p=0.1)
# 0.01215767
plot(dgeom(1:20,0.1))
```

## Testing Geometric Distributions {.smaller}

-   pgeom gives the cumulative probability of event (p) in (q) trials

```{r}
pgeom(q=20, p=0.1)
```

## **Binomial Distribution** {.smaller}

-   A **binomial distribution** results from the **combination** of several independent Bernoulli events

-   **Example**

    -   Pretend that you flip 20 fair coins
        -   or collect alleles from a heterozygote
    -   Now repeat that process and record the number of heads
    -   We expect that most of the time we will get approximately 10 heads
    -   Sometimes we get many fewer heads or many more heads

## Binomial Distribution {.smaller}

-   The distribution of probabilities for each combination of outcomes is

$$\large f(k) = {n \choose k} p^{k} (1-p)^{n-k}$$

-   `n` is the total number of trials
-   `k` is the number of successes
-   `p` is the probability of success
-   `q` is the probability of not success
-   For binomial as with the Bernoulli `p = 1-q`

## Binomial Probability Distribution {.smaller}

```{r, echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/week_2.003.jpeg")
```

## Testing Binomial Distributions {.smaller}

-   dbinom gives the density (probability) of number successes (x) in number trials (size), with (prob) probability between 0-1

```{r, echo=TRUE, out.width="40%", fig.align="center"}
# dbinom(x=5, size=10, p=0.5)
# 0.246
plot(dbinom(x=1:10, size=10, p=0.5))
```

## Testing Binomial Distributions {.smaller}

-   pbinom gives the cumulative probability of reaching at least (q) number of successes after (size) number of trials

```{r}
plot(pbinom(q=1:100, size=100, p=0.5))
```

## **Negative Binomial Distribution** {.smaller}

-   Extension of the geometric distribution describing the waiting time until `r` "successes" have happened.
-   Generalizes the geometric distribution
-   Probability of the $r^{th}$ "success" appearing on the $k^{th}$ trial
-   mean = $\frac{r}{p}$
-   variance = $r(1-p)/p^2$

## **Negative Binomial Distribution** {.smaller}

-   Example: If a predator must capture 10 prey before it can grow large enough to reproduce
-   What would the mean age of onset of reproduction be if the probability of capturing a prey on any given day is 0.1?
-   Notice that the variance is quite high (\~1000) and that the distribution looks quite skewed

```{r, echo=FALSE, out.width='50%', fig.align='center'}
knitr::include_graphics("images/prob.018.jpeg")
```

## **Poisson Probability Distribution** {.smaller}

-   Another common situation in biology is when each trial is discrete, but the number of observations of each outcome is observed/counted

-   Some examples are

    -   counts of snails in several plots of land
    -   observations of the firing of a neuron in a unit of time
    -   count of genes in a genome binned to units of 500 AA

-   Just like before you have 'successes', but

    -   now you count them for each replicate
    -   the replicates now are units of area or time
    -   the values can now range from 0 to a large number

## Poisson Probability Distribution {.smaller}

-   For example, you can examine 1000 genes
    -   count the number of base pairs in the coding region of each gene
    -   what is the probability of observing a gene with 'r' bp?
-   `Pr(Y=r)` is the probability that the number of occurrences of an event `y` equals a count `r` in the total number of trials

## Poisson Probability Distribution {.smaller}

-   Note that this is a single parameter function because $\mu = \sigma^2$
-   The two together are often just represented by $\lambda$

$$Pr(y=r) = \frac{e^{-\lambda}\lambda^r}{r!}$$

-   This means that for a variable that is truly Poisson distributed:
    -   the mean and variance should be equal to one another
    -   variables that are approximately Poisson distributed but have a larger variance than mean are often called 'overdispersed'
    -   quite common in RNA-seq and microbiome data

## Poisson Probability Distribution \| gene length by bins of 500 nucleotides {.smaller}

```{r, echo=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics("images/week_2.004.jpeg")
```

## Poisson Probability Distribution \| increasing parameter values of $\lambda$ {.smaller}

```{r, echo=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics("images/week_2.005.jpeg")
```

## Testing Poisson Distributions {.smaller}

Number of counts (x) given a mean and variance of lambda

```{r, out.width="40%", fig.align="center"}
dpois(x=2, lambda=1)
plot(dpois(x=1:10, lambda=3))
```

## **Uniform Distribution** {.smaller}

-   Means that the probability is equal for all possible outcomes
-   Like drawing m&m out of a bag with equal proportions of colors

```{r}
dunif(x=1,min=0, max=10)
```

## Uniform Distribution {.smaller}

```{r}
plot(dunif(1:10, 0, 10))
```

```{r, echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/prob.019.jpeg")
```

## Exponential Distribution {.smaller}

-   For example, let equal the instantaneous death rate of an individual.
-   The lifespan of the individual would be described by an exponential distribution (assuming that does not change over time).

```{r, echo=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics("images/prob.020.jpeg")
```

## Exponential Distribution {.smaller}

```{r}
plot(dexp(1:100, rate = 0.1))
```

## **Gamma Distribution** {.smaller}

-   The gamma distribution generalizes the exponential distribution.
-   It describes the waiting time until the $r^{th}$ event for a process that occurs randomly over time at a rate $\lambda$

## Gamma Distribution {.smaller}

-   **Example**: If, in a PCR reaction, DNA polymerase synthesizes new DNA strands at a rate of 1 per millisecond, how long until 1000 new DNA strands are produced?

-   Assume that DNA synthesis does not deplete the pool of primers or nucleotides in the chamber, so that each event is independent of other events in the PCR chamber.

## The Gaussian or Normal Distribution {.smaller}

## Log-normal PDF \| Continuous version of Poisson (-ish) {.smaller}

```{r, echo=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics("images/week_2.006.jpeg")
```

## Transformations to ‘normalize’ data {.smaller .vcenter .flexbox}

```{r, echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/week_2.007.jpeg")
```

## Transformations to ‘normalize’ data {.smaller .vcenter .flexbox}

```{r, echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/week_2.008.jpeg")
```

## Binomial to Normal \| Categorical to continuous {.smaller .vcenter .flexbox}

```{r, echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/week_2.009.jpeg")
```

## The Normal (aka Gaussian) \| Probability Density Function (PDF) {.smaller}

```{r, echo=FALSE, out.width='90%', fig.align='center'}
knitr::include_graphics("images/week_2.011.jpeg")
```

## Normal PDF {.smaller}

```{r, echo=FALSE, out.width='50%', fig.align='center'}
knitr::include_graphics("images/week_2.012.jpeg")
```

## Normal PDF \| A function of two parameters

### ($\mu$ and $\sigma$) {.smaller}

```{r, echo=FALSE, out.width='40%', fig.align='center'}
knitr::include_graphics("images/week_2.032.jpeg")
```

where $$\large \pi \approx 3.14159$$

$$\large \epsilon \approx 2.71828$$

To write that a variable (v) is distributed as a normal distribution with mean $\mu$ and variance $\sigma^2$, we write the following:

$$\large v \sim \mathcal{N} (\mu,\sigma^2)$$

## Normal PDF \| estimates of mean and variance {.smaller}

Estimate of the mean from a single sample

$$\Large \bar{x} = \frac{1}{n}\sum_{i=1}^{n}{x_i} $$

Estimate of the variance from a single sample

$$\Large s^2 = \frac{1}{n-1}\sum_{i=1}^{n}{(x_i - \bar{x})^2} $$

## Normal PDF {.smaller}

```{r, echo=FALSE, out.width='90%', fig.align='center'}
knitr::include_graphics("images/week_2.010.jpeg")
```

## Why is the Normal special in biology? {.smaller}

```{r, echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/week_2.013.jpeg")
```

## Why is the Normal special in biology? {.smaller}

```{r, echo=FALSE, out.width='100%', fig.align='center'}
knitr::include_graphics("images/week_2.015.jpeg")
```

## Why is the Normal special in biology? {.smaller}

```{r, echo=FALSE, out.width='60%', fig.align='center'}
knitr::include_graphics("images/week_2.014.jpeg")
```

## Parent-offspring resemblance {.smaller}

```{r, echo=FALSE, out.width='45%', fig.align='center'}
knitr::include_graphics("images/week_2.016.jpeg")
```

## Genetic model of complex traits {.smaller}

```{r, echo=FALSE, out.width='90%', fig.align='center'}
knitr::include_graphics("images/week_2.017.jpeg")
```

## Distribution of $F_2$ genotypes \| really just binomial sampling {.smaller}

```{r, echo=FALSE, out.width='70%', fig.align='center'}
knitr::include_graphics("images/week_2.018.jpeg")
```

## Why else is the Normal special? {.smaller}

-   The normal distribution is immensely useful because of the central limit theorem, which says that the mean of many random variables independently drawn from the same distribution is distributed approximately normally
-   One can think of numerous situations, such as
    -   when multiple genes contribute to a phenotype
    -   or that many factors contribute to a biological process
-   In addition, whenever there is variance introduced by stochastic factors the central limit theorem holds
-   Thus, normal distributions occur throughout genomics
-   It's also the basis of the majority of classical statistics

## z-scores of normal variables {.smaller}

-   Often we want to make variables more comparable to one another
-   For example, consider measuring the leg length of mice and of elephants
    -   Which animal has longer legs in absolute terms?
    -   Proportional to their body size?
-   A good way to answer these last questions is to use 'z-scores'

## z-scores of normal variables {.smaller}

-   z-scores are standardized to a mean of 0 and a standard deviation of 1
-   We can modify any normal distribution to have a mean of 0 and a standard deviation of 1
-   Another term for this is the standard normal distribution

$$\huge z_i = \frac{(x_i - \bar{x})}{s}$$

------------------------------------------------------------------------

# Data Wrangling?

## Resources:

-   [Vignette](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) (from the **tidyr** package)
-   [Original paper](https://vita.had.co.nz/papers/tidy-data.pdf) (Hadley Wickham, 2014 JSS)

## R packages you'll need today

[**tidyverse**](https://www.tidyverse.org/)

[**nycflights13**](hhttps://github.com/hadley/nycflights13)

## Tidyverse vs. base R (cont.)

One point of convenience is that there is often a direct correspondence between a tidyverse command and its base R equivalent.

These generally follow a `tidyverse::snake_case` vs `base::period.case` rule. E.g. Compare:

| tidyverse          | base                |
|--------------------|---------------------|
| `?readr::read_csv` | `?utils::read.csv`  |
| `?dplyr::if_else`  | `?base::ifelse`     |
| `?tibble::tibble`  | `?base::data.frame` |

Etcetera.

If you call up the above examples, you'll see that the tidyverse alternative typically offers some enhancements or other useful options (and sometimes restrictions) over its base counterpart.

--

**Remember:** There are (almost) always multiple ways to achieve a single goal in R.

------------------------------------------------------------------------

## Tidyverse packages

Let's load the tidyverse meta-package and check the output.

```{r tverse, cache = FALSE}
library(tidyverse)
```

--

We see that we have actually loaded a number of packages (which could also be loaded individually): **ggplot2**, **tibble**, **dplyr**, etc. - We can also see information about the package versions and some [namespace conflicts](https://raw.githack.com/uo-ec607/lectures/master/04-rlang/04-rlang.html#59).

------------------------------------------------------------------------

## Tidyverse packages (cont.)

The tidyverse actually comes with a lot more packages than those that are just loaded automatically.<sup>1</sup>

```{r tverse_pkgs}
tidyverse_packages()
```

We'll use several of these additional packages during the remainder of this course.

-   E.g. The **lubridate** package for working with dates and the **rvest** package for webscraping.
-   However, bear in mind that these packages will have to be loaded separately.

.footnote\[ <sup>1</sup> It also includes a *lot* of dependencies upon installation. This is a matter of some [controversy](http://www.tinyverse.org/).\]

------------------------------------------------------------------------

## Tidyverse packages (cont.)

I hope to cover most of the tidyverse packages over the length of this course.

Today, however, I'm only really going to focus on two packages: 1. [**dplyr**](https://dplyr.tidyverse.org/) 2. [**tidyr**](https://tidyr.tidyverse.org/)

These are the workhorse packages for cleaning and wrangling data. They are thus the ones that you will likely make the most use of (alongside **ggplot2**, which we already met back in Lecture 1). - Data cleaning and wrangling occupies an inordinate amount of time, no matter where you are in your research career.

## An aside on pipes: %\>%

The tidyverse loads its own pipe operator, denoted `%>%`.

```{r, eval = F}
## These next two lines of code do exactly the same thing.
mpg %>% filter(manufacturer=="audi") %>% group_by(model) %>% summarise(hwy_mean = mean(hwy))
summarise(group_by(filter(mpg, manufacturer=="audi"), model), hwy_mean = mean(hwy))
```

--

The first line reads from left to right, exactly how I thought of the operations in my head. - Take this object (`mpg`), do this (`filter`), then do this (`group_by`), etc.

The second line totally inverts this logical order (the final operation comes first!) - Who wants to read things inside out?

## An aside on pipes: %\>% (cont.)

The piped version of the code is even more readable if we write it over several lines. Here it is again and, this time, I'll run it for good measure so you can see the output:

```{r pipe}
mpg %>% 
  filter(manufacturer=="audi") %>% 
  group_by(model) %>% 
  summarise(hwy_mean = mean(hwy))
```

Remember: Using vertical space costs nothing and makes for much more readable/writeable code than cramming things horizontally.

## dplyr

<html>

::: {style="float:left"}
:::

<hr color='#EB811B' size=1px width=796px>

</html>

## Key dplyr verbs

There are five key dplyr verbs that you need to learn.

1.  `filter`: Filter (i.e. subset) rows based on their values.

2.  `arrange`: Arrange (i.e. reorder) rows based on their values.

3.  `select`: Select (i.e. subset) columns by their names:

4.  `mutate`: Create new columns.

5.  `summarise`: Collapse multiple rows into a single summary value.<sup>1</sup>

## Other dplyr goodies

`group_by` and `ungroup`: For (un)grouping. - Particularly useful with the `summarise` and `mutate` commands, as we've already seen.

\

`slice`: Subset rows by position rather than filtering by values. - E.g. `starwars %>% slice(c(1, 5))`

\

`pull`: Extract a column from as a data frame as a vector or scalar. - E.g. `starwars %>% filter(gender=="female") %>% pull(height)`

\

`count` and `distinct`: Number and isolate unique observations. - E.g. `starwars %>% count(species)`, or `starwars %>% distinct(species)` - You could also use a combination of `mutate`, `group_by`, and `n()`, e.g. `starwars %>% group_by(species) %>% mutate(num = n())`.

------------------------------------------------------------------------

## Data cleaning and manipulation

-   Here, we differentiate "data cleaning" from "data manipulation", which is perhaps an arbitrary distinction.

-   "Data cleaning" typically refers to altering variable class information, fixing mistakes that could have arisen in the data (e.g., an extra '.' symbol in a numeric value), and things of this nature.

-   "Data manipulation", in my mind, refers to altering the structure of the data in a way that changes the functional structure the data (e.g., an addition of a column, deletion of rows, long/wide formatting change).

## gapminder data

The gapminder data are commonly used to explore concepts of data exploration and manipulation, maybe because of the combination of character and numeric variables, nested structure in terms of country and year, or maybe it is just out of ease in copying notes from other people.

```{r}

dat <- read.delim(file = "http://www.stat.ubc.ca/~jenny/notOcto/STAT545A/examples/gapminder/data/gapminderDataFiveYear.txt")

```

```{r}

head(dat)
str(dat)

```

## gapminder data

We can use what we learned before in terms of base `R` functions to calculate summary statistics.

```{r}

# mean life expectancy
mean(dat$lifeExp)

```

But what does mean life expectancy really tell us, when we also have information on space (`country`) and time (`year`)? So we may wish to subset the data to a specific country or time period. We can do this using `which` statements.

```{r, eval=FALSE, echo=TRUE}

dat[which(dat$country == 'Afghanistan'), ]
dat[which(dat$year < 1960), ]

```

Recall that `which` evaluates a condition, and then determines the index of each TRUE value. So for the first example, the `which` tells us the indices where the vector `dat$country` is equal to "Afghanistan". Putting this result vector of indices within the square brackets allows us to subset the data.frame based on these indices (specifically, we are subsetting specific rows of data).

In the second example, we want to see all data that was recorded prior to 1960. As you will quickly realize, there are always multiple ways to do the same thing when programming. For instance, this second statement could be done in base `R` using the `subset` function.

```{r, eval=FALSE, echo=TRUE}

subset(dat, dat$year < 1960)

```

The `subset` function also allows you to 'select' specific columns in the output.

```{r, eval=FALSE, echo=TRUE}

subset(dat, dat$year < 1955, select=c(lifeExp,gdpPercap))

```

However, this is the same as

```{r, eval=FALSE, echo=TRUE}

dat[which(dat$year < 1960), c("lifeExp","gdpPercap")]

```

## gapminder data

To refresh your memory and clarify the use of conditionals, the list below provides a bit more information.

-   ==: equals exactly

-   \<, \<=: is smaller than, is smaller than or equal to

-   `>`, \>=: is bigger than, is bigger than or equal to

-   !=: not equal to

## gapminder data

And some that we did not go into before, but will go into a bit more detail on now:

-   !: NOT operator, to specify things that should be omitted

-   &: AND operator, allows you to chain two conditions which must both be met

-   \|: OR operator, to chains two conditions when at least one should be met

-   %in%: belongs to one of the following (usually followed by a vector of possible values)

The AND (&) and the OR (\|) operators are also super useful when you want to separate data based on multiple conditions.

## gapminder data

```{r, eval=FALSE, echo=TRUE}

dat[which(dat$country=='Afghanistan' & dat$year==1977),]
dat[which(dat$lifeExp < 40 | dat$gdpPercap < 500), ]

```

Finally, the %in% operator is super useful when you want to subset data based on multiple conditions

```{r}

#fails
dat[which(dat$country == c('Afghanistan', 'Turkey')), ]

#does not fail
dat[which(dat$country %in% c('Afghanistan', 'Turkey')), ]

```

## gapminder data

Related to %in%, is `match`. `match` is best for identifying the index of single types in a vector of unique values. For instance,

```{r}

dat[match(c('Afghanistan', 'Turkey'), dat$country),]

```

## gapminder data

only returns two rows, because it only matches the first instance of both countries in the data. We can use match to get the index associated with a single value (useful when writing functions).

```{r}

match('dog', c('dog', 'cat', 'snake'))

#not ideal behavior
match('dog', c('dog', 'cat', 'snake', 'dog'))

```

or it can be used to identify multiple instances of a single value across a vector of values.

```{r}
match(c('dog', 'cat', 'snake', 'dog'), 'dog')
match(c('dog', 'cat', 'snake', 'dog'), c('dog', 'cat'))

```

## gapminder data

While a bit opaque, these functions are pretty useful in a variety of situations. Speaking of data manipulation functions that are useful but a bit conceptually difficult, `do.call` and `Reduce` are solid base `R` functions.

`do.call` is a way of calling the same function recursively on multiple objects, and may have similar output to `Reduce`, which is also a way to recursively apply a function.

```{r}

lst <- list(1:10, 1:10, 1:10, 1:10, 1:10)
lst

#this makes a single rbind call with each element of the list as an argument
str(do.call(rbind, lst))

#this does it iteratively (so makes n-1 rbind calls)
Reduce(rbind, lst)

```

## The tidyverse

-   There are many `R` libraries designed to manipulate data and work with specific data structures (e.g., `purrr` for list objects, `lubridate` for dates, etc.).

-   For the sake of brevity and generality, we will examine one main useful packages for data manipulation: `dplyr`.

# dplyr-specific functions

## rename

```{r}

df <- data.frame(A=runif(100), B=runif(100), D=rnorm(100,1,1))
df2 <- dplyr::rename(df, a=A, b=B, d=D)

```

This is the same functionality as the base `R` function `colnames` (or `names` for a data.frame)

```         
names(df2) <- c('a', 'b', 'd')
#or 
names(df2) <- tolower(names(df))
```

The nice part about `dplyr::rename()` is that we specify the old and new column names, meaning that there is little risk of an indexing error as with using the `colnames()` or `names()` functions.

## select

Many of the next functions are directly analogs of functions from another programming language used to query databases (SQL). This makes it really nice to learn, as you can essentially learn two languages while learning one. SQL is pretty powerful when working with relational data. I will not go into what I mean by this, unless there is time during lecture and interest among you all.

We use `dplyr::select` when we want to...select...columns.

```{r, eval=FALSE, echo=TRUE}

dplyr::select(df2, a)
dplyr::select(df2, obs = starts_with('a'))

```

## filter

`dplyr::filter` is another one of those useful functions that we already know how to use in base `R`. Previously, we have used `which` statements or the `subset` function. `dplyr::filter` is used to filter down a data.frame by some condition applied to rows.

```{r, eval=FALSE, echo=TRUE}

dplyr::filter(df2, a < 0.5)

```

## mutate

`dplyr::mutate` is used when we wish to create a new covariate based on our existing covariates. For instance, if we wanted to create a column `e` on `df2` that was the sum of `a+b` divided by `d`...

```{r}

df2 <- dplyr::mutate(df2, e=(a+b)/d)
head(df2,5)

```

Notice that the function creates a new column and appends it to the existing data.frame, but does not "write in place". That is, the `df2` object is not modified unless it is stored (which we do above).

## group_by

`dplyr::group_by` is really useful as an intermediate step to getting at summary statistics which take into account grouping by a character or factor variable. For instance, if we wanted to calculate the mean life expectancy (`lifeExp`) for every `country` in the gapminder data (`dat`), we would first have to group by `country`.

```{r}

datG <- dplyr::group_by(dat, country)

```

This is a bit like a non-function, since `dat` and `datG` are essentially the same....but they are not for the purposes of computing group-wise statistics. This is done using the `dplyr::summarise` function.

## summarise

So if we wanted to calculate mean life expectancy (`lifeExp`) per country, we could use the grouped data.frame `datG` and the `dplyr::summarise` function to do so.

```{r}

dplyr::summarise(datG, mnLife=mean(lifeExp))

```

## joins

joins are something taken directly from SQL. Table joins are ways of combining relational data by some index variable. That is, we often have situations where our data are inherently multi-dimensional. If we have a data.frame containing rows corresponding to observations of a species at a given location, we could have another data.frame containing species-level morphometric or trait data. While we could mash this into a single data.frame, it would repeat many values, which is not ideal for data clarity or memory management.

```{r}

df$species <- sample(c('dog', 'cat', 'bird'),100, replace=TRUE)

info <- data.frame(species=c('dog', 'cat', 'bird', 'snake'),
	annoying=c(10, 2, 100, 1), 
	meanBodySize=c(20, 5, 1, 2))

```

## joins

Now we can join some stuff together, combining data on mean species-level characteristics with individual-level observations.

```{r, eval=FALSE, echo=TRUE}

# maintains the structure of df (the "left" data structure)
left_join(df, info, by='species')

# maintains the structure of info (the "right" data structure)
right_join(df,info, by='species')

# return things that are in info but not in df
anti_join(info, df, by='species')

```

There are other forms of joins (`full_join`, `inner_join`, etc.), but I find that I mostly use the `left` or `right` variations of the joins, as it specifically allows me to control the output (i.e., using `dplyr::left_join`, I know that the resulting data.frame will have the same number of rows as the left hand data.frame).

## piping

Alright. So before we discussed joins, we were describing the different main verbs of `dplyr`. We discussed `rename`, `select`, `mutate`, `group_by`, and `summarise`. A final point, and something `tidyverse` folks really love, is the use of these functions in nested statements through the use of piping.

Pipes in bash scripting look like \|, pipes in `R` syntax look like %\>%. It does not matter what it looks like though, it matter what it does. Here is a simple example of the use of piping. We can go back to the example of calculating the mean life expectancy per country from the gapminder data.

The usual way

```{r}
tmp <- dplyr::group_by(dat, country)
tmp2 <- dplyr::summarise(tmp, mnLifeExp=mean(lifeExp))
```

The piped way

```{r}

tmp3 <- dat %>%
	dplyr::group_by(country) %>%
	dplyr::summarise(mnLifeExp=mean(lifeExp))

```

The results of these two are identical (`all(tmp3==tmp2)` returns TRUE).

This is useful, as commands can be chained together, including the creation of new variables, subsetting and summarising of existing variables, etc. One thing to keep in mind is to check intermediate results -- instead of just piping all the way through -- as data manipulation errors can be introduced mid-statement and go unnoticed. That is, in some situations, piping does not help reproducibility. Many proponents argue that it helps with code readability, while many others say that actively makes code less human readable. It definitely does require adopting a certain syntax and the assumption that every end user is on the `tidyverse` train, which is not ideal when reproducibility involves everyone, not just the cool tidy/Hadley/RStudio crowd.





# Next time

-   Exploratory data analysis - what do you do immediately after collecting your data?

# Week 7A - Exploratory Data Analysis

## Midterm Feedback

-   Thank you all for providing thoughtful feedback on the last Problem Set!
-   You all really enjoy in-class exercises - we'll do more of that!
-   The homework difficulty doesn't always align well with the lectures - will try to remedy this

# Where do we begin?

-   A major goal of statistics is to estimate **parameters** of a population so that we can compare them to values that are of practical importance to our understanding of the system, or to compare parameter estimates between and among different populations
-   This is the first important step before getting to hypothesis testing!

## Understanding Populations and their Parameters

-   We often think about the samples we are collecting as a part of a larger population
-   Since we can't measure every member of that population, we instead use sampling to estimate the parameters of the population as a whole
    -   Some common parameters: mean, range, median
    -   If we performed random sampling, we assume that the parameter estimates of our sample are equitable to the true population parameters

# Simulations to compare parameter estimates

-   Let's use our distribution functions from last time to set up some data to play with
-   Let's imagine our data is made up of counts, with an average of 5 counts - which distribution would fit that data best?
-   Ex: number of hours spent doing homework by UO undergraduates

# Simulations to compare parameter estimates

-   Let's use our distribution functions from last time to set up some data to play with
-   Let's imagine our data is made up of counts, with an average of 5 counts - which distribution would fit that data best?
-   Ex: number of hours spent doing homework by UO undergraduates

```{r,  echo=TRUE, eval=FALSE}
true_pop <- rpois(n=1000, lambda = 5)
hist(true_pop)
```

## Simulations to compare parameter estimates

```{r,  echo=TRUE, out.width="50%"}
set.seed(32)
true_pop <- rpois(n=1000, lambda = 5)
hist(true_pop, xlim = c(0,16))
```

## Calculating parameters

-   How would we calculate the mean and range for this population?

## Calculating parameters

-   How would we calculate the mean and range for this population?

```{r, echo=TRUE, eval=TRUE}
mean(true_pop)
range(true_pop)
median(true_pop)
```

# Sampling

-   Those values represent our true population parameters, which we often cannot know.
-   Sampling is the method used in academia and science to try and estimate the true population values.
-   We have also discussed how **sample size** can greatly affect the accuracy of our estimates

## Sampling Exercise

-   Since we are working with simulated data, we can also afford to simulate our sampling!
-   Try taking a sample from our `true_pop` dataset and change the sample size, then calculate the mean and range for your sample and see how it compares to the true values.
-   How many college students are you including in your survey?

## Sampling Exercise

```{r, echo=FALSE, out.width="50%"}
sample1 <- sample(true_pop, size = 20)
hist(sample1,  xlim =  c(0,16))
print(c("Mean: ", mean(sample1))) 
print(c("Range: ", range(sample1)))
```

## Randomness in Sampling

-   Because of the randomness of sampling, you may get close to the true estimates even with a small sample size - but your results will change each time you take a new sample of the same size
-   How do we get a feel for how accurate each sample size is? Or which sample size is recommended?

## Surveying your Sampling

```{r, out.width="50%"}

samps_var <- replicate(50, sample(true_pop, 10)) #Take 50 samples of size 10
samps_var_means <- apply(samps_var, 2, mean) #Calculate the mean from each sample
hist(samps_var_means) #Plot the distribution of sample means


```

## Surveying your Sampling

-   We get close to the true mean (4.9) about 2/3rds of the time - is this good enough?

```{r}
table(samps_var_means > 4.5 & samps_var_means < 5.5)
```

## Surveying your Sampling

-   This sampling variation is what we have to deal with, and account for, as empirical scientists.
-   If this had been a real-world scenario, we likely would be basing our estimate for the true mean on just a single sample mean.
-   Getting close to the idea of **power** - does our experimental design have the power to detect the parameters we are interested in?

# Sampling Distributions

-   The previous exercise illustrates the concept of a sampling distribution.
-   We sampled over and over again (50 times) and calculated the mean for each sample to demonstrate the sampling distribution for the mean, our original parameter of interest.
-   One important point is that the sampling distribution for a given parameter is often very different from the variable’s distribution in the population. In many cases, the sampling distribution is normal or approximately so.

## Normal Sampling Distributions

```{r, echo=FALSE, out.width='80%', fig.align='center'}
knitr::include_graphics("images/week_2.025.jpeg")
```

## Normal Sampling Distributions

```{r, echo=FALSE}
set.seed(32)
true_pop <- runif(n=1000, min=0, max=100)
```

```{r, out.width="50%", echo=FALSE}
samps_var <- replicate(50, sample(true_pop, 10)) #Take 50 samples of size 10
samps_var_means <- apply(samps_var, 2, mean) #Calculate the mean from each sample
#title(main = "Sampling distribution from a Uniform distribution of 0-100")
hist(samps_var_means, main = "Sampling distribution from a Uniform distribution of 0-100", xlim = c(0,100))
```

## Sampling Distributions - It's been Normal this whole time?!

```{r, echo=FALSE, out.width='80%', fig.align='center'}
knitr::include_graphics("images/week7_normalsampling.jpeg")
```

## Sampling Distributions - Exercise

-   Try creating some data using one of the other distributions we discussed last time (Exponential Distribution) and then create a sampling distribution. Is it normal?

```{r, eval=FALSE}
true_pop <- rexp(n, rate)
samps_var <- replicate(n = , sample(true_pop, size = ))
samps_var_means <- apply(samps_var, 2, mean)
```

## The Central Limit Theorem

-   For most real world data sets we can’t empirically determine a sampling distribution by taking many actual samples, because we often have just the one sample.
-   Fortunately, we can rely on the **Central Limit Theorem** to make some assumptions about sampling distributions, particularly when estimating a mean from a single sample, or when estimating most any parameter using a “pseudo” or re-sampling process we refer to as “**bootstrapping**”

# Standard Error - the Range of the Sampling Distribution

-   The sampling distribution models all values we might have obtained from our sample and their probabilities of occurrence.
-   The standard error of an estimate can be conceptualized as the standard deviation of a sampling distribution.
-   So, whenever we obtain a parameter estimate, we need to include the standard error in some form or another, which is a measure of the precision of our estimate.

## Standard Error

-   The sample size and the spread of the distribution (range) - contribute to what is known as the standard error of a random variable.
-   The standard error for any given sample attribute (such as a sample mean), can be calculated either based on distributional assumptions, or by a process called “resampling.”

## Standard error

```{r, echo=FALSE, out.width='80%', fig.align='center'}
knitr::include_graphics("images/week_2.028.jpeg")
```

## Calculating the Standard Error

-   Standard Error of the Mean (SEM)
-   SEM = SD / sqrt(sample size)

## Calculating the Standard Error

-   Think conceptually - how will SEM change as sample size increases?

```{r}
set.seed(32)
true_pop <- rpois(n=1000, lambda = 5)
samps_var <- replicate(n = 50, sample(true_pop, size = 5))
samps_var_means <- apply(samps_var, 2, mean)
sem = sd(samps_var_means)/ sqrt(length(samps_var_means))
print(sem)
```

# Bootstrapping

-Unfortunately, most other kinds of estimates (anything not the mean) do not have this amazing property, but we can rely on another approach to calculate the standard error. - This involves generating your own sampling distribution for the estimate using the “**bootstrap**,” a method invented by Efron (1979). - We call the bootstrap, and other methods that do not rely on distributional assumptions of the variable itself, “**nonparametric**” approaches.

## Easy steps for bootstrapping in R

1.  Take a random sample (with replacement) from your sample data
2.  Calculate the estimate using the measurements in the bootstrap sample (step 1). This is the first bootstrap replicate estimate
3.  Repeat steps 1 and 2 a large number of times (1000 times is reasonable)
4.  Calculate the sample standard deviation of all the bootstrap replicate estimates obtained in step 3 (SSD = sd(sample)/sqrt(sample size))

## Bootstrapping

-   The resulting quantity is called the “bootstrap standard error”
-   The bootstrap can be applied to almost any sample statistic, including means, proportions, correlations, and regression parameters.
-   It works when there is no ready formula for a standard error, for example when estimating the median, trimmed mean, correlation, eigenvalue, etc.
-   It is nonparametric, so doesn’t require normally-distributed data, as mentioned. - - It works well for parameter estimates that are based on complicated sampling procedures or calculations. For example, it is used to assess confidence in local relationships within phylogenetic trees.

## Confidence Intervals

-   A confidence interval is a range of values about a parameter estimate, such that we are X% certain that the true population parameter value lies within that interval.
-   For now, know that for a normally distributed sample, a confidence interval about the population mean can be calculated using the t.test() function in base R.
-   The 95% confidence interval is commonly reported in statistical analysis results, by convention, but other values are occasionally reported as well.

# Relationship between Mean and Variance

-   Population means and variances (and hence standard deviations) tend to have a strong, positive relationship.
-   So an otherwise similarly shaped distribution, but with a much larger mean, will by default have a much larger standard deviation

## Relationship between mean and variance

-   This means it is inappropriate to compare variations of different populations with largely different means
    -   For instance, comparing the standard deviation for a body measurement in a population of mice, with the same body measurement in a population of elephants is not meaningful.

## Test it out

-   Try changing the lambda of the original population, and see how the SD changes

```{r, eval=FALSE}
set.seed(32)
true_pop <- rpois(n=1000, lambda = 5)
print(sd(true_pop))
```

## Coefficient of Variation

-   To make standard deviations comparable across populations with very different means, we can instead compare a standardized metric called the “coefficient of variation” (CV), which is simply the sample standard deviation divided by the sample mean (and usually expressed as a % by multiplying by 100).

------------------------------------------------------------------------

------------------------------------------------------------------------

# Week 6A: Ethics in Data Science

## Today we'll be talking about

-   A roadmap for the rest of the term
-   The unethical history of statistics and data science
-   Last week's homework

# What Stats are we learning?

-   Probability, distributions, and sampling
-   Exploratory data analysis
-   Test statistics, p-values, type 1 and 2 errors
-   Types of data, t-tests
-   Linear models, ordinary least squares, ANOVA
-   Experimental design

## Thinking about your final project

-   As we're going through these statistical techniques, think about which one(s) might be most appropriate for your data analysis.
-   The project ideally will take in some sort of raw data, analyze it (summary statistics, significance tests), and plot your findings.
-   Can incorporate Unix and/or R, can be presented via a script and/or RMarkdown document.

# The History of Statistics

## Galton as the Father of Eugenics

-   Francis Galton: Darwin's half cousin
-   Studied human variation and genetic inheritance
    -   Human height, fingerprints, intelligence
    -   Correlation, regression toward the mean, and "nature versus nurture"
    -   Pioneered twin studies

![Galton](images/week6_Galton.jpeg){fig.align="center"}

## Galton as the Father of Eugenics

-   Believed that intelligence was hereditary based on surveying prominent academics in Europe
-   Used the ideas of **correlation** and **regression towards the mean** to argue that the upper class should breed amongst themselves to keep those "good genes" pure
-   Wanted to provide monetary incentives for "good" couples to marry and reproduce as a way to avoid the upper class being genetically muddied by the lower class

## A common sight at state fairs around the U.S. in the 1930s

-   Competitions for the "perfect family" to encourage public consciousness and support for eugenics

```{r, echo=FALSE, out.width='50%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/IMAGES2.013.jpeg")
```

## Galton with Charles Davenport and G. Stanley Hall

-   American Eugenics Record Office (ERO) founded in Cold Springs Harbor

```{r, echo=FALSE, out.width='50%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/IMAGES2.014.jpeg")
```

## Logo of the US eugenics society

```{r, echo=FALSE, out.width='80%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/IMAGES2.015.jpeg")
```

## Eugenics societies in America

-   Advocated for state laws to ban interracial marriages and promote sterilization of "unfit" individuals (negative eugenics) - especially black, Latinx, and Native American women
-   30 states passed laws to force mental institution patients to be sterilized
-   Between 1907 and 1963, over 64,000 individuals were forcibly sterilized under eugenic legislation in the United States

## RA Fisher and Eugenics in London

-   Developer of Fishers exact test, analysis of variance (ANOVA), null hypothesis, p values, maximum likelihood, probability density functions
-   Founding Chairman of the University of Cambridge Eugenics Society

```{r, echo=FALSE, out.width='20%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/week6_fisher.jpeg")
```

## RA Fisher and Eugenics

-   1/3rd of his work "The Genetical Theory of Natural Selection" discussed eugenics and his theory that the fall of civilizations was due to the fertility of their upper classes being diminished
-   Used these statistical methods to test data on human variation to prove biological differences between human races
-   Eugenics and racism were the primary motivators for many of these statistical tests that we use today

## Eugenics has a direct line to Hitler and Nazism

-   Eugenics existed in America (and England) before it became popular in Germany.
-   By 1933, California had subjected more people to forceful sterilization than all other U.S. states combined.
-   The forced sterilization program engineered by the Nazis was partly inspired by California's.

```{r, echo=FALSE, out.width='70%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/IMAGES2.016.jpeg")
```

## Sterilizations continue in America

-   Our history books paint Nazi Germany as the primary evil of that time, while we try to cover up our significant role in eugenics
-   It wasn't until 1978 that the US passed regulations on sterilization procedures
-   California only passed a bill to outlaw sterilization of inmates in 2014
-   Certain members of the genetic engineering community threaten to bring back eugenics ideas

## So, what do we do from here?

-   The statistical methods that Galton, Fisher, and others developed are useful science tools
-   Important to use these tools for good - improving our planet, human health, and technology
-   Important to acknowledge and not forget the history of science - educate others to avoid repeating history

## Interested in learning more?

```{r, echo=FALSE, out.width='20%', fig.asp=.75, fig.align='center'}
knitr::include_graphics("images/IMAGES2.017.jpeg")
```

-   See also "The Gene" by Siddhartha Mukherjee

------------------------------------------------------------------------
