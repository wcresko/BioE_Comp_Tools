---
title: "Week 10a - Statistics for Bioengineering"
author: "Bill Cresko"
format: 
  revealjs:
    transition: fade
    transition-speed: slow
editor: visual
---

```{r}
library(tidyverse)
library(gt)
library(readxl)
library(ggplot2)
library(tibble)
library(boot)
```

## This week

-   The problem of overfitting

-   LOESS (Locally Estimated Scatterplot Smoothing)

-   Cross validation using the `boot` package

-   Decision trees and random forests

-   Support Vector Machines (if there's time)

## Building Models for Prediction

-   We build a model using the sample data with the goal of applying it to new data from the population to make predictions about response variables of interest.

-   Statistical and machine learning models are built to fit the data on which they're trained as closely as possible within their particular structures.

## Optimization Examples

-   **Linear regression** picks out the line that minimizes the squared residuals (RSE) in the set
-   **Logistic regression** minimizes residual deviance
-   **Classification models** might be optimized to identify as high a proportion of the sample correctly as possible

## The Overfitting Problem

-   Unfortunately, models that perform better on their training data are often worse at making predictions from new data.

-   **Overfitting** refers to when a model goes too far out of its way to accommodate the peculiarities of the set used to build it, integrating random noise as if it were meaningful information.

-   More flexible modeling techniques are more susceptible to overfitting.

## Overfitting vs. Underfitting

::::::: columns
:::: {.column width="50%"}
::: callout-caution
## Underfitting

-   **Model too simple**
-   High bias
-   Misses important patterns
-   Poor performance everywhere
:::
::::

:::: {.column width="50%"}
::: callout-warning
## Overfitting

-   **Model too complex**
-   High variance\
-   Memorizes noise
-   Great on training, poor on testing
:::
::::
:::::::

::: callout-tip
## The Solution

Measuring model performance on training data is unreliable - we need **cross-validation**.
:::

# LOESS

## What is LOESS?

-   **LOESS** = **LO**cally w**E**ighted **S**catterplot **S**moothing
-   Also known as LOWESS (locally weighted regression)
-   Non-parametric regression method
-   It fits **local regressions** to subsets of the data
-   Each local fit is weighted using a **kernel function**
-   One popular kernel: the **tricube weighting function**

## The Big Picture

::::::: columns
::: {.column width="60%"}
```{r}
#| echo: false
#| fig-width: 8
#| fig-height: 6
# Simulated data showing non-linear relationship
set.seed(123)
x <- seq(0, 4*pi, length.out = 100)
y <- sin(x) + 0.3*rnorm(100)

# Create comparison plot
plot(x, y, pch = 16, col = "gray60", 
     main = "Parametric vs Non-Parametric",
     xlab = "X", ylab = "Y")

# Linear regression
abline(lm(y ~ x), col = "red", lwd = 2)

# LOESS
loess_fit <- loess(y ~ x, span = 0.3)
lines(x, predict(loess_fit), col = "blue", lwd = 3)

legend("topright", 
       c("Linear Regression", "LOESS"), 
       col = c("red", "blue"), 
       lwd = c(2, 3))
```
:::

::::: {.column width="40%"}
::: fragment
**Linear regression** assumes a straight line relationship
:::

::: fragment
**LOESS** discovers the underlying pattern without assumptions
:::
:::::
:::::::

## Historical Context

-   **Built on earlier work:**
    -   Kernel regression methods
    -   k-nearest neighbor approaches
    -   Local polynomial fitting
-   **Part of the "smooth" revolution** in statistics

## How LOESS Works: The Intuition

::: fragment
**For each point where you want a prediction:**
:::

::: incremental
1.  **Find neighbors** - Identify nearby data points
2.  **Assign weights** - Closer points get higher weights\
3.  **Fit locally** - Use weighted regression on neighbors
4.  **Make prediction** - Use the local model
5.  **Repeat** - Do this for every point
:::

## Visual Intuition

```{r}
#| echo: false
#| fig-width: 10
#| fig-height: 7
# Create visualization of local fitting
set.seed(42)
x <- seq(0, 10, length.out = 50)
y <- 2*sin(x) + rnorm(50, 0, 0.5)

# Choose a focal point
focal_x <- 5
focal_y <- 2*sin(focal_x)

# Calculate distances and weights
distances <- abs(x - focal_x)
span <- 0.4
k <- round(span * length(x))
cutoff <- sort(distances)[k]
weights <- ifelse(distances <= cutoff, 
                  (1 - (distances/cutoff)^3)^3, 0)

# Create the plot
plot(x, y, pch = 16, cex = 1.2, col = "lightgray",
     main = "LOESS Local Fitting Concept",
     xlab = "X", ylab = "Y")

# Highlight the focal point
points(focal_x, focal_y, pch = 16, cex = 2, col = "red")

# Show weighted points
point_colors <- rgb(0, 0, 1, alpha = weights)
points(x, y, pch = 16, cex = 1.5, col = point_colors)

# Add local regression line
local_data <- data.frame(x = x[weights > 0], 
                        y = y[weights > 0], 
                        w = weights[weights > 0])
local_fit <- lm(y ~ x, data = local_data, weights = w)
abline(local_fit, col = "blue", lwd = 2)

# Add legend
legend("topright", 
       c("Focal Point", "Weighted Points", "Local Fit"),
       col = c("red", "blue", "blue"),
       pch = c(16, 16, NA),
       lty = c(NA, NA, 1),
       lwd = c(NA, NA, 2))
```

## The Tricube Function

The tricube weight function is defined as:

$$
w(x) =
\begin{cases}
(1 - |x|^3)^3 & \text{if } |x| < 1 \\
0 & \text{otherwise}
\end{cases}
$$

-   Defined on the interval $[-1, 1]$
-   Smooth and bell-shaped
-   Weight goes to 0 smoothly at the boundary

## Tricube Weighting Function

```{r}
#| echo: false
#| eval: true
tricube_weight <- function(x) {
  ifelse(abs(x) < 1, (1 - abs(x)^3)^3, 0)
}

x_vals <- seq(-1.5, 1.5, length.out = 1000)
y_vals <- tricube_weight(x_vals)
```

```{r}
#| echo: false
#| fig-width: 8
#| fig-height: 5
ggplot(data.frame(x = x_vals, weight = y_vals), aes(x, weight)) +
  geom_ribbon(aes(ymin = 0, ymax = weight), alpha = 0.3, fill = "blue") +
  geom_line(color = "darkblue", linewidth = 1.5) +
  geom_vline(xintercept = c(-1, 1), color = "red", linetype = "dashed") +
  labs(title = "Tricube Weighting Function",
       subtitle = "Points get zero weight outside [-1, 1] interval",
       x = "Normalized Distance", y = "Weight") +
  theme_minimal()
```

## Step 1: Neighborhood Selection

-   Choose a **bandwidth** or **span** parameter (α)
-   Typically ranges from 0.2 to 0.8
-   α = 0.5 means use 50% of data points for each local fit
-   Larger α → smoother curve
-   Smaller α → more flexible, follows data closely

## Step 2: Distance-Based Weighting

-   Points closer to target point get higher weights
-   Points farther away get lower weights
-   Weight decreases smoothly with distance
-   Creates smooth transitions between local fits

## Step 3: Local Polynomial Fitting

-   Fit polynomial regression to weighted neighborhood
-   Usually **degree 1** (linear) or **degree 2** (quadratic)
-   Linear: good for straight-line segments
-   Quadratic: better for curved regions
-   Higher degrees rarely used (overfitting risk)

## When to Use LOESS

::: callout-tip
## LOESS is Great For:

-   **Exploratory data analysis** - Understanding relationships
-   **Non-linear patterns** - Complex curves and trends\
-   **Visualization** - Smooth trend lines
-   **Flexible modeling** - When you don't know the functional form
:::

::: callout-warning
## LOESS Limitations:

-   **Large datasets** - Computationally intensive
-   **Extrapolation** - Poor predictions outside data range
-   **High dimensions** - Curse of dimensionality
-   **Inference** - Limited theoretical framework
:::

# Example of overfitting a model

## Data Generation

```{r}
#| echo: true
#| eval: true
set.seed(1)
x <- runif(1000, 0, 10)
y <- 6 - .5 * x + rnorm(1000, sd = 2)
df <- data.frame(x, y)

set.seed(1)
df_sample <- df |> slice_sample(n = 25)
```

::: callout-note
## What We're Creating

-   **Population**: 1000 points with true relationship y = 6 - 0.5x + noise
-   **Sample**: 25 randomly selected points for model fitting
:::

## Population Data with True Relationship

```{r}
#| echo: true
#| eval: false
#| fig-width: 8
#| fig-height: 6
ggplot(df, aes(x, y)) + 
  geom_point(color = "lightblue") + 
  geom_abline(slope = -.5, 
              intercept = 6,
              color = "blue",
              linewidth = 1) +
  labs(title = "Population Data with True Relationship",
       subtitle = "Blue line shows true relationship: y = 6 - 0.5x")
```

## Population Data with True Relationship

```{r}
#| echo: false
#| eval: true
#| fig-width: 8
#| fig-height: 6
ggplot(df, aes(x, y)) + 
  geom_point(color = "lightblue") + 
  geom_abline(slope = -.5, 
              intercept = 6,
              color = "blue",
              linewidth = 1) +
  labs(title = "Population Data with True Relationship",
       subtitle = "Blue line shows true relationship: y = 6 - 0.5x")
```

## Sample Data with Smooth Fit

```{r}
#| echo: true
#| eval: false
#| fig-width: 8
#| fig-height: 6
ggplot(df, aes(x, y)) + 
  geom_point(color = "lightblue") + 
  geom_abline(slope = -.5, 
              intercept = 6,
              color = "blue",
              linewidth = 1) +
  geom_point(data = df_sample,
             color = "black") +
  geom_smooth(data = df_sample,
              se = FALSE,
              color = "black") +
  labs(title = "Sample Data with LOESS Smooth",
       subtitle = "Black points: sample data, Black curve: LOESS fit")
```

## Sample Data with Linear Fit

```{r}
#| echo: true
#| eval: false
#| fig-width: 8
#| fig-height: 6
ggplot(df, aes(x, y)) + 
  geom_point(color = "lightblue") + 
  geom_abline(slope = -.5, 
              intercept = 6,
              color = "blue",
              linewidth = 1) +
  geom_point(data = df_sample,
             color = "black") +
  geom_smooth(data = df_sample,
              se = FALSE,
              method = "lm",
              color = "black") +
  labs(title = "Sample Data with Linear Fit",
       subtitle = "Black points: sample data, Black line: linear regression")
```

## 

```{r}
#| echo: false
#| layout-ncol: 2
#| fig-width: 6
#| fig-height: 4.5
#| fig-cap: 
#|   - "Sample Data with LOESS Smooth"
#|   - "Sample Data with Linear Fit"

# Plot 1: LOESS Smooth
ggplot(df, aes(x, y)) + 
  geom_point(color = "lightblue") + 
  geom_abline(slope = -.5, 
              intercept = 6,
              color = "blue",
              linewidth = 1) +
  geom_point(data = df_sample,
             color = "black") +
  geom_smooth(data = df_sample,
              se = FALSE,
              color = "black") +
  labs(title = "Sample Data with LOESS Smooth",
       subtitle = "Black points: sample data\nBlack curve: LOESS fit")

# Plot 2: Linear Fit
ggplot(df, aes(x, y)) + 
  geom_point(color = "lightblue") + 
  geom_abline(slope = -.5, 
              intercept = 6,
              color = "blue",
              linewidth = 1) +
  geom_point(data = df_sample,
             color = "black") +
  geom_smooth(data = df_sample,
              se = FALSE,
              method = "lm",
              color = "black") +
  labs(title = "Sample Data with Linear Fit",
       subtitle = "Black points: sample data\nBlack line: linear regression")
```

## Model Building

```{r}
#| echo: true
#| eval: true

model_linear <- lm(y ~ x, data = df_sample)
model_loess <- loess(y ~ x, data = df_sample)
```

##  {.smaller}

::::: columns
::: {.column width="50%"}
Linear Model Summary

```{r}
#| echo: true
summary(model_linear)
```
:::

::: {.column width="50%"}
LOESS Model Summary

```{r}
#| echo: true
summary(model_loess)
```
:::
:::::

## Model Evaluation

```{r}
#| echo: true
# Calculate RSS for both models

df_resid <- df %>% 
  mutate(linear_resid = df$y - predict(model_linear, df),
         loess_resid = df$y - predict(model_loess, df)) %>% 
  drop_na()

rss_linear <- sum(df_resid$linear_resid^2)
rss_loess <- sum(df_resid$loess_resid^2)
```

```{r}
#| echo: false
#| eval: true
cat("Linear Model RSS:", round(rss_linear, 2), "\n")
cat("LOESS Model RSS:", round(rss_loess, 2), "\n")
cat("Difference:", round(rss_linear - rss_loess, 2))
```

::: callout-important
## Key Finding

The more flexible LOESS model performs **worse** on new data despite fitting the sample better - this is **overfitting**!
:::

# Testing set vs. cross-validation

## The Testing Set Approach

::::::: columns
:::: {.column width="50%"}
::: callout-note
## Simple Solution

Split data into training and testing sets

**Process:** 1. Train model on training set 2. Evaluate on testing set 3. Report testing performance
:::
::::

:::: {.column width="50%"}
::: callout-tip
## ✅ Benefits

-   Easy to understand and implement
-   Computationally very efficient
-   Clear separation of training/testing
:::
::::
:::::::

::: callout-warning
## ❌ Major Drawbacks

-   **Reduced training data** - Less data to learn from
-   **Split dependency** - Results vary based on random split
-   **High variance** - Unreliable performance estimates
:::

## Cross-Validation: A Better Approach

::: callout-important
## The Cross-Validation Process

1.  **Split** data into K folds
2.  **Train** on K-1 folds, test on 1 fold\
3.  **Repeat** K times (each fold gets to be the test set)
4.  **Average** results across all folds
:::

::::::: columns
:::: {.column width="50%"}
::: callout-tip
## Key Benefits

-   Uses all data for both training and testing
-   More reliable performance estimates
-   Reduces variance in results
:::
::::

:::: {.column width="50%"}
::: callout-note
## Common Choices

-   **K = 5**: Good balance of bias/variance
-   **K = 10**: Most popular choice
-   **K = n**: Leave-one-out (LOOCV)
:::
::::
:::::::

# Cross-Validation with the boot Library in R

## What is Cross-Validation?

Cross-validation is a statistical method used to:

-   **Assess model performance** on unseen data
-   **Reduce overfitting** by testing generalization
-   **Compare different models** objectively
-   **Estimate prediction error** more reliably

The core principle: Use part of your data for training, part for testing

## What is Cross-Validation?

Cross-validation is a statistical technique for assessing how well a model generalizes to new data by repeatedly splitting your dataset into training and testing portions.

**The Process:**

1.  Split your data into k groups (folds)
2.  Train your model on k-1 folds\
3.  Test on the remaining fold
4.  Repeat this process k times
5.  Average the results to get a robust estimate of model performance

## Key Functions in Boot Package

| Function        | Purpose                         |
|-----------------|---------------------------------|
| `cv.glm()`      | Cross-validation for GLMs       |
| `boot()`        | Bootstrap resampling            |
| `boot.ci()`     | Bootstrap confidence intervals  |
| `cv.glm.object` | Cross-validation results object |

## Types of Cross-Validation

1.  **Leave-One-Out (LOOCV)**
2.  **K-Fold Cross-Validation**
3.  **Stratified Cross-Validation**
4.  **Time Series Cross-Validation**

## Leave-One-Out Cross-Validation (LOOCV) {.smaller}

**Concept**: Use n-1 observations for training, 1 for testing, repeat n times

```{r}
#| echo: true

library(boot)
# LOOCV with cv.glm
model <- glm(mpg ~ hp + wt, data = mtcars)
loocv_error <- cv.glm(mtcars, model)

# Extract CV error
loocv_mse <- loocv_error$delta[1]
print(paste("LOOCV MSE:", round(loocv_mse, 3)))
```

**Advantages**: Uses maximum data for training **Disadvantages**: Computationally expensive for large datasets

## K-Fold Cross-Validation {.smaller}

**Concept**: Divide data into K folds, use K-1 for training, 1 for testing

```{r}
#| echo: true
# 10-fold cross-validation
model <- glm(mpg ~ hp + wt + disp, data = mtcars)
k10_error <- cv.glm(mtcars, model, K = 10)

# Results
cv_mse <- k10_error$delta[1]
cv_mse_adjusted <- k10_error$delta[2]  # Bias-corrected

print(paste("10-fold CV MSE:", round(cv_mse, 3)))
```

**Common choices**: K = 5, 10, or √n

## Practical Example: Polynomial Regression {.smaller}

```{r}
#| echo: true
library(boot)
library(ggplot2)

# Generate sample data
set.seed(123)
n <- 100
x <- runif(n, 0, 2*pi)
y <- sin(x) + rnorm(n, 0, 0.3)
data <- data.frame(x = x, y = y)

# Test different polynomial degrees
degrees <- 1:10
cv_errors <- numeric(length(degrees))

for(i in seq_along(degrees)) {
  # Fit polynomial model
  model <- glm(y ~ poly(x, degrees[i]), data = data)
  
  # 10-fold cross-validation
  cv_result <- cv.glm(data, model, K = 10)
  cv_errors[i] <- cv_result$delta[1]
}
```

## Visualizing CV Results {.smaller}

```{r}
#| echo: true
# Create results data frame
results <- data.frame(
  degree = degrees,
  cv_error = cv_errors
)

# Plot CV error vs polynomial degree
ggplot(results, aes(x = degree, y = cv_error)) +
  geom_line(color = "blue", size = 1) +
  geom_point(color = "red", size = 2) +
  labs(title = "Cross-Validation Error vs Polynomial Degree",
       x = "Polynomial Degree",
       y = "10-Fold CV Error") +
  theme_minimal()

# Find optimal degree
optimal_degree <- degrees[which.min(cv_errors)]
print(paste("Optimal polynomial degree:", optimal_degree))
```

## Custom Cross-Validation Function {.smaller}

```{r}
#| echo: true
# Custom CV function for specific needs
custom_cv <- function(data, model_formula, K = 10) {
  n <- nrow(data)
  folds <- sample(rep(1:K, length.out = n))
  errors <- numeric(K)
  
  for(k in 1:K) {
    # Split data
    train_data <- data[folds != k, ]
    test_data <- data[folds == k, ]
    
    # Fit model on training data
    model <- lm(model_formula, data = train_data)
    
    # Predict on test data
    predictions <- predict(model, test_data)
    
    # Calculate error
    errors[k] <- mean((test_data$y - predictions)^2)
  }
  
  return(list(
    cv_error = mean(errors),
    fold_errors = errors,
    se = sd(errors) / sqrt(K)
  ))
}
```

## Model Comparison Example {.smaller}

```{r}
#| echo: true
# Compare different models
models <- list(
  linear = y ~ x,
  quadratic = y ~ x + I(x^2),
  cubic = y ~ x + I(x^2) + I(x^3),
  sine = y ~ sin(x) + cos(x)
)

# Perform CV for each model
cv_results <- list()
for(name in names(models)) {
  model <- glm(models[[name]], data = data)
  cv_results[[name]] <- cv.glm(data, model, K = 10)$delta[1]
}

# Display results
cv_comparison <- data.frame(
  Model = names(cv_results),
  CV_Error = unlist(cv_results)
)
print(cv_comparison)
```

## Bootstrap Cross-Validation {.smaller}

```{r}
#| echo: true
# Bootstrap approach for CV
bootstrap_cv <- function(data, indices) {
  # Sample data with replacement
  boot_data <- data[indices, ]
  
  # Fit model
  model <- lm(y ~ poly(x, 3), data = boot_data)
  
  # Predict on original data (out-of-bag prediction)
  predictions <- predict(model, data)
  mse <- mean((data$y - predictions)^2)
  
  return(mse)
}

# Perform bootstrap CV
set.seed(123)
boot_results <- boot(data, bootstrap_cv, R = 1000)

# Bootstrap CV estimate
boot_cv_error <- mean(boot_results$t)
boot_cv_se <- sd(boot_results$t)

print(paste("Bootstrap CV Error:", round(boot_cv_error, 3), 
           "±", round(boot_cv_se, 3)))
```

## Stratified Cross-Validation {.smaller}

For classification problems:

```{r}
#| echo: true
# Example with classification
library(MASS)
data(iris)

# Create stratified folds manually
create_stratified_folds <- function(y, K = 10) {
  folds <- vector("list", K)
  classes <- unique(y)
  
  for(class in classes) {
    class_indices <- which(y == class)
    class_folds <- sample(rep(1:K, length.out = length(class_indices)))
    
    for(k in 1:K) {
      folds[[k]] <- c(folds[[k]], class_indices[class_folds == k])
    }
  }
  return(folds)
}

# Use stratified folds
folds <- create_stratified_folds(iris$Species, K = 5)
```

## Time Series Cross-Validation {.smaller}

For temporal data:

```{r}
#| echo: true
# Time series CV (forward chaining)
ts_cv <- function(data, window_size, horizon = 1) {
  n <- nrow(data)
  n_folds <- n - window_size - horizon + 1
  errors <- numeric(n_folds)
  
  for(i in 1:n_folds) {
    # Training window
    train_end <- window_size + i - 1
    train_data <- data[i:train_end, ]
    
    # Test data
    test_start <- train_end + 1
    test_end <- min(test_start + horizon - 1, n)
    test_data <- data[test_start:test_end, ]
    
    # Fit and predict (example with linear model)
    model <- lm(y ~ x, data = train_data)
    pred <- predict(model, test_data)
    errors[i] <- mean((test_data$y - pred)^2)
  }
  
  return(mean(errors))
}
```

## Advanced: Nested Cross-Validation {.smaller}

For hyperparameter tuning:

```{r}
#| echo: true
# Nested CV for model selection and evaluation
nested_cv <- function(data, param_grid, outer_K = 10, inner_K = 5) {
  n <- nrow(data)
  outer_folds <- sample(rep(1:outer_K, length.out = n))
  outer_errors <- numeric(outer_K)
  
  for(k in 1:outer_K) {
    # Outer split
    train_data <- data[outer_folds != k, ]
    test_data <- data[outer_folds == k, ]
    
    # Inner CV for parameter selection
    best_param <- tune_parameters(train_data, param_grid, inner_K)
    
    # Fit final model with best parameters
    final_model <- fit_model(train_data, best_param)
    
    # Evaluate on outer test set
    pred <- predict(final_model, test_data)
    outer_errors[k] <- mean((test_data$y - pred)^2)
  }
  
  return(list(
    cv_error = mean(outer_errors),
    se = sd(outer_errors) / sqrt(outer_K)
  ))
}
```
