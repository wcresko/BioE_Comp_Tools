---
title: "Week 3a - Statistics for Bioengineering"
author: "Bill Cresko"
format: 
  revealjs:
    footer: BioE_Stats_2025 - Knight Campus
    transition: fade
    transition-speed: slow
editor: visual
---

```{r}
library(tidyverse)
library(gt)
library(readxl)
```

## Today we will

-   Wrangle some data and explore it
-   Foundations of probability
-   Sampling and point estimations

::: callout-note
Homeworks will be assigned this evening and be due in 2 weeks
:::

## What type of plot do I use for each data type?

![Flow chart to determine what type of data visualization and which ggplot geom to use](images/Chart_flow_chart.jpeg)

## Three dimensional data

```{r echo=TRUE}
set.seed(345) # make the example reproducible
a <- rnorm(100,10,10)
b <- rnorm(100,5,5)
c <- rnorm(100,1,1)
d <- data.frame(a,b,c)

library(MASS)
DENS <- kde2d(d$a,d$b)
contour(DENS)
```

## Three dimensional data

```{r echo=TRUE}
set.seed(345) # make the example reproducible
a <- rnorm(100,10,10)
b <- rnorm(100,5,5)
c <- rnorm(100,1,1)
d <- data.frame(a,b,c)

library(MASS)
DENS <- kde2d(d$a,d$b)
filled.contour(DENS,plot.axes = {
  axis(1)
  axis(2)
contour(DENS,add = TRUE)})
```

## Three dimensional data

```{r echo=TRUE}
set.seed(345) # make the example reproducible
a <- rnorm(100,10,10)
b <- rnorm(100,5,5)
c <- rnorm(100,1,1)
d <- data.frame(a,b,c)

library(ggplot2)
ggplot(d,aes(x=a,y=b)) +
  geom_density2d()
```

# Examples of the good, bad and the ugly of graphical representation of data

##  {.flexbox .vcenter}

-   Examples of bad graphs and how to improve them.
-   Courtesy of K.W. Broman\
-   www.biostat.wisc.edu/\~kbroman/topten_worstgraphs/

## Ticker tape parade {.smaller}

![Roeder K 1994 DNA fingerprinting: A review of the controversy with discussion . Statistical Science 9:222T278, Figure 4](images/images_4a.003.jpeg)

## A line to no understanding {.smaller}

![Epstein MP, Satten GA 2003 Inference on haplotype effects in caseTcontrol studies using unphased genotype data. American Journal of Human Genetics 73:1316T1329, Figure 1](images/images_4a.004.jpeg)

## A hot cup o' mixed messages {.flexbox .vcenter}

![](images/images_4a.006.jpeg)

## I want the biggest slice of the TFBS pie {.flexbox .vcenter .smaller}

![Cawley S, et al. 2004 Unbiased mapping of transcription factor binding sites along human chromosomes 21 and 22 points to widespread regulation of noncoding RNAs. Cell 116:499T509, Figure 1](images/images_4a.005.jpeg)

## A bake sale of pie charts {.smaller}

![Bell ML, et al. 2007 Spatial and temporal variation in PM2.5 chemical composition in the United States for health effects studies. Environmental Health Perspectives 115:989T995, Figure 3](images/images_4a.007.jpeg)

## Wack a mole

![Cotter DJ, et al. 2004 Hematocrit was not validated as a surrogate endpoint for survival amoung epoetinTtreated hemodialysis patients. Journal of Clinical Epidemiology 57:1086T1095, Figure 2](images/images_4a.008.jpeg)

------------------------------------------------------------------------

# Best Practices

## Principles of effective display {.flexbox .vcenter}

> "Graphical excellence is that which gives to the viewer the greatest number of ideas in the shortest time with the least ink in the smallest space"
>
> --- Edward Tufte

## The best statistical graphic ever drawn according to Edward Tufte {.flexbox .vcenter}

![This map by Charles Joseph Minard portrays the losses suffered by Napoleon's army in the Russian campaign of 1812](images/images_4a.009.jpeg)

## Principles of effective display

-   Show the data\
-   Encourage the eye to compare differences
-   Represent magnitudes honestly and accurately\
-   Draw graphical elements clearly, minimizing clutter\
-   Make displays easy to interpret

## “Above all else show the data” \| Tufte 1983

![The relationship between the numbers of native tropical stingless bees and Africanized honey bees onflowering shrubs in French Guiana. The data have been erased in the left panel. Redrawn from Roubik 1978.](images/images_4a.010.jpeg)

## “Maximize the data to ink ratio, within reason” \| Tufte 1983

Draw graphical elements clearly, minimizing clutter

![The percentage of adults over 18 with a “body mass index” greater than 25 in different years The Economist 2006 . Body mass index is a measure of weight relative to height.](images/images_4a.011.jpeg)

## “A graphic does not distort if the visual representation of the data is consistent with the numerical representation” – Tufte 1983

Represent magnitudes honestly and accurately

![Slow wave sleep in the brain hemispheres of mallard ducks sleeping with one eye open. From Rattenborg et al. 1999 Nature](images/images_4a.012.jpeg)

## How Fox News makes a figure ....

![](images/images_4a.013.jpeg)

## How Fox News makes a figure ....

![](images/images_4a.014.jpeg)

“Graphical excellence begins with telling the truth about the data” – Tufte 1983

------------------------------------------------------------------------

## R Interlude

-   read in the data set `Week1b_Stickle_RNAseq.tsv` and assign it to a data object

-   try out the command `View` for the whole data set

-   try out the command `summary` for one of the variables

-   load the package `tidyverse` and try the command `glimpse`

-   try making some nice plots of the different types of data

------------------------------------------------------------------------

# Probability, distributions and sampling

## Stochastic Processes in Statistics {.smaller}

-   We often want to know truths about the world, but the best we can do is estimate them
-   Uncertainty in those estimates is a given.
-   The process of statistics is largely about quantifying and managing uncertainty.
-   Random variables are the product of stochastic processes
-   Expectations can be based on theoretical probability distributions
-   We are going to start with probability rules and then slowly experience different distributions

## Different flavors of inferential statistics {.smaller}

-   **Frequentist Statistics**
    -   Classical or standard approaches
    -   Null hypothesis testing\
        \
-   **Hierarchical Probabilistic Modeling**
    -   Maximum Likelihood
    -   Bayesian Analyses
    -   Machine Learning

## What is probability {.smaller}

-   **Frequency interpretation**

"Probabilities are understood as mathematically convenient approximations to long run relative frequencies."

-   **Subjective (Bayesian) interpretation**

"A probability statement expresses the opinion of some individual regarding how certain an event is to occur."

## Random variables & probability {.smaller}

-   **Probability** is the expression of belief in some future outcome

-   A **random variable** can take on different values with different probabilities

-   The **sample space** of a random variable is the universe of all possible values

## Random variables & probability {.smaller}

-   The **sample space** can be represented by a
    -   **probability mass distribution** (discrete)
    -   **probability density function (PDF)** (continuous)
    -   algebra and calculus are used for each respectively
    -   probabilities of a sample space **always sum to 1.0**
-   How does it make sense that a sample space will always sum to 1?

## Bernoulli distribution {.smaller}

-   Describes the expected outcome of a single event with probability `p`

-   Example of flipping of a **fair** coin once

$$Pr(X=\text{Head}) = \frac{1}{2} = 0.5 = p $$

$$Pr(X=\text{Tails}) = \frac{1}{2} = 0.5 = 1 - p = q $$

## Bernoulli distribution {.smaller}

-   If the coin isn't fair then $p \neq 0.5$
-   However, the probabilities still sum to 1

$$ p + (1-p) = 1 $$ $$ p + q = 1 $$

-   Same is true for other binary possibilities
    -   success or failure
    -   yes or no answers
    -   choosing an allele from a population based upon allele frequencies (Hardy-Weinberg ring any bells??)

## Probability rules {.smaller}

-   Flip a coin twice
-   Represent the first flip as ‘X’ and the second flip as ‘Y’

$$ Pr(\text{X=H and Y=H}) = p*p = p^2 $$ $$ Pr(\text{X=H and Y=T}) = p*q = pq = p^2 $$ $$ Pr(\text{X=T and Y=H}) = q*p = pq $$ $$ Pr(\text{X=T and Y=T}) = q*q = q^2 $$

## Probability rules {.smaller}

-   Probability that the `H` and `T` can occur in any order

$$ \text{Pr(X=H) or Pr(X=T)} = p+q=1$$

$$ \text{Pr(X=H and Y=T) or Pr(X=T and Y=H)} = $$ $$ (p*q) + (p*q) = 2pq $$

-   These are the **'and'** and **'or'** rules of probability
    -   'and' means multiply the probabilities
    -   'or' means sum the probabilities
    -   most probability distributions can be built up from these simple rules

## Let's simulate some coin flips {.smaller}

```{r, echo=TRUE}
# tossing a fair coin
coin <- c("heads", "tails")

sample(coin)

```

## Let's simulate some coin flips {.smaller}

-   What happens when we change the probabilities or the sample size? How confident are we that our coin is fair?

```{r, echo=TRUE, out.width="50%"}
flips <- sample(coin, prob = c(0.5, 0.5), size=13, replace=TRUE)
barplot(table(flips))
```

## Expectation and Moments of Distributions {.smaller}

-   Distributions have **moments** that can be estimated
-   1st, 2nd, 3rd and 4th moments of a distribution?
-   The expectation or mean of a random variable X is:

$$E[X] = \sum_{\text{all x}}^{}xP(X=x) = \mu$$

-   Often we want to know how dispersed the random variable is around its mean.
-   One measure of dispersion is the variance
-   There are higher moments of distributions (e.g. skew and kurtosis)

$$Var(X) = E[X^2] = \sigma^2$$

## Joint probability {.smaller}

$$Pr(X,Y) = Pr(X) * Pr(Y)$$

-   Note that this is true for two **independent** events
-   However, for two non-independent events we also have to take into account their **covariance**
-   To do this we need **conditional probabilities**

## Conditional probability {.smaller}

-   For two **independent** variables: Probability of Y, given X, or the probability of X, given Y.

$$Pr(Y|X) = Pr(Y)\text{ and }Pr(X|Y) = Pr(X)$$

-   For two **non-independent** variables

$$Pr(Y|X) \neq Pr(Y)\text{ and }Pr(X|Y) \neq Pr(X)$$

-   Variables that are non-independent have a shared variance, which is also known as the **covariance**
-   Covariance standardized to a mean of zero and a unit standard deviation is **correlation**

## What is Likelihood vs. Probability? {.smaller}

-   The **probability** of an event is the proportion of times that the event would occur if we repeated a random trial over and over again under the same conditions.

-   The **likelihood** is a conditional probability of a parameter value given a set of data

-   The likelihood of a population parameter **equaling a specific value, given the data**

`L[parameter|data] = Pr[data|parameter]`

-   **Likelihood function** which can have a maximum

-   What is a **Bayesian estimate**? - the use of prior distribution to update a posterior distribution

------------------------------------------------------------------------

# Data wrangling and exploratory data analysis (EDA)

## Data cleaning and manipulation

-   Here, we differentiate "data cleaning" from "data manipulation", which is perhaps an arbitrary distinction.

-   "Data cleaning" typically refers to altering variable class information, fixing mistakes that could have arisen in the data (e.g., an extra '.' symbol in a numeric value), and things of this nature.

-   "Data manipulation", in my mind, refers to altering the structure of the data in a way that changes the functional structure the data (e.g., an addition of a column, deletion of rows, long/wide formatting change).

## Tidyverse family of packages {.smaller}

```{r, echo=FALSE, out.width='90%', fig.align='center'}
knitr::include_graphics("images/week_3.011.jpeg")
```

Let's load the tidyverse meta-package and check the output.

```{r tverse, echo=TRUE, cache = FALSE}
library(tidyverse)
```

## Tidyverse family of packages {.vcenter .flexbox}

-   Hadley Wickham and others have written R packages to modify data

-   These packages do many of the same things as base functions in R

-   However, they are specifically designed to do them faster and more easily

-   Wickham also wrote the package ggplot2 for elegant graphics creations

## Example of a tibble {.smaller}

```{r, echo=FALSE, out.width='90%', fig.align='center'}
knitr::include_graphics("images/week_3.012.jpeg")
```

## Example of a tibble {.smaller}

```{r, echo=FALSE, out.width='90%', fig.align='center'}
knitr::include_graphics("images/week_3.013.jpeg")
```

## Types of vectors of data {.vcenter}

`int` stands for integers

`db`l stands for doubles, or real numbers

`chr` stands for character vectors, or strings

`dttm` stands for date-times (a date + a time)

`lgl` stands for logical, vectors that contain only TRUE or FALSE

`fctr` stands for factors, which R uses to represent categorical variables with fixed possible values

`date` stands for dates

## Types of vectors of data {.smaller}

-   Logical vectors can take only three possible values:
    -   `FALSE`
    -   `TRUE`
    -   `NA` which is 'not available'.
-   Integer and double vectors are known collectively as numeric vectors.
    -   In `R` numbers are doubles by default.
-   Integers have one special value: NA, while doubles have four:
    -   `NA`
    -   `NaN` which is 'not a number'
    -   `Inf`
    -   `-Inf`

## Tidyverse packages (cont.)

The tidyverse actually comes with a lot more packages than those that are just loaded automatically.<sup>1</sup>

```{r tverse_pkgs}
tidyverse_packages()
```

We'll use several of these additional packages during the remainder of this course.

-   E.g. The **lubridate** package for working with dates and the **rvest** package for webscraping.
-   However, bear in mind that these packages will have to be loaded separately.

## Key dplyr verbs

There are five key dplyr verbs that you need to learn.

1.  `filter`: Filter (i.e. subset) observations (rows) based on their values.\
2.  `select`: Select (i.e. subset) variables (columns) by their names:\
3.  `arrange`: Arrange (i.e. reorder) rows based on their values.\
4.  `mutate`: Create new columns.\
5.  `summarise`: Collapse multiple rows into a single summary value.<sup>1</sup>

## `filter()`, `arrange()` & `select()`

```{r, echo=T, eval=F}
filter(flights, month == 11 | month == 12)
```

```{r, echo=T, eval=F}
arrange(flights, year, month, day)
```

```{r, echo=T, eval=F}
select(flights, year)
```

```{r, echo=FALSE, out.width='60%', fig.align='center'}
knitr::include_graphics("images/week_3.015.jpeg")
```

## conditionals {.smaller}

-   `==` equals exactly

-   `<, <=` is smaller than, is smaller than or equal to

-   `>`, \>=\` is bigger than, is bigger than or equal to

-   `!=` not equal to

-   `!` NOT operator, to specify things that should be omitted

-   `&` AND operator, allows you to chain two conditions which must both be met

-   `\|` OR operator, to chains two conditions when at least one should be met

-   `%in%` belongs to one of the following (usually followed by a vector of possible values)

The `AND (`&`) and the OR (`\|\`) operators are also super useful when you want to separate data based on multiple conditions.

## `mutate()` & `transmutate()`

This function will add a new variable that is a function of other variable(s)

```{r, echo=T, eval=F}
mutate(flights_sml,
  gain = arr_delay - dep_delay,
  hours = air_time / 60,
  gain_per_hour = gain / hours
)
```

## `group_by( )` & `summarize( )` {.smaller}

This first function allows you to aggregate data by values of categorical variables

```{r, echo=T, eval=F}
by_day <- group_by(flights, year, month, day)
```

Once you have done this aggregation, you can then calculate values (in this case the mean) of other variables split by the new aggregated levels of the categorical variable

```{r, echo=T, eval=F}
summarise(by_day, delay = mean(dep_delay, na.rm = TRUE))
```

-   Note - you can get a lot of missing values!
-   That’s because aggregation functions obey the usual rule of missing values:
    -   if there’s any missing value in the input, the output will be a missing value.
    -   fortunately, all aggregation functions have an na.rm argument which removes the missing values prior to computation

## Other dplyr goodies {.smaller}

-   `group_by` and `ungroup`: For (un)grouping. - Particularly useful with the `summarise` and `mutate` commands, as we've already seen.\

-   `slice`: Subset rows by position rather than filtering by values. - E.g. `starwars %>% slice(c(1, 5))`\

-   `pull`: Extract a column from as a data frame as a vector or scalar. - E.g. `starwars %>% filter(gender=="female") %>% pull(height)`\

-   `count` and `distinct`: Number and isolate unique observations. - E.g. `starwars %>% count(species)`, or `starwars %>% distinct(species)`\

-   You could also use a combination of `mutate`, `group_by`, and `n()`, e.g. `starwars %>% group_by(species) %>% mutate(num = n())`.

------------------------------------------------------------------------

## R INTERLUDE \| Playing with `Tidyverse` functions {.smaller}

-   Step 1 - Read in the `Week1b_Stickle_RNAseq.tsv` dataset
-   Step 2 - Make the dataset into a tibble
-   Step 3 - Select all of the categorical variables and only 4 of the gene count variables and put them into a new data object
-   Step 4 - Mutate each of the 4 gene expression values by performing a square root transformation making a new variable for each of the original (keep all 8 in the dataset).
-   Step 5 - Summarize the mean and standard deviation for each of the gene count variables grouped by the ‘sex’ and ‘population’ and ‘treatment’ categorical variables
-   Step 6 - Create a histogram for one of the original gene expression variables, and one of the derived variables\
-   Step 7 - Create a box plot for one of the original gene expression variables, and one of the derived variables, split by treatment
-   Step 8 - Write the final data table to a .csv file and one of the figures to a .pdf file
